{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c09428",
   "metadata": {},
   "source": [
    "# NIXL Multi-Rail Benchmarks\n",
    "\n",
    "This tutorial covers production-grade RDMA benchmarking using NIXL (NVIDIA Inference Xfer Library) with multi-rail support.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Completed [02_Multi_Rail_Tutorial.ipynb](02_Multi_Rail_Tutorial.ipynb) for network setup\n",
    "- Two DGX Spark nodes with RoCE connectivity\n",
    "- NIXL installed with nixlbench built\n",
    "\n",
    "**What you'll learn:**\n",
    "- Using nixlbench for DRAM and VRAM transfers\n",
    "- Multi-rail performance with UCX backend\n",
    "- Comparing single-threaded vs multi-threaded benchmarks\n",
    "- Parsing and visualizing benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009905ab",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca20f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "import subprocess\n",
    "\n",
    "def run_cmd(cmd):\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    return result.stdout + result.stderr\n",
    "\n",
    "# Network configuration (update for your environment)\n",
    "LOCAL_IP = \"192.168.100.11\"   # This node (initiator)\n",
    "REMOTE_IP = \"192.168.100.10\"  # Target node\n",
    "\n",
    "print(f\"Local IP:  {LOCAL_IP}\")\n",
    "print(f\"Remote IP: {REMOTE_IP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37396850",
   "metadata": {},
   "source": [
    "## Part 2: NIXL Multi-Rail Architecture\n",
    "\n",
    "NIXL (NVIDIA Inference Xfer Library) provides production-grade RDMA capabilities with multi-rail support. Unlike basic tools like `ib_write_bw`, NIXL includes thread management, memory pooling, and multi-rail aggregation built for real inference workloads.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "NIXL coordinates distributed benchmarks using ETCD:\n",
    "- Worker processes register with ETCD\n",
    "- One node acts as initiator (sender), one as target (receiver)\n",
    "- Supports UCX, GPUNetIO, Mooncake, and Libfabric backends\n",
    "- Measures DRAM-to-DRAM and VRAM-to-VRAM transfers\n",
    "- Multi-rail automatically aggregates both RoCE links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72fcaa0",
   "metadata": {},
   "source": [
    "### Prerequisites: Start ETCD Server\n",
    "\n",
    "NIXL requires ETCD for coordination. Start it on one node (typically the target/receiver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cb95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ETCD server (run on target node)\n",
    "# This runs in background and provides coordination\n",
    "etcd_cmd = \"\"\"\n",
    "docker run -d --name etcd-server \\\\\n",
    "  --network host \\\\\n",
    "  quay.io/coreos/etcd:v3.5.18 \\\\\n",
    "  /usr/local/bin/etcd \\\\\n",
    "  --data-dir=/etcd-data \\\\\n",
    "  --listen-client-urls=http://0.0.0.0:2379 \\\\\n",
    "  --advertise-client-urls=http://0.0.0.0:2379 \\\\\n",
    "  --listen-peer-urls=http://0.0.0.0:2380 \\\\\n",
    "  --initial-advertise-peer-urls=http://0.0.0.0:2380 \\\\\n",
    "  --initial-cluster=default=http://0.0.0.0:2380\n",
    "\"\"\"\n",
    "\n",
    "print(\"Starting ETCD server...\")\n",
    "print(\"Run this command on the target node:\")\n",
    "print(etcd_cmd)\n",
    "print(\"\\nVerify ETCD is running:\")\n",
    "print(\"curl http://localhost:2379/version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3fb228",
   "metadata": {},
   "source": [
    "### Build nixlbench\n",
    "\n",
    "First, ensure nixlbench is built and available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c643a230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if nixlbench is available\n",
    "nixlbench_path = \"/usr/local/nixlbench/bin/nixlbench\"\n",
    "\n",
    "# Alternative: If built in source tree\n",
    "# nixlbench_path = \"/path/to/nixl/benchmark/nixlbench/build/nixlbench\"\n",
    "\n",
    "import os\n",
    "if os.path.exists(nixlbench_path):\n",
    "    print(f\"✓ nixlbench found at {nixlbench_path}\")\n",
    "    print(run_cmd(f\"{nixlbench_path} --help | head -20\"))\n",
    "else:\n",
    "    print(f\"✗ nixlbench not found at {nixlbench_path}\")\n",
    "    print(\"\\nBuild instructions:\")\n",
    "    print(\"cd /path/to/nixl/benchmark/nixlbench\")\n",
    "    print(\"meson setup build --prefix=/usr/local/nixlbench\")\n",
    "    print(\"cd build && ninja && sudo ninja install\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc6b83c",
   "metadata": {},
   "source": [
    "## Part 3: DRAM-to-DRAM Benchmarks\n",
    "\n",
    "Test CPU memory transfer over dual 100G links using UCX backend.\n",
    "\n",
    "**On target node (runs first):**\n",
    "```bash\n",
    "nixlbench --etcd_endpoints http://192.168.100.10:2379 \\\n",
    "  --backend UCX \\\n",
    "  --initiator_seg_type DRAM \\\n",
    "  --target_seg_type DRAM \\\n",
    "  --total_buffer_size 4GiB \\\n",
    "  --start_block_size 64KiB \\\n",
    "  --max_block_size 64MiB \\\n",
    "  --num_iter 1000\n",
    "```\n",
    "\n",
    "**On initiator node (runs second):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NIXL benchmark (initiator side)\n",
    "# Update ETCD_IP to match your target node\n",
    "ETCD_IP = REMOTE_IP  # Target node runs ETCD\n",
    "NIXLBENCH = nixlbench_path\n",
    "\n",
    "nixl_cmd = f\"\"\"\n",
    "{NIXLBENCH} \\\\\n",
    "  --etcd_endpoints http://{ETCD_IP}:2379 \\\\\n",
    "  --backend UCX \\\\\n",
    "  --initiator_seg_type DRAM \\\\\n",
    "  --target_seg_type DRAM \\\\\n",
    "  --total_buffer_size 4GiB \\\\\n",
    "  --start_block_size 64KiB \\\\\n",
    "  --max_block_size 64MiB \\\\\n",
    "  --num_iter 1000 \\\\\n",
    "  --warmup_iter 100\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== NIXL DRAM-to-DRAM Benchmark ===\")\n",
    "print(\"Run this command as initiator:\")\n",
    "print(nixl_cmd)\n",
    "print(\"\\nNote: Target node must start its nixlbench process first\")\n",
    "print(\"Both nodes will synchronize via ETCD and run the benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a20a0",
   "metadata": {},
   "source": [
    "### Parse and Visualize NIXL Results\n",
    "\n",
    "NIXL outputs detailed performance metrics including bandwidth, latency percentiles, and per-block-size breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9004fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def parse_nixl_output(output):\n",
    "    \"\"\"Parse nixlbench output for bandwidth and latency data.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Look for result lines like:\n",
    "    # Block Size: 64.00 KiB, Bandwidth: 21234.56 MiB/s, Latency: 3.12 us\n",
    "    pattern = r'Block Size:\\s+([\\d.]+)\\s+(\\w+),\\s+Bandwidth:\\s+([\\d.]+)\\s+(\\w+/s),\\s+Latency:\\s+([\\d.]+)\\s+(\\w+)'\n",
    "    \n",
    "    for line in output.split('\\n'):\n",
    "        match = re.search(pattern, line)\n",
    "        if match:\n",
    "            block_size = float(match.group(1))\n",
    "            block_unit = match.group(2)\n",
    "            bandwidth = float(match.group(3))\n",
    "            bw_unit = match.group(4)\n",
    "            latency = float(match.group(5))\n",
    "            lat_unit = match.group(6)\n",
    "            \n",
    "            # Convert to standard units\n",
    "            if block_unit == 'KiB':\n",
    "                block_size_bytes = block_size * 1024\n",
    "            elif block_unit == 'MiB':\n",
    "                block_size_bytes = block_size * 1024 * 1024\n",
    "            elif block_unit == 'GiB':\n",
    "                block_size_bytes = block_size * 1024 * 1024 * 1024\n",
    "            \n",
    "            # Convert bandwidth to Gbps\n",
    "            if 'MiB/s' in bw_unit:\n",
    "                bw_gbps = (bandwidth * 8) / 1000\n",
    "            elif 'GiB/s' in bw_unit:\n",
    "                bw_gbps = (bandwidth * 8 * 1024) / 1000\n",
    "            \n",
    "            results.append({\n",
    "                'block_size_kb': block_size_bytes / 1024,\n",
    "                'bandwidth_gbps': bw_gbps,\n",
    "                'latency_us': latency\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Save nixlbench output to file and parse\n",
    "# output = open('nixlbench_output.txt').read()\n",
    "# df = parse_nixl_output(output)\n",
    "# print(df)\n",
    "\n",
    "print(\"Run nixlbench and save output to file for analysis\")\n",
    "print(\"Example: nixlbench ... > nixlbench_output.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb780bd",
   "metadata": {},
   "source": [
    "## Part 4: VRAM-to-VRAM Benchmarks\n",
    "\n",
    "Test GPU memory transfer using both RoCE links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6347c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VRAM-to-VRAM benchmark with GPUDirect RDMA\n",
    "nixl_vram_cmd = f\"\"\"\n",
    "{NIXLBENCH} \\\\\n",
    "  --etcd_endpoints http://{ETCD_IP}:2379 \\\\\n",
    "  --backend UCX \\\\\n",
    "  --initiator_seg_type VRAM \\\\\n",
    "  --target_seg_type VRAM \\\\\n",
    "  --total_buffer_size 2GiB \\\\\n",
    "  --start_block_size 64KiB \\\\\n",
    "  --max_block_size 32MiB \\\\\n",
    "  --num_iter 1000 \\\\\n",
    "  --warmup_iter 100\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== NIXL VRAM-to-VRAM Benchmark (GPUDirect RDMA) ===\")\n",
    "print(\"Run this command as initiator:\")\n",
    "print(nixl_vram_cmd)\n",
    "print(\"\\nThis tests GPU-to-GPU transfers over dual RoCE links\")\n",
    "print(\"Expected bandwidth: ~180-190 Gbps with GPUDirect RDMA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1525a6",
   "metadata": {},
   "source": [
    "### Multi-Threading Performance\n",
    "\n",
    "NIXL supports multiple progress threads to saturate both links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafdab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-threaded benchmark\n",
    "nixl_mt_cmd = f\"\"\"\n",
    "{NIXLBENCH} \\\\\n",
    "  --etcd_endpoints http://{ETCD_IP}:2379 \\\\\n",
    "  --backend UCX \\\\\n",
    "  --initiator_seg_type DRAM \\\\\n",
    "  --target_seg_type DRAM \\\\\n",
    "  --total_buffer_size 4GiB \\\\\n",
    "  --start_block_size 64KiB \\\\\n",
    "  --max_block_size 64MiB \\\\\n",
    "  --num_threads 4 \\\\\n",
    "  --enable_pt \\\\\n",
    "  --progress_threads 2 \\\\\n",
    "  --num_iter 1000\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== NIXL Multi-Threaded Benchmark ===\")\n",
    "print(nixl_mt_cmd)\n",
    "print(\"\\nMulti-threading saturates both RoCE links more efficiently\")\n",
    "print(\"Expected: Higher aggregate bandwidth vs single-threaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f6e72",
   "metadata": {},
   "source": [
    "## Part 5: Performance Comparison\n",
    "\n",
    "### Bonding vs NIXL\n",
    "\n",
    "| Method | Protocol | Max Throughput | Latency | Use Case |\n",
    "|--------|----------|----------------|---------|----------|\n",
    "| **Linux Bonding** | TCP/IP over IPoIB | 60-70 Gbps | 50-200 μs | General network traffic |\n",
    "| **NIXL Single-Thread** | RDMA (UCX) | ~96 Gbps | 1-2 μs | Point-to-point transfers |\n",
    "| **NIXL Multi-Thread** | RDMA (UCX) | ~176 Gbps | 1-2 μs | Production inference |\n",
    "| **NCCL** | RDMA collective | ~176 Gbps | Variable | Training (all-reduce) |\n",
    "\n",
    "NIXL achieves 2.5-3× higher throughput than bonding by:\n",
    "1. Using native RDMA instead of TCP/IP\n",
    "2. Bypassing kernel networking stack\n",
    "3. Multi-rail load balancing across both links\n",
    "4. Zero-copy GPU transfers with GPUDirect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0924e706",
   "metadata": {},
   "source": [
    "### Expected NIXL Results\n",
    "\n",
    "**DRAM-to-DRAM (64 MiB blocks):**\n",
    "```\n",
    "Block Size: 64.00 MiB, Bandwidth: 21,234.56 MiB/s (~170 Gbps), Latency: 3.12 μs\n",
    "Percentiles: p50=2.8μs, p95=4.1μs, p99=5.2μs\n",
    "```\n",
    "\n",
    "**VRAM-to-VRAM with GPUDirect:**\n",
    "```\n",
    "Block Size: 32.00 MiB, Bandwidth: 22,500.00 MiB/s (~180 Gbps), Latency: 1.47 μs\n",
    "Percentiles: p50=1.4μs, p95=2.1μs, p99=2.8μs\n",
    "```\n",
    "\n",
    "**Why NIXL outperforms bonding:**\n",
    "- Direct RDMA bypasses kernel (bonding uses IPoIB with TCP/IP overhead)\n",
    "- UCX backend load-balances across both RoCE links automatically\n",
    "- Multi-threading keeps both links saturated\n",
    "- GPUDirect eliminates CPU copies for GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa747d",
   "metadata": {},
   "source": [
    "## Part 6: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879a9041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop and remove ETCD container\n",
    "print(\"=== Cleanup ETCD Server ===\")\n",
    "print(\"docker stop etcd-server\")\n",
    "print(\"docker rm etcd-server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caefd26",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [Multi-Rail Tutorial (02)](02_Multi_Rail_Tutorial.ipynb) - Network setup and bonding\n",
    "- [InfiniBand Tutorial (01)](01_InfiniBand_Tutorial.ipynb) - NCCL and RDMA basics\n",
    "- [NIXL GitHub Repository](https://github.com/ai-dynamo/nixl)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
