{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1e6409",
   "metadata": {},
   "source": [
    "# ğŸš€ RDMA Networking Tutorial for DGX Spark Systems\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook will teach you:\n",
    "1. **What is RDMA?** - The basics explained simply (InfiniBand vs RoCE)\n",
    "2. **How to check your hardware** - See what's connected\n",
    "3. **Run speed tests** - Compare RDMA vs TCP/IP\n",
    "4. **Understand the results** - Why RDMA matters for AI/ML\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Part 1: Understanding RDMA, InfiniBand, and RoCE\n",
    "\n",
    "### What is RDMA?\n",
    "\n",
    "**RDMA (Remote Direct Memory Access)** lets network cards read/write directly to application memoryâ€”bypassing the CPU entirely. This is what makes high-speed GPU networking possible.\n",
    "\n",
    "There are two main ways to get RDMA:\n",
    "\n",
    "| Technology | InfiniBand | RoCE (RDMA over Converged Ethernet) |\n",
    "|------------|------------|-------------------------------------|\n",
    "| Physical Layer | InfiniBand fabric | Standard Ethernet |\n",
    "| Speed | 100-400 Gbps | 100-400 Gbps |\n",
    "| Latency | ~1-2 microseconds | ~1-2 microseconds |\n",
    "| Switches Required | InfiniBand switches | Regular Ethernet switches |\n",
    "| Configuration | Works out of the box | Needs PFC/ECN on switches |\n",
    "| **DGX Spark uses** | âŒ | âœ… **Yes** |\n",
    "\n",
    "### Wait, DGX Spark Uses RoCE?\n",
    "\n",
    "Yes! If you run `ibv_devinfo`, you'll see device names like `roceP2p1s0f0`. That `roce` prefix tells you this is RoCE, not native InfiniBand.\n",
    "\n",
    "**The good news**: It doesn't matter for your workloads. Same RDMA benefits, same tools (`ib_write_bw`, etc.), same performance. NCCL doesn't care whether it's InfiniBand or RoCEâ€”it just sees RDMA.\n",
    "\n",
    "### Why Does This Matter for AI/ML?\n",
    "\n",
    "When running large AI models across multiple GPUs/machines:\n",
    "- GPUs need to **constantly share data** (KV-cache, activations, model shards)\n",
    "- Slow connections = GPUs waiting = wasted expensive hardware\n",
    "- RDMA keeps GPUs fed with data at maximum speed\n",
    "\n",
    "### Your Setup: 2 DGX Spark Boxes + 2 RoCE Links\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           RoCE/RDMA Links         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   DGX Spark 1   â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚   DGX Spark 2   â”‚\n",
    "â”‚                 â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚                 â”‚\n",
    "â”‚  [GPU][GPU]     â”‚      (Super fast connection!)    â”‚  [GPU][GPU]     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Note**: The tools are called `ib_*` (e.g., `ib_write_bw`) because they use the InfiniBand verbs APIâ€”but they work identically with RoCE.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's check what hardware you have! Run the cells below one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d0dc2",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 0: Install Required Packages\n",
    "\n",
    "Before we can use InfiniBand diagnostic tools, we need to install them. Run the cells below to install all necessary packages.\n",
    "\n",
    "**What we're installing:**\n",
    "- `infiniband-diags` - Diagnostic tools (`ibstat`, `ibv_devinfo`, `perfquery`, etc.)\n",
    "- `rdma-core` - Core RDMA libraries\n",
    "- `ibverbs-utils` - InfiniBand verbs utilities\n",
    "- `perftest` - Performance testing tools (`ib_write_bw`, `ib_write_lat`)\n",
    "- `iperf3` - Network bandwidth testing (works on both Ethernet and InfiniBand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76b6c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
      "â•‘  ğŸ“¦ INSTALLATION INSTRUCTIONS                                        â•‘\n",
      "â•‘                                                                      â•‘\n",
      "â•‘  Copy and paste these commands into a terminal window:               â•‘\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "# Step 1: Update package list\n",
      "sudo apt-get update\n",
      "\n",
      "# Step 2: Install InfiniBand tools\n",
      "sudo apt-get install -y infiniband-diags rdma-core ibverbs-utils perftest iperf3\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "After running the commands above in a terminal, come back here and run\n",
      "the NEXT cell (Step 0.2) to verify the installation was successful.\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 0.1: Install InfiniBand diagnostic and performance tools\n",
    "# \n",
    "# âš ï¸ IMPORTANT: Run these commands in a TERMINAL (not in this notebook)\n",
    "# The notebook cannot prompt for your sudo password.\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  ğŸ“¦ INSTALLATION INSTRUCTIONS                                        â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  Copy and paste these commands into a terminal window:               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Step 1: Update package list\n",
    "sudo apt-get update\n",
    "\n",
    "# Step 2: Install InfiniBand tools\n",
    "sudo apt-get install -y infiniband-diags rdma-core ibverbs-utils perftest iperf3\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "After running the commands above in a terminal, come back here and run\n",
    "the NEXT cell (Step 0.2) to verify the installation was successful.\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98966c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking installed tools...\n",
      "==================================================\n",
      "âœ… ibstat          - InfiniBand status tool\n",
      "âœ… ibv_devinfo     - InfiniBand device info\n",
      "âœ… ib_write_bw     - Bandwidth test tool\n",
      "âœ… ib_write_lat    - Latency test tool\n",
      "âœ… iperf3          - Network bandwidth tester\n",
      "âœ… perfquery       - Performance query tool\n",
      "==================================================\n",
      "ğŸ‰ All tools installed! You're ready to proceed.\n"
     ]
    }
   ],
   "source": [
    "# Step 0.2: Verify installation - check that all tools are available\n",
    "\n",
    "import shutil\n",
    "\n",
    "tools_to_check = [\n",
    "    (\"ibstat\", \"InfiniBand status tool\"),\n",
    "    (\"ibv_devinfo\", \"InfiniBand device info\"),\n",
    "    (\"ib_write_bw\", \"Bandwidth test tool\"),\n",
    "    (\"ib_write_lat\", \"Latency test tool\"),\n",
    "    (\"iperf3\", \"Network bandwidth tester\"),\n",
    "    (\"perfquery\", \"Performance query tool\"),\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Checking installed tools...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_found = True\n",
    "for tool, description in tools_to_check:\n",
    "    path = shutil.which(tool)\n",
    "    if path:\n",
    "        print(f\"âœ… {tool:15} - {description}\")\n",
    "    else:\n",
    "        print(f\"âŒ {tool:15} - NOT FOUND\")\n",
    "        all_found = False\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_found:\n",
    "    print(\"ğŸ‰ All tools installed! You're ready to proceed.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some tools missing. Re-run the installation cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4009f",
   "metadata": {},
   "source": [
    "### âš ï¸ Troubleshooting Installation\n",
    "\n",
    "If installation fails, try running these commands manually in a terminal:\n",
    "\n",
    "```bash\n",
    "# Update and install\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y infiniband-diags rdma-core ibverbs-utils perftest iperf3\n",
    "\n",
    "# On some systems, you may also need:\n",
    "sudo apt-get install -y libibverbs-dev librdmacm-dev\n",
    "```\n",
    "\n",
    "**Common issues:**\n",
    "- \"Package not found\" â†’ Your system may use different package names. Try `apt search infiniband`\n",
    "- \"Permission denied\" â†’ Make sure you have sudo access\n",
    "- Already installed â†’ Great! Move on to Part 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b7891",
   "metadata": {},
   "source": [
    "## ğŸ” Part 2: Check Your System\n",
    "\n",
    "First, let's import the libraries we need and create some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fd2fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Helper functions loaded! Ready to explore your system.\n"
     ]
    }
   ],
   "source": [
    "# Standard imports - run this first!\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def run_command(cmd, description=\"\"):\n",
    "    \"\"\"Run a shell command and display the output nicely\"\"\"\n",
    "    if description:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“‹ {description}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Command: {cmd}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "        if result.stderr and result.returncode != 0:\n",
    "            print(f\"âš ï¸ Error: {result.stderr}\")\n",
    "        return result.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Command timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"âœ… Helper functions loaded! Ready to explore your system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48245af1",
   "metadata": {},
   "source": [
    "### Step 2.1: What machine am I on?\n",
    "\n",
    "Let's first check which DGX Spark box you're currently using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6775a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Your machine's name (hostname)\n",
      "============================================================\n",
      "Command: hostname\n",
      "\n",
      "spark-02\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Operating system details\n",
      "============================================================\n",
      "Command: uname -a\n",
      "\n",
      "Linux spark-02 6.14.0-1015-nvidia #15-Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 25 18:02:16 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check your hostname and basic system info\n",
    "run_command(\"hostname\", \"Your machine's name (hostname)\")\n",
    "run_command(\"uname -a\", \"Operating system details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d15f2",
   "metadata": {},
   "source": [
    "### Step 2.2: Do I have InfiniBand hardware?\n",
    "\n",
    "Let's check if InfiniBand devices are detected on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f3f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Looking for InfiniBand/Mellanox hardware in PCI devices\n",
      "============================================================\n",
      "Command: lspci | grep -i 'infiniband\\|mellanox\\|connectx'\n",
      "\n",
      "0000:01:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]\n",
      "0000:01:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]\n",
      "0002:01:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]\n",
      "0002:01:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\|'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\|'\n",
      "/tmp/ipykernel_48456/2990204773.py:2: SyntaxWarning: invalid escape sequence '\\|'\n",
      "  run_command(\"lspci | grep -i 'infiniband\\|mellanox\\|connectx'\",\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for InfiniBand devices\n",
    "run_command(\"lspci | grep -i 'infiniband\\|mellanox\\|connectx'\", \n",
    "            \"Looking for InfiniBand/Mellanox hardware in PCI devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936d9f3",
   "metadata": {},
   "source": [
    "**What to expect:** You should see something like \"Mellanox ConnectX-7\" or similar. This is your RDMA network card (used for both InfiniBand and RoCE)!\n",
    "\n",
    "### Step 2.3: Check RDMA Device Status\n",
    "\n",
    "Now let's see the status of your RDMA connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01433081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ InfiniBand device status (ibstat)\n",
      "============================================================\n",
      "Command: ibstat\n",
      "\n",
      "CA 'roceP2p1s0f0'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a17\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a17\n",
      "\t\tLink layer: Ethernet\n",
      "CA 'roceP2p1s0f1'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a18\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a18\n",
      "\t\tLink layer: Ethernet\n",
      "CA 'rocep1s0f0'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a13\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a13\n",
      "\t\tLink layer: Ethernet\n",
      "CA 'rocep1s0f1'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a14\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a14\n",
      "\t\tLink layer: Ethernet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check InfiniBand status - this is the main command!\n",
    "run_command(\"ibstat\", \"InfiniBand device status (ibstat)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821fd4f",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding ibstat Output\n",
    "\n",
    "Here's what to look for:\n",
    "\n",
    "| Field | Good Value | Meaning |\n",
    "|-------|------------|----------|\n",
    "| **State** | `Active` | Cable connected and working! |\n",
    "| **State** | `Down` | No cable or other end not connected |\n",
    "| **Physical state** | `LinkUp` | Physical connection established |\n",
    "| **Rate** | `100` or higher | Speed in Gbps |\n",
    "\n",
    "**If you see `Active` and `LinkUp` â†’ Your InfiniBand cables are connected correctly! ğŸ‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38cdacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Detailed InfiniBand device information\n",
      "============================================================\n",
      "Command: ibv_devinfo\n",
      "\n",
      "hca_id:\trocep1s0f0\n",
      "\ttransport:\t\t\tInfiniBand (0)\n",
      "\tfw_ver:\t\t\t\t28.45.4028\n",
      "\tnode_guid:\t\t\t30c5:9903:003e:6a13\n",
      "\tsys_image_guid:\t\t\t30c5:9903:003e:6a13\n",
      "\tvendor_id:\t\t\t0x02c9\n",
      "\tvendor_part_id:\t\t\t4129\n",
      "\thw_ver:\t\t\t\t0x0\n",
      "\tboard_id:\t\t\tNVD0000000087\n",
      "\tphys_port_cnt:\t\t\t1\n",
      "\t\tport:\t1\n",
      "\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n",
      "\t\t\tmax_mtu:\t\t4096 (5)\n",
      "\t\t\tactive_mtu:\t\t1024 (3)\n",
      "\t\t\tsm_lid:\t\t\t0\n",
      "\t\t\tport_lid:\t\t0\n",
      "\t\t\tport_lmc:\t\t0x00\n",
      "\t\t\tlink_layer:\t\tEthernet\n",
      "\n",
      "hca_id:\trocep1s0f1\n",
      "\ttransport:\t\t\tInfiniBand (0)\n",
      "\tfw_ver:\t\t\t\t28.45.4028\n",
      "\tnode_guid:\t\t\t30c5:9903:003e:6a14\n",
      "\tsys_image_guid:\t\t\t30c5:9903:003e:6a13\n",
      "\tvendor_id:\t\t\t0x02c9\n",
      "\tvendor_part_id:\t\t\t4129\n",
      "\thw_ver:\t\t\t\t0x0\n",
      "\tboard_id:\t\t\tNVD0000000087\n",
      "\tphys_port_cnt:\t\t\t1\n",
      "\t\tport:\t1\n",
      "\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n",
      "\t\t\tmax_mtu:\t\t4096 (5)\n",
      "\t\t\tactive_mtu:\t\t1024 (3)\n",
      "\t\t\tsm_lid:\t\t\t0\n",
      "\t\t\tport_lid:\t\t0\n",
      "\t\t\tport_lmc:\t\t0x00\n",
      "\t\t\tlink_layer:\t\tEthernet\n",
      "\n",
      "hca_id:\troceP2p1s0f0\n",
      "\ttransport:\t\t\tInfiniBand (0)\n",
      "\tfw_ver:\t\t\t\t28.45.4028\n",
      "\tnode_guid:\t\t\t30c5:9903:003e:6a17\n",
      "\tsys_image_guid:\t\t\t30c5:9903:003e:6a13\n",
      "\tvendor_id:\t\t\t0x02c9\n",
      "\tvendor_part_id:\t\t\t4129\n",
      "\thw_ver:\t\t\t\t0x0\n",
      "\tboard_id:\t\t\tNVD0000000087\n",
      "\tphys_port_cnt:\t\t\t1\n",
      "\t\tport:\t1\n",
      "\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n",
      "\t\t\tmax_mtu:\t\t4096 (5)\n",
      "\t\t\tactive_mtu:\t\t1024 (3)\n",
      "\t\t\tsm_lid:\t\t\t0\n",
      "\t\t\tport_lid:\t\t0\n",
      "\t\t\tport_lmc:\t\t0x00\n",
      "\t\t\tlink_layer:\t\tEthernet\n",
      "\n",
      "hca_id:\troceP2p1s0f1\n",
      "\ttransport:\t\t\tInfiniBand (0)\n",
      "\tfw_ver:\t\t\t\t28.45.4028\n",
      "\tnode_guid:\t\t\t30c5:9903:003e:6a18\n",
      "\tsys_image_guid:\t\t\t30c5:9903:003e:6a13\n",
      "\tvendor_id:\t\t\t0x02c9\n",
      "\tvendor_part_id:\t\t\t4129\n",
      "\thw_ver:\t\t\t\t0x0\n",
      "\tboard_id:\t\t\tNVD0000000087\n",
      "\tphys_port_cnt:\t\t\t1\n",
      "\t\tport:\t1\n",
      "\t\t\tstate:\t\t\tPORT_ACTIVE (4)\n",
      "\t\t\tmax_mtu:\t\t4096 (5)\n",
      "\t\t\tactive_mtu:\t\t1024 (3)\n",
      "\t\t\tsm_lid:\t\t\t0\n",
      "\t\t\tport_lid:\t\t0\n",
      "\t\t\tport_lmc:\t\t0x00\n",
      "\t\t\tlink_layer:\t\tEthernet\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More detailed view of InfiniBand devices\n",
    "run_command(\"ibv_devinfo\", \"Detailed InfiniBand device information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fab0ab",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding ibv_devinfo Output\n",
    "\n",
    "Look at the device names in the output above. You'll see something like:\n",
    "\n",
    "- `roceP2p1s0f0` â†’ **This is RoCE** (RDMA over Converged Ethernet)\n",
    "- `mlx5_0` â†’ Generic Mellanox/NVIDIA device name\n",
    "- `ib0` â†’ Traditional InfiniBand device name (rare on modern systems)\n",
    "\n",
    "**If you see `roce` in the device name**: Your system uses RoCE, not native InfiniBand. This is normal for DGX Spark! All the same tools work, and performance is identical.\n",
    "\n",
    "The key fields to check:\n",
    "| Field | What to Look For |\n",
    "|-------|------------------|\n",
    "| `transport` | `InfiniBand` (even for RoCE, the API is the same) |\n",
    "| `link_layer` | `Ethernet` (for RoCE) or `InfiniBand` (for native IB) |\n",
    "| `state` | `PORT_ACTIVE` = ready to use |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951074",
   "metadata": {},
   "source": [
    "### Step 2.4: Check Network Interfaces\n",
    "\n",
    "InfiniBand creates network interfaces just like Ethernet. Let's see all your network interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c83eecf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ All network interfaces on this system\n",
      "============================================================\n",
      "Command: ip link show\n",
      "\n",
      "1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n",
      "    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n",
      "2: enP7s7: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN mode DEFAULT group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:12 brd ff:ff:ff:ff:ff:ff\n",
      "    altname enP7p1s0\n",
      "3: enp1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:13 brd ff:ff:ff:ff:ff:ff\n",
      "4: enp1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:14 brd ff:ff:ff:ff:ff:ff\n",
      "5: enP2p1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:17 brd ff:ff:ff:ff:ff:ff\n",
      "6: enP2p1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:18 brd ff:ff:ff:ff:ff:ff\n",
      "7: wlP9s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP mode DORMANT group default qlen 1000\n",
      "    link/ether 50:bb:b5:a3:b9:28 brd ff:ff:ff:ff:ff:ff\n",
      "    altname wlP9p1s0\n",
      "8: tailscale0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1280 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 500\n",
      "    link/none \n",
      "9: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default \n",
      "    link/ether b2:f2:35:c8:c9:41 brd ff:ff:ff:ff:ff:ff\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all network interfaces\n",
    "run_command(\"ip link show\", \"All network interfaces on this system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ad00637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ IP addresses assigned to each interface\n",
      "============================================================\n",
      "Command: ip addr show\n",
      "\n",
      "1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n",
      "    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n",
      "    inet 127.0.0.1/8 scope host lo\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 ::1/128 scope host noprefixroute \n",
      "       valid_lft forever preferred_lft forever\n",
      "2: enP7s7: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:12 brd ff:ff:ff:ff:ff:ff\n",
      "    altname enP7p1s0\n",
      "3: enp1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:13 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.100.11/24 brd 192.168.100.255 scope global noprefixroute enp1s0f0np0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a13/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "4: enp1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:14 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.200.13/24 brd 192.168.200.255 scope global noprefixroute enp1s0f1np1\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a14/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "5: enP2p1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:17 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.100.15/24 brd 192.168.100.255 scope global noprefixroute enP2p1s0f0np0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a17/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "6: enP2p1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:18 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.200.17/24 brd 192.168.200.255 scope global noprefixroute enP2p1s0f1np1\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a18/64 scope link \n",
      "       valid_lft forever preferred_lft forever\n",
      "7: wlP9s9: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000\n",
      "    link/ether 50:bb:b5:a3:b9:28 brd ff:ff:ff:ff:ff:ff\n",
      "    altname wlP9p1s0\n",
      "    inet 192.168.1.71/24 brd 192.168.1.255 scope global dynamic noprefixroute wlP9s9\n",
      "       valid_lft 50925sec preferred_lft 50925sec\n",
      "    inet6 2600:1702:56e5:4e10::45/128 scope global dynamic noprefixroute \n",
      "       valid_lft 3100sec preferred_lft 3100sec\n",
      "    inet6 2600:1702:56e5:4e10:728e:2a38:25af:5137/64 scope global temporary dynamic \n",
      "       valid_lft 3548sec preferred_lft 3548sec\n",
      "    inet6 2600:1702:56e5:4e10:c117:dc5:4a34:1a46/64 scope global dynamic mngtmpaddr noprefixroute \n",
      "       valid_lft 3548sec preferred_lft 3548sec\n",
      "    inet6 fe80::4955:674b:6f5a:1019/64 scope link noprefixroute \n",
      "       valid_lft forever preferred_lft forever\n",
      "8: tailscale0: <POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP> mtu 1280 qdisc fq_codel state UNKNOWN group default qlen 500\n",
      "    link/none \n",
      "    inet 100.116.56.45/32 scope global tailscale0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fd7a:115c:a1e0::3501:3844/128 scope global \n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::7921:b91a:193f:a232/64 scope link stable-privacy \n",
      "       valid_lft forever preferred_lft forever\n",
      "9: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default \n",
      "    link/ether b2:f2:35:c8:c9:41 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n",
      "       valid_lft forever preferred_lft forever\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show IP addresses assigned to interfaces\n",
    "run_command(\"ip addr show\", \"IP addresses assigned to each interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b9f11",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Network Interfaces\n",
    "\n",
    "You'll likely see several types:\n",
    "\n",
    "| Interface Name | Type | Description |\n",
    "|----------------|------|-------------|\n",
    "| `lo` | Loopback | Internal (127.0.0.1) |\n",
    "| `eth0`, `enp...` | Ethernet | Regular network cable |\n",
    "| `ib0`, `ibp...` | **InfiniBand** | Traditional IB interface names (rare now) |\n",
    "| `enp*np*` (e.g., `enp1s0f0np0`) | **RoCE/RDMA** | Predictable naming for RDMA interfaces |\n",
    "| `docker0`, `br-...` | Virtual | Docker/container networks |\n",
    "\n",
    "**On DGX Spark**: Look for interfaces with the `np` suffix (like `enp1s0f0np0`) - these are your RoCE/RDMA interfaces. They look like Ethernet interfaces but support RDMA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9639045",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Part 3: Understanding Your Two DGX Spark Boxes\n",
    "\n",
    "Before we run tests, we need to know:\n",
    "1. The **IP address** of THIS machine (on the RoCE/RDMA interface)\n",
    "2. The **IP address** of the OTHER machine (on the RoCE/RDMA interface)\n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4750472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for InfiniBand interfaces (ib0, ib1, enp*np*, etc.)...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ InfiniBand interface IP addresses\n",
      "============================================================\n",
      "Command: ip addr show | grep -E -A 2 'ib[0-9]|enp.*np[0-9]'\n",
      "\n",
      "3: enp1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:13 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.100.11/24 brd 192.168.100.255 scope global noprefixroute enp1s0f0np0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a13/64 scope link \n",
      "--\n",
      "4: enp1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:14 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.200.13/24 brd 192.168.200.255 scope global noprefixroute enp1s0f1np1\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a14/64 scope link \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find InfiniBand IP addresses specifically\n",
    "print(\"Looking for InfiniBand interfaces (ib0, ib1, enp*np*, etc.)...\\n\")\n",
    "run_command(\"ip addr show | grep -E -A 2 'ib[0-9]|enp.*np[0-9]'\", \"InfiniBand interface IP addresses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7a29b",
   "metadata": {},
   "source": [
    "### ğŸ“ Write Down Your IPs!\n",
    "\n",
    "Fill in these values based on what you see above:\n",
    "\n",
    "**This machine (DGX Spark 2):**\n",
    "- Hostname: `_spark-02___________`\n",
    "- InfiniBand IP (e.g., enp1s0f0np0): `_192.168.100.11___________`\n",
    "\n",
    "**Other machine (DGX Spark 1):**\n",
    "- Hostname: `_spark-01___________`  \n",
    "- InfiniBand IP (e.g., enp1s0f0np0): `192.168.100.10____________`\n",
    "\n",
    "*(You'll need to run this notebook on BOTH machines to get both IPs, or SSH to the other machine)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb3bdedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Will test connection to: 192.168.100.10\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURE THIS: Set the IP of the OTHER DGX Spark box\n",
    "# After running the cells above on both machines, fill in the other machine's InfiniBand IP\n",
    "\n",
    "OTHER_MACHINE_IB_IP = \"192.168.100.10\"  # Example: \"192.168.1.2\" - FILL THIS IN!\n",
    "\n",
    "if not OTHER_MACHINE_IB_IP:\n",
    "    print(\"âš ï¸ Please set OTHER_MACHINE_IB_IP above!\")\n",
    "    print(\"   Run the previous cell on your other DGX Spark to find its InfiniBand IP.\")\n",
    "else:\n",
    "    print(f\"âœ… Will test connection to: {OTHER_MACHINE_IB_IP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f004a9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Part 4: Basic Connectivity Test\n",
    "\n",
    "Let's make sure the two machines can talk to each other over InfiniBand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46f581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Pinging other DGX Spark at 192.168.100.10\n",
      "============================================================\n",
      "Command: ping -c 5 192.168.100.10\n",
      "\n",
      "PING 192.168.100.10 (192.168.100.10) 56(84) bytes of data.\n",
      "64 bytes from 192.168.100.10: icmp_seq=1 ttl=64 time=0.573 ms\n",
      "64 bytes from 192.168.100.10: icmp_seq=2 ttl=64 time=0.842 ms\n",
      "64 bytes from 192.168.100.10: icmp_seq=3 ttl=64 time=1.31 ms\n",
      "64 bytes from 192.168.100.10: icmp_seq=4 ttl=64 time=0.878 ms\n",
      "64 bytes from 192.168.100.10: icmp_seq=5 ttl=64 time=0.611 ms\n",
      "\n",
      "--- 192.168.100.10 ping statistics ---\n",
      "5 packets transmitted, 5 received, 0% packet loss, time 4091ms\n",
      "rtt min/avg/max/mdev = 0.573/0.842/1.310/0.263 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ping test over InfiniBand\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    run_command(f\"ping -c 5 {OTHER_MACHINE_IB_IP}\", \n",
    "                f\"Pinging other DGX Spark at {OTHER_MACHINE_IB_IP}\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb31ed6",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Ping Results\n",
    "\n",
    "Look at the **time** values:\n",
    "- `time=0.1 ms` â†’ Excellent! This is InfiniBand speed\n",
    "- `time=1-5 ms` â†’ Good, still fast\n",
    "- `time=10+ ms` â†’ Might be going through Ethernet instead\n",
    "\n",
    "**InfiniBand ping times should be very low (under 1ms typically)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89dc4af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Part 5: Speed Tests - The Fun Part!\n",
    "\n",
    "Now let's measure how fast InfiniBand really is! We'll use tools called `ib_write_bw` and `ib_write_lat`.\n",
    "\n",
    "### How These Tests Work\n",
    "\n",
    "```\n",
    "Machine 1 (Server)          Machine 2 (Client)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Listening...   â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  Connect & Test â”‚\n",
    "â”‚  Ready to       â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚  Send data back â”‚\n",
    "â”‚  receive data   â”‚         â”‚  and forth      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Important:** You need to run commands on BOTH machines!\n",
    "1. Start the **server** on one machine first\n",
    "2. Then start the **client** on the other machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f40ac",
   "metadata": {},
   "source": [
    "### Step 5.1: Bandwidth Test\n",
    "\n",
    "**Bandwidth** = How much data can flow per second (like water through a pipe)\n",
    "\n",
    "#### On Machine 1 (Server) - Run this in a terminal:\n",
    "```bash\n",
    "ib_write_bw\n",
    "```\n",
    "\n",
    "#### On Machine 2 (Client) - Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "069ca9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Running bandwidth test... (takes about 10 seconds)\n",
      "\n",
      "ğŸ”´ MAKE SURE you ran 'ib_write_bw' on the other machine first!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ InfiniBand Bandwidth Test\n",
      "============================================================\n",
      "Command: ib_write_bw 192.168.100.10\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "                    RDMA_Write BW Test\n",
      " Dual-port       : OFF\t\tDevice         : rocep1s0f0\n",
      " Number of qps   : 1\t\tTransport type : IB\n",
      " Connection type : RC\t\tUsing SRQ      : OFF\n",
      " PCIe relax order: ON\n",
      " ibv_wr* API     : ON\n",
      " TX depth        : 128\n",
      " CQ Moderation   : 1\n",
      " Mtu             : 1024[B]\n",
      " Link type       : Ethernet\n",
      " GID index       : 3\n",
      " Max inline data : 0[B]\n",
      " rdma_cm QPs\t : OFF\n",
      " Data ex. method : Ethernet\n",
      "---------------------------------------------------------------------------------------\n",
      " local address: LID 0000 QPN 0x012b PSN 0xf502fc RKey 0x184300 VAddr 0x00f21bf406d000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:100:11\n",
      " remote address: LID 0000 QPN 0x012b PSN 0x70ff6e RKey 0x184300 VAddr 0x00fa0beb9cd000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:100:10\n",
      "---------------------------------------------------------------------------------------\n",
      " #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]\n",
      " 65536      5000             11032.69            11031.60\t\t   0.176506\n",
      "---------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BANDWIDTH TEST - Run this AFTER starting server on the other machine!\n",
    "# On the OTHER machine, first run: ib_write_bw\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"â³ Running bandwidth test... (takes about 10 seconds)\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE you ran 'ib_write_bw' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_bw {OTHER_MACHINE_IB_IP}\", \n",
    "                \"InfiniBand Bandwidth Test\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4065415",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Bandwidth Results\n",
    "\n",
    "The output looks something like this:\n",
    "```\n",
    "---------------------------------------------------------------------------------------\n",
    " #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]\n",
    " 65536      5000           12045.23           12032.45             0.183521\n",
    "---------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "**Look at the `BW average[MB/sec]` column** - this is your sustained bandwidth.\n",
    "\n",
    "#### Converting MB/sec to Gbps:\n",
    "$$\\text{Gbps} = \\frac{\\text{MB/sec} \\times 8}{1000}$$\n",
    "\n",
    "**Why Ã— 8 and Ã· 1000?**\n",
    "\n",
    "| Step | What it does | Reason |\n",
    "|------|--------------|--------|\n",
    "| **Ã— 8** | Converts **Bytes** â†’ **bits** | 1 Byte = 8 bits |\n",
    "| **Ã· 1000** | Converts **Mega** â†’ **Giga** | 1 Giga = 1000 Mega |\n",
    "\n",
    "```\n",
    "MB/sec  â†’  Mb/sec  â†’  Gb/sec\n",
    "        Ã—8         Ã·1000\n",
    "```\n",
    "\n",
    "- **Benchmark tools** report in **Bytes** (B) - what you actually transfer\n",
    "- **Network specs** advertise in **bits** (b) - industry standard\n",
    "\n",
    "This is why a \"100 Gbps\" InfiniBand link shows ~12,000 MB/sec in benchmarks - same speed, different units!\n",
    "\n",
    "#### Reference Table:\n",
    "\n",
    "| BW average (MB/sec) | Calculation | Speed (Gbps) | Hardware |\n",
    "|---------------------|-------------|--------------|----------|\n",
    "| ~12,000 MB/sec | 12000 Ã— 8 Ã· 1000 | **~96 Gbps** | ConnectX-6 (100G) |\n",
    "| ~24,000 MB/sec | 24000 Ã— 8 Ã· 1000 | **~192 Gbps** | ConnectX-6 HDR (200G) |\n",
    "| ~48,000 MB/sec | 48000 Ã— 8 Ã· 1000 | **~384 Gbps** | ConnectX-7 NDR (400G) |\n",
    "\n",
    "**Quick mental math:** Divide MB/sec by 125 to get Gbps  \n",
    "Example: 12,000 Ã· 125 = **96 Gbps** âœ…\n",
    "\n",
    "**For comparison:**\n",
    "- WiFi 6: ~150 MB/sec (~1.2 Gbps)\n",
    "- Gigabit Ethernet: ~125 MB/sec (1 Gbps)\n",
    "- 10G Ethernet: ~1,250 MB/sec (10 Gbps)\n",
    "\n",
    "**InfiniBand is typically 10-100x faster than standard networking!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b74a8",
   "metadata": {},
   "source": [
    "### ğŸ“– ib_write_bw vs ib_send_bw - What's the Difference?\n",
    "\n",
    "You might see different RDMA bandwidth tests mentioned. Here's what they do:\n",
    "\n",
    "| Test | RDMA Operation | How it Works |\n",
    "|------|----------------|--------------|\n",
    "| `ib_write_bw` | **RDMA Write** | One-sided: sender writes directly to receiver's memory |\n",
    "| `ib_send_bw` | **RDMA Send** | Two-sided: receiver must post receive buffers first |\n",
    "| `ib_read_bw` | **RDMA Read** | One-sided: reader pulls data from remote memory |\n",
    "\n",
    "#### RDMA Write (`ib_write_bw`) - One-Sided\n",
    "\n",
    "```\n",
    "Sender                          Receiver\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ App  â”‚ \"Write to addr X\"      â”‚ App  â”‚\n",
    "â”‚      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      â”‚ (CPU not notified!)\n",
    "â”‚ RAM  â”‚                        â”‚ RAM  â”‚ â† Data just appears\n",
    "â””â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "The sender writes directly to the receiver's memory. The receiver's CPU is never involvedâ€”data just appears. **Lowest overhead, best for raw performance testing.**\n",
    "\n",
    "#### RDMA Send (`ib_send_bw`) - Two-Sided\n",
    "\n",
    "```\n",
    "Sender                          Receiver\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ App  â”‚ \"Send this data\"       â”‚ App  â”‚ (must pre-post receives)\n",
    "â”‚      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      â”‚ â† Gets completion event\n",
    "â”‚ RAM  â”‚                        â”‚ RAM  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "The receiver must post receive buffers in advance and gets notified when data arrives. **More like traditional message passing (MPI-style).**\n",
    "\n",
    "#### Which Test to Use?\n",
    "\n",
    "| Scenario | Recommended Test |\n",
    "|----------|------------------|\n",
    "| Validating InfiniBand hardware | `ib_write_bw` (standard) |\n",
    "| MPI-style workload simulation | `ib_send_bw` |\n",
    "| Storage/NVMe-oF patterns | `ib_write_bw` or `ib_read_bw` |\n",
    "| General validation | Both should hit near line-rate |\n",
    "\n",
    "**Performance difference is typically minimal** - both should achieve similar bandwidth. `ib_write_bw` may show slightly higher numbers due to zero receive-side overhead.\n",
    "\n",
    "**The full perftest suite:**\n",
    "- `ib_write_bw` / `ib_write_lat` - RDMA Write operations\n",
    "- `ib_read_bw` / `ib_read_lat` - RDMA Read operations\n",
    "- `ib_send_bw` / `ib_send_lat` - RDMA Send operations\n",
    "- `ib_atomic_bw` / `ib_atomic_lat` - Atomic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52028377",
   "metadata": {},
   "source": [
    "### Step 5.2: Latency Test\n",
    "\n",
    "**Latency** = How long it takes for a single message to arrive (like response time)\n",
    "\n",
    "#### On Machine 1 (Server) - Run this in a terminal:\n",
    "```bash\n",
    "ib_write_lat\n",
    "```\n",
    "\n",
    "#### On Machine 2 (Client) - Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcb941db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Running latency test... (takes about 10 seconds)\n",
      "\n",
      "ğŸ”´ MAKE SURE you ran 'ib_write_lat' on the other machine first!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ InfiniBand Latency Test\n",
      "============================================================\n",
      "Command: ib_write_lat 192.168.100.10\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "                    RDMA_Write Latency Test\n",
      " Dual-port       : OFF\t\tDevice         : rocep1s0f0\n",
      " Number of qps   : 1\t\tTransport type : IB\n",
      " Connection type : RC\t\tUsing SRQ      : OFF\n",
      " PCIe relax order: OFF\n",
      " ibv_wr* API     : ON\n",
      " TX depth        : 1\n",
      " Mtu             : 1024[B]\n",
      " Link type       : Ethernet\n",
      " GID index       : 3\n",
      " Max inline data : 220[B]\n",
      " rdma_cm QPs\t : OFF\n",
      " Data ex. method : Ethernet\n",
      "---------------------------------------------------------------------------------------\n",
      " local address: LID 0000 QPN 0x012c PSN 0xa5d7c1 RKey 0x1802bf VAddr 0x00b98c6c6aa000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:100:11\n",
      " remote address: LID 0000 QPN 0x012c PSN 0x42187a RKey 0x181ad7 VAddr 0x00c19558a8c000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:100:10\n",
      "---------------------------------------------------------------------------------------\n",
      " #bytes #iterations    t_min[usec]    t_max[usec]  t_typical[usec]    t_avg[usec]    t_stdev[usec]   99% percentile[usec]   99.9% percentile[usec] \n",
      " 2       1000          1.86           3.37         1.98     \t       1.99        \t0.04   \t\t2.58    \t\t3.37   \n",
      "---------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LATENCY TEST - Run this AFTER starting server on the other machine!\n",
    "# On the OTHER machine, first run: ib_write_lat\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"â³ Running latency test... (takes about 10 seconds)\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE you ran 'ib_write_lat' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_lat {OTHER_MACHINE_IB_IP}\", \n",
    "                \"InfiniBand Latency Test\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a91145",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Latency Results\n",
    "\n",
    "Look for the **t_avg** (average time) column:\n",
    "\n",
    "| Latency | What it means |\n",
    "|---------|---------------|\n",
    "| 1-2 Î¼s (microseconds) | Excellent! InfiniBand shining |\n",
    "| 5-10 Î¼s | Good |\n",
    "| 50+ Î¼s | Something might be wrong |\n",
    "\n",
    "**For comparison:**\n",
    "- InfiniBand: ~1-2 microseconds\n",
    "- Ethernet (local): ~50-200 microseconds  \n",
    "- WiFi: ~1,000-10,000 microseconds (1-10 ms)\n",
    "\n",
    "**InfiniBand latency is 50-100x lower than Ethernet!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0765c8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”— Part 5.5: Single Cable vs Dual Cable Experiment\n",
    "\n",
    "Currently, you have **one InfiniBand cable** connecting the two DGX Spark boxes. Let's see what happens when you add a **second cable**!\n",
    "\n",
    "### Why Use Two Cables?\n",
    "\n",
    "```\n",
    "SINGLE CABLE:                          DUAL CABLE:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1 cable    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   2 cables   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Spark 1 â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚  Spark 2 â”‚    â”‚  Spark 1 â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚  Spark 2 â”‚\n",
    "â”‚          â”‚              â”‚          â”‚    â”‚          â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚          â”‚\n",
    "â”‚ Port 1   â”‚              â”‚ Port 1   â”‚    â”‚ Port 1   â”‚              â”‚ Port 1   â”‚\n",
    "â”‚ Port 2 â—‹ â”‚  (unused)    â”‚ â—‹ Port 2 â”‚    â”‚ Port 2   â”‚              â”‚ Port 2   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   ~100 Gbps                                  ~200 Gbps (aggregated)\n",
    "```\n",
    "\n",
    "**Benefits of dual cables:**\n",
    "- **Double bandwidth** for large data transfers\n",
    "- **Redundancy** - if one cable fails, traffic continues\n",
    "- **Lower latency** for parallel workloads (multiple streams)\n",
    "\n",
    "### Step 5.5.1: Record Your Single Cable Results\n",
    "\n",
    "Before adding the second cable, record your current performance:\n",
    "\n",
    "| Metric | Single Cable Result |\n",
    "|--------|---------------------|\n",
    "| `ib_write_bw` (MB/sec) | 88.261 Gbps_____________ |\n",
    "| `ib_write_lat` (Î¼s) | 1.99 us_____________ |\n",
    "| `iperf3` (Gbps) | 35 Gbps_____________ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c414bd",
   "metadata": {},
   "source": [
    "### Step 5.5.2: Connect the Second Cable\n",
    "\n",
    "**Physical Setup:**\n",
    "1. Power down is NOT required (InfiniBand supports hot-plug)\n",
    "2. Connect the second cable between the unused ports\n",
    "3. Wait ~10 seconds for the link to come up\n",
    "\n",
    "**Verify both links are active:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee55d5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Check both InfiniBand ports status\n",
      "============================================================\n",
      "Command: ibstat\n",
      "\n",
      "CA 'roceP2p1s0f0'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a17\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a17\n",
      "\t\tLink layer: Ethernet\n",
      "CA 'roceP2p1s0f1'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a18\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a18\n",
      "\t\tLink layer: Ethernet\n",
      "CA 'rocep1s0f0'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a13\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a13\n",
      "\t\tLink layer: Ethernet\n",
      "CA 'rocep1s0f1'\n",
      "\tCA type: MT4129\n",
      "\tNumber of ports: 1\n",
      "\tFirmware version: 28.45.4028\n",
      "\tHardware version: 0\n",
      "\tNode GUID: 0x30c59903003e6a14\n",
      "\tSystem image GUID: 0x30c59903003e6a13\n",
      "\tPort 1:\n",
      "\t\tState: Active\n",
      "\t\tPhysical state: LinkUp\n",
      "\t\tRate: 100\n",
      "\t\tBase lid: 0\n",
      "\t\tLMC: 0\n",
      "\t\tSM lid: 0\n",
      "\t\tCapability mask: 0x00010000\n",
      "\t\tPort GUID: 0x32c599fffe3e6a14\n",
      "\t\tLink layer: Ethernet\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that BOTH InfiniBand ports are now active\n",
    "run_command(\"ibstat\", \"Check both InfiniBand ports status\")\n",
    "\n",
    "# You should see TWO ports with State: Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81399ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for both InfiniBand interfaces...\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Both InfiniBand interface IPs\n",
      "============================================================\n",
      "Command: ip addr show | grep -E -A 2 'enp.*np[0-9]'\n",
      "\n",
      "3: enp1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:13 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.100.11/24 brd 192.168.100.255 scope global noprefixroute enp1s0f0np0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a13/64 scope link \n",
      "--\n",
      "4: enp1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:14 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.200.13/24 brd 192.168.200.255 scope global noprefixroute enp1s0f1np1\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a14/64 scope link \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find IP addresses for BOTH InfiniBand interfaces\n",
    "print(\"Looking for both InfiniBand interfaces...\\n\")\n",
    "run_command(\"ip addr show | grep -E -A 2 'enp.*np[0-9]'\", \"Both InfiniBand interface IPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c159e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set OTHER_MACHINE_IB_IP_2 to the second InfiniBand interface IP of the other machine\n",
      "  First interface IP:  192.168.100.10\n",
      "  Second interface IP: 192.168.200.12\n"
     ]
    }
   ],
   "source": [
    "# Configure the second interface IPs (if not already assigned)\n",
    "# You'll need to set IPs on BOTH machines for the second interface\n",
    "\n",
    "# Example IP scheme:\n",
    "#   Machine 1: enp1s0f0np0 = 192.168.100.10, enp1s0f1np1 = 192.168.101.10\n",
    "#   Machine 2: enp1s0f0np0 = 192.168.100.11, enp1s0f1np1 = 192.168.101.11\n",
    "\n",
    "OTHER_MACHINE_IB_IP_2 = \"192.168.200.12\"  # Fill in the second interface IP of the other machine\n",
    "\n",
    "print(\"Set OTHER_MACHINE_IB_IP_2 to the second InfiniBand interface IP of the other machine\")\n",
    "print(f\"  First interface IP:  {OTHER_MACHINE_IB_IP}\")\n",
    "print(f\"  Second interface IP: {OTHER_MACHINE_IB_IP_2 if OTHER_MACHINE_IB_IP_2 else '(not set)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ad675",
   "metadata": {},
   "source": [
    "### Step 5.5.3: Test Bandwidth on Each Link Individually\n",
    "\n",
    "First, test each cable separately to confirm both are working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fb767d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Testing FIRST InfiniBand link...\n",
      "ğŸ”´ Run 'ib_write_bw' on the other machine first!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Bandwidth Test - Link 1 (192.168.100.10)\n",
      "============================================================\n",
      "Command: ib_write_bw 192.168.100.10\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "                    RDMA_Write BW Test\n",
      " Dual-port       : OFF\t\tDevice         : rocep1s0f0\n",
      " Number of qps   : 1\t\tTransport type : IB\n",
      " Connection type : RC\t\tUsing SRQ      : OFF\n",
      " PCIe relax order: ON\n",
      " ibv_wr* API     : ON\n",
      " TX depth        : 128\n",
      " CQ Moderation   : 1\n",
      " Mtu             : 1024[B]\n",
      " Link type       : Ethernet\n",
      " GID index       : 3\n",
      " Max inline data : 0[B]\n",
      " rdma_cm QPs\t : OFF\n",
      " Data ex. method : Ethernet\n",
      "---------------------------------------------------------------------------------------\n",
      " local address: LID 0000 QPN 0x012d PSN 0xed8c02 RKey 0x184300 VAddr 0x00e9caca57d000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:100:11\n",
      " remote address: LID 0000 QPN 0x012d PSN 0xb45d13 RKey 0x184300 VAddr 0x00fe9d6d697000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:100:10\n",
      "---------------------------------------------------------------------------------------\n",
      " #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]\n",
      " 65536      5000             11032.63            11031.55\t\t   0.176505\n",
      "---------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test bandwidth on FIRST cable (same as before)\n",
    "# On OTHER machine, run: ib_write_bw\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"ğŸ”— Testing FIRST InfiniBand link...\")\n",
    "    print(\"ğŸ”´ Run 'ib_write_bw' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_bw {OTHER_MACHINE_IB_IP}\", \n",
    "                f\"Bandwidth Test - Link 1 ({OTHER_MACHINE_IB_IP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e56ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Testing SECOND InfiniBand link...\n",
      "ğŸ”´ Run 'ib_write_bw -d rocep1s0f1' on the other machine first!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ Bandwidth Test - Link 2 (192.168.200.12)\n",
      "============================================================\n",
      "Command: ib_write_bw -d rocep1s0f1 192.168.200.12\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "                    RDMA_Write BW Test\n",
      " Dual-port       : OFF\t\tDevice         : rocep1s0f1\n",
      " Number of qps   : 1\t\tTransport type : IB\n",
      " Connection type : RC\t\tUsing SRQ      : OFF\n",
      " PCIe relax order: ON\n",
      " ibv_wr* API     : ON\n",
      " TX depth        : 128\n",
      " CQ Moderation   : 1\n",
      " Mtu             : 1024[B]\n",
      " Link type       : Ethernet\n",
      " GID index       : 3\n",
      " Max inline data : 0[B]\n",
      " rdma_cm QPs\t : OFF\n",
      " Data ex. method : Ethernet\n",
      "---------------------------------------------------------------------------------------\n",
      " local address: LID 0000 QPN 0x022a PSN 0xc8f583 RKey 0x1c0300 VAddr 0x00fb76a0496000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:200:13\n",
      " remote address: LID 0000 QPN 0x022a PSN 0x193651 RKey 0x1c03ee VAddr 0x00fab7304fd000\n",
      " GID: 00:00:00:00:00:00:00:00:00:00:255:255:192:168:200:12\n",
      "---------------------------------------------------------------------------------------\n",
      " #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]\n",
      " 65536      5000             11032.72            11031.78\t\t   0.176508\n",
      "---------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test bandwidth on SECOND cable\n",
    "# On OTHER machine, run: ib_write_bw -d rocep1s0f1  (or the second device name)\n",
    "\n",
    "if OTHER_MACHINE_IB_IP_2:\n",
    "    print(\"ğŸ”— Testing SECOND InfiniBand link...\")\n",
    "    print(\"ğŸ”´ Run 'ib_write_bw -d rocep1s0f1' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_bw -d rocep1s0f1 {OTHER_MACHINE_IB_IP_2}\", \n",
    "                f\"Bandwidth Test - Link 2 ({OTHER_MACHINE_IB_IP_2})\")\n",
    "else:\n",
    "    print(\"âŒ Set OTHER_MACHINE_IB_IP_2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811a434",
   "metadata": {},
   "source": [
    "### Step 5.5.4: Test Aggregated Bandwidth (Both Links Simultaneously)\n",
    "\n",
    "Now the real test - run traffic on BOTH cables at the same time!\n",
    "\n",
    "**Method 1: Parallel ib_write_bw tests**\n",
    "\n",
    "This requires running tests in parallel. Open two terminals on each machine:\n",
    "\n",
    "```bash\n",
    "# On Machine 1 (server) - Terminal 1:\n",
    "ib_write_bw -d rocep1s0f0\n",
    "\n",
    "# On Machine 1 (server) - Terminal 2:\n",
    "ib_write_bw -d rocep1s0f1 -p 18516\n",
    "\n",
    "# On Machine 2 (client) - Terminal 1:\n",
    "ib_write_bw -d rocep1s0f0 192.168.100.10\n",
    "\n",
    "# On Machine 2 (client) - Terminal 2:\n",
    "ib_write_bw -d rocep1s0f1 192.168.200.12 -p 18516\n",
    "```\n",
    "\n",
    "**Method 2: iperf3 with parallel streams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07929ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Running PARALLEL iperf3 tests on BOTH links...\n",
      "ğŸ”´ Start two iperf3 servers on the other machine:\n",
      "   Terminal 1: iperf3 -s -p 5201\n",
      "   Terminal 2: iperf3 -s -p 5202\n",
      "\n",
      "Starting parallel tests...\n",
      "\n",
      "âœ… Both tests complete!\n",
      "Check the individual results above and add them together for total bandwidth.\n"
     ]
    }
   ],
   "source": [
    "# Run parallel iperf3 tests on both interfaces\n",
    "# Start servers on the other machine first:\n",
    "#   iperf3 -s -p 5201\n",
    "#   iperf3 -s -p 5202\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def run_iperf_test(ip, port, results, index):\n",
    "    \"\"\"Run iperf3 test and store result\"\"\"\n",
    "    try:\n",
    "        cmd = f\"iperf3 -c {ip} -p {port} -t 10 -J\"  # JSON output\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "        results[index] = result.stdout\n",
    "    except Exception as e:\n",
    "        results[index] = str(e)\n",
    "\n",
    "if OTHER_MACHINE_IB_IP and OTHER_MACHINE_IB_IP_2:\n",
    "    print(\"ğŸ”— Running PARALLEL iperf3 tests on BOTH links...\")\n",
    "    print(\"ğŸ”´ Start two iperf3 servers on the other machine:\")\n",
    "    print(\"   Terminal 1: iperf3 -s -p 5201\")\n",
    "    print(\"   Terminal 2: iperf3 -s -p 5202\\n\")\n",
    "    \n",
    "    results = [None, None]\n",
    "    \n",
    "    # Create threads for parallel execution\n",
    "    t1 = threading.Thread(target=run_iperf_test, args=(OTHER_MACHINE_IB_IP, 5201, results, 0))\n",
    "    t2 = threading.Thread(target=run_iperf_test, args=(OTHER_MACHINE_IB_IP_2, 5202, results, 1))\n",
    "    \n",
    "    print(\"Starting parallel tests...\")\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    \n",
    "    print(\"\\nâœ… Both tests complete!\")\n",
    "    print(\"Check the individual results above and add them together for total bandwidth.\")\n",
    "else:\n",
    "    print(\"âŒ Set both OTHER_MACHINE_IB_IP and OTHER_MACHINE_IB_IP_2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd23a44",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Dual Cable Results\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "| Configuration | Bandwidth (ib_write_bw) | Bandwidth (iperf3) |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Single Cable | ~12,000 MB/sec (~96 Gbps) | ~35 Gbps |\n",
    "| Dual Cable (each) | ~12,000 MB/sec each | ~35 Gbps each |\n",
    "| Dual Cable (total) | **~24,000 MB/sec (~192 Gbps)** | **~70 Gbps** |\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Linear Scaling**: With two independent links, bandwidth approximately doubles\n",
    "2. **Latency Unchanged**: Adding cables doesn't reduce latency for single messages\n",
    "3. **Parallelism Required**: To use both links, workloads must send traffic on both interfaces\n",
    "4. **NCCL Awareness**: NCCL (used by PyTorch/TensorFlow) automatically uses multiple InfiniBand ports when available\n",
    "\n",
    "### Step 5.5.5: Record Your Dual Cable Results\n",
    "\n",
    "| Metric | Single Cable | Dual Cable (Total) | Improvement |\n",
    "|--------|--------------|-------------------|-------------|\n",
    "| `ib_write_bw` (MB/sec) | 88.252 | 176.5 | 2x |\n",
    "| `iperf3` (Gbps) | _________ | _________ | _____x |\n",
    "| `ib_write_lat` (Î¼s) | 1.98 | 4 | 2x |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497b906",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ†š Part 6: Compare InfiniBand vs Ethernet\n",
    "\n",
    "Let's do a side-by-side comparison using `iperf3` (works on both InfiniBand and Ethernet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fcfcd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Checking for iperf3\n",
      "============================================================\n",
      "Command: which iperf3\n",
      "\n",
      "/usr/bin/iperf3\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ iperf3 version\n",
      "============================================================\n",
      "Command: iperf3 --version\n",
      "\n",
      "iperf 3.16 (cJSON 1.7.15)\n",
      "Linux spark-02 6.14.0-1015-nvidia #15-Ubuntu SMP PREEMPT_DYNAMIC Tue Nov 25 18:02:16 UTC 2025 aarch64\n",
      "Optional features available: CPU affinity setting, IPv6 flow label, SCTP, TCP congestion algorithm setting, sendfile / zerocopy, socket pacing, authentication, bind to device, support IPv4 don't fragment, POSIX threads\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if iperf3 is installed\n",
    "run_command(\"which iperf3\", \"Checking for iperf3\")\n",
    "run_command(\"iperf3 --version\", \"iperf3 version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f9ca8",
   "metadata": {},
   "source": [
    "### Step 6.1: Test InfiniBand with iperf3\n",
    "\n",
    "#### On Machine 1 (Server) - Run in terminal:\n",
    "```bash\n",
    "iperf3 -s\n",
    "```\n",
    "\n",
    "#### âš ï¸ Troubleshooting: \"Address already in use\"\n",
    "\n",
    "If you get the error `unable to start listener for connections: Address already in use`, the default port 5201 is busy. Try one of these:\n",
    "\n",
    "```bash\n",
    "# Option 1: Kill existing iperf3 process\n",
    "pkill iperf3\n",
    "iperf3 -s\n",
    "\n",
    "# Option 2: Use a different port (e.g., 5202)\n",
    "iperf3 -s -p 5202\n",
    "# Then on client use: iperf3 -c <IP> -p 5202\n",
    "```\n",
    "\n",
    "#### On Machine 2 (Client) - Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d9f88e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Running iperf3 bandwidth test over InfiniBand...\n",
      "\n",
      "ğŸ”´ MAKE SURE you ran 'iperf3 -s' on the other machine first!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ iperf3 Bandwidth Test (10 seconds)\n",
      "============================================================\n",
      "Command: iperf3 -c 192.168.100.10 -p 5202 -t 10\n",
      "\n",
      "Connecting to host 192.168.100.10, port 5202\n",
      "[  5] local 192.168.100.15 port 45220 connected to 192.168.100.10 port 5202\n",
      "[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n",
      "[  5]   0.00-1.00   sec  2.76 GBytes  23.7 Gbits/sec  680    740 KBytes       \n",
      "[  5]   1.00-2.00   sec  3.09 GBytes  26.5 Gbits/sec  1027    703 KBytes       \n",
      "[  5]   2.00-3.00   sec  2.72 GBytes  23.3 Gbits/sec  489    875 KBytes       \n",
      "[  5]   3.00-4.00   sec  2.64 GBytes  22.7 Gbits/sec  671    861 KBytes       \n",
      "[  5]   4.00-5.00   sec  2.64 GBytes  22.7 Gbits/sec  744    833 KBytes       \n",
      "[  5]   5.00-6.00   sec  2.94 GBytes  25.3 Gbits/sec  847    823 KBytes       \n",
      "[  5]   6.00-7.00   sec  3.01 GBytes  25.8 Gbits/sec  686   1.03 MBytes       \n",
      "[  5]   7.00-8.00   sec  2.71 GBytes  23.3 Gbits/sec  516    772 KBytes       \n",
      "[  5]   8.00-9.00   sec  3.48 GBytes  29.9 Gbits/sec  490    751 KBytes       \n",
      "[  5]   9.00-10.00  sec  3.08 GBytes  26.4 Gbits/sec  498   1007 KBytes       \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[ ID] Interval           Transfer     Bitrate         Retr\n",
      "[  5]   0.00-10.00  sec  29.1 GBytes  25.0 Gbits/sec  6648             sender\n",
      "[  5]   0.00-10.00  sec  29.1 GBytes  25.0 Gbits/sec                  receiver\n",
      "\n",
      "iperf Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iperf3 test over InfiniBand\n",
    "# On the OTHER machine, first run: iperf3 -s\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"â³ Running iperf3 bandwidth test over InfiniBand...\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE you ran 'iperf3 -s' on the other machine first!\\n\")\n",
    "    run_command(f\"iperf3 -c {OTHER_MACHINE_IB_IP} -p 5202 -t 10\", \n",
    "                \"iperf3 Bandwidth Test (10 seconds)\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daebf88",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding iperf3 vs ib_write_bw Results\n",
    "\n",
    "**Why is iperf3 showing ~35 Gbps instead of 100+ Gbps?**\n",
    "\n",
    "This is **expected behavior**! Here's why:\n",
    "\n",
    "| Tool | Protocol | What it tests | Expected Speed |\n",
    "|------|----------|---------------|----------------|\n",
    "| `ib_write_bw` | **Native RDMA** | True InfiniBand capability | 100-400 Gbps |\n",
    "| `iperf3` | **TCP/IP over IPoIB** | IP networking over InfiniBand | 20-50 Gbps |\n",
    "\n",
    "```\n",
    "ib_write_bw (RDMA):              iperf3 (TCP/IP):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Application   â”‚              â”‚   Application   â”‚\n",
    "â”‚        â†“        â”‚              â”‚        â†“        â”‚\n",
    "â”‚   Direct RDMA   â”‚ â† Fast!      â”‚   TCP/IP Stack  â”‚ â† Overhead!\n",
    "â”‚        â†“        â”‚              â”‚        â†“        â”‚\n",
    "â”‚  InfiniBand HW  â”‚              â”‚  IPoIB Layer    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚        â†“        â”‚\n",
    "                                 â”‚  InfiniBand HW  â”‚\n",
    "                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key insight:** \n",
    "- **iperf3 at ~35 Gbps** = Your IPoIB (IP-over-InfiniBand) is working well\n",
    "- **ib_write_bw at 100+ Gbps** = Your native RDMA is working at full speed\n",
    "\n",
    "**AI/ML frameworks (PyTorch, TensorFlow) use NCCL which leverages native RDMA, not TCP/IP!**\n",
    "\n",
    "ğŸ’¡ **To improve iperf3 results**, try parallel streams (still won't match RDMA):\n",
    "```bash\n",
    "# Server: iperf3 -s\n",
    "# Client: iperf3 -c <IP> -P 8 -t 10   # 8 parallel streams\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43846ea3",
   "metadata": {},
   "source": [
    "### Step 6.2: Now Test Regular Ethernet (for comparison)\n",
    "\n",
    "If your machines also have Ethernet connections, you can compare!\n",
    "\n",
    "First, find the Ethernet IP of the other machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a263643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ Ethernet interface IP addresses\n",
      "============================================================\n",
      "Command: ip addr show | grep -A 2 -E 'eth|enp'\n",
      "\n",
      "    link/ether 30:c5:99:3e:6a:12 brd ff:ff:ff:ff:ff:ff\n",
      "    altname enP7p1s0\n",
      "3: enp1s0f0np0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:13 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.100.11/24 brd 192.168.100.255 scope global noprefixroute enp1s0f0np0\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a13/64 scope link \n",
      "--\n",
      "4: enp1s0f1np1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000\n",
      "    link/ether 30:c5:99:3e:6a:14 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.200.13/24 brd 192.168.200.255 scope global noprefixroute enp1s0f1np1\n",
      "       valid_lft forever preferred_lft forever\n",
      "    inet6 fe80::32c5:99ff:fe3e:6a14/64 scope link \n",
      "--\n",
      "    link/ether 30:c5:99:3e:6a:17 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.100.15/24 brd 192.168.100.255 scope global noprefixroute enP2p1s0f0np0\n",
      "       valid_lft forever preferred_lft forever\n",
      "--\n",
      "    link/ether 30:c5:99:3e:6a:18 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 192.168.200.17/24 brd 192.168.200.255 scope global noprefixroute enP2p1s0f1np1\n",
      "       valid_lft forever preferred_lft forever\n",
      "--\n",
      "    link/ether 50:bb:b5:a3:b9:28 brd ff:ff:ff:ff:ff:ff\n",
      "    altname wlP9p1s0\n",
      "    inet 192.168.1.71/24 brd 192.168.1.255 scope global dynamic noprefixroute wlP9s9\n",
      "--\n",
      "    link/ether b2:f2:35:c8:c9:41 brd ff:ff:ff:ff:ff:ff\n",
      "    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0\n",
      "       valid_lft forever preferred_lft forever\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show Ethernet interfaces (not InfiniBand)\n",
    "run_command(\"ip addr show | grep -A 2 -E 'eth|enp'\", \"Ethernet interface IP addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e82bf31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Running iperf3 bandwidth test over Ethernet...\n",
      "\n",
      "ğŸ”´ MAKE SURE 'iperf3 -s' is still running on the other machine!\n",
      "\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ iperf3 Bandwidth Test over ETHERNET\n",
      "============================================================\n",
      "Command: iperf3 -c 192.168.100.10 -p 5202 -t 10\n",
      "\n",
      "Connecting to host 192.168.100.10, port 5202\n",
      "[  5] local 192.168.100.15 port 55740 connected to 192.168.100.10 port 5202\n",
      "[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n",
      "[  5]   0.00-1.00   sec  4.68 GBytes  40.1 Gbits/sec  695    837 KBytes       \n",
      "[  5]   1.00-2.00   sec  3.41 GBytes  29.3 Gbits/sec  865    660 KBytes       \n",
      "[  5]   2.00-3.00   sec  4.36 GBytes  37.4 Gbits/sec  1429    755 KBytes       \n",
      "[  5]   3.00-4.00   sec  3.76 GBytes  32.3 Gbits/sec  623    721 KBytes       \n",
      "[  5]   4.00-5.00   sec  3.96 GBytes  34.0 Gbits/sec  706    834 KBytes       \n",
      "[  5]   5.00-6.00   sec  5.36 GBytes  46.1 Gbits/sec  1484    885 KBytes       \n",
      "[  5]   6.00-7.00   sec  4.26 GBytes  36.6 Gbits/sec  1330    913 KBytes       \n",
      "[  5]   7.00-8.00   sec  3.47 GBytes  29.8 Gbits/sec  1351    846 KBytes       \n",
      "[  5]   8.00-9.00   sec  3.64 GBytes  31.3 Gbits/sec  359    755 KBytes       \n",
      "[  5]   9.00-10.00  sec  3.99 GBytes  34.2 Gbits/sec  1066    622 KBytes       \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - -\n",
      "[ ID] Interval           Transfer     Bitrate         Retr\n",
      "[  5]   0.00-10.00  sec  40.9 GBytes  35.1 Gbits/sec  9908             sender\n",
      "[  5]   0.00-10.00  sec  40.9 GBytes  35.1 Gbits/sec                  receiver\n",
      "\n",
      "iperf Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Set the Ethernet IP of the other machine for comparison\n",
    "OTHER_MACHINE_ETH_IP = \"192.168.100.10\"  # Example: \"10.0.0.2\" - FILL THIS IN!\n",
    "\n",
    "if OTHER_MACHINE_ETH_IP:\n",
    "    print(\"â³ Running iperf3 bandwidth test over Ethernet...\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE 'iperf3 -s' is still running on the other machine!\\n\")\n",
    "    run_command(f\"iperf3 -c {OTHER_MACHINE_ETH_IP} -p 5202 -t 10\", \n",
    "                \"iperf3 Bandwidth Test over ETHERNET\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Set OTHER_MACHINE_ETH_IP to compare Ethernet vs InfiniBand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1a811",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ˆ Part 7: Summary & Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Metric | Ethernet | InfiniBand | Improvement |\n",
    "|--------|----------|------------|-------------|\n",
    "| **Bandwidth** | 1-25 Gbps | 100-400 Gbps | **10-100x faster** |\n",
    "| **Latency** | 50-500 Î¼s | 1-2 Î¼s | **25-250x lower** |\n",
    "| **CPU Usage** | High (TCP/IP stack) | Low (RDMA bypass) | **Less CPU overhead** |\n",
    "\n",
    "### Why This Matters for AI/ML\n",
    "\n",
    "When training large models like GPT, LLaMA, etc.:\n",
    "\n",
    "1. **Distributed Training**: Multiple GPUs need to sync gradients millions of times\n",
    "2. **Data Loading**: Training data needs to flow to GPUs constantly\n",
    "3. **Model Parallelism**: Large models split across machines need fast communication\n",
    "\n",
    "**Without InfiniBand:** GPUs spend more time waiting for data than computing\n",
    "**With InfiniBand:** GPUs stay busy, training completes faster, money saved!\n",
    "\n",
    "### The RDMA Advantage\n",
    "\n",
    "InfiniBand uses **RDMA (Remote Direct Memory Access)**:\n",
    "\n",
    "```\n",
    "Traditional Ethernet:                    InfiniBand RDMA:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Application   â”‚                     â”‚   Application   â”‚\n",
    "â”‚        â†“        â”‚                     â”‚        â†“        â”‚\n",
    "â”‚   TCP/IP Stack  â”‚ â† CPU involved      â”‚   Direct to     â”‚\n",
    "â”‚        â†“        â”‚                     â”‚   Network Card  â”‚ â† CPU bypassed!\n",
    "â”‚  Network Card   â”‚                     â”‚                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "RDMA lets data go directly from one machine's memory to another's, **bypassing the CPU entirely**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837e5f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Quick Reference Commands\n",
    "\n",
    "Here are the most useful InfiniBand commands:\n",
    "\n",
    "| Command | What it does |\n",
    "|---------|-------------|\n",
    "| `ibstat` | Show InfiniBand device status |\n",
    "| `ibv_devinfo` | Detailed device information |\n",
    "| `ib_write_bw <IP>` | Bandwidth test |\n",
    "| `ib_write_lat <IP>` | Latency test |\n",
    "| `ibping -S` | Start ping server |\n",
    "| `ibping -c <IP>` | Ping another machine |\n",
    "| `perfquery` | Performance counters |\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Next Steps\n",
    "\n",
    "1. **Run this notebook on both DGX Spark boxes**\n",
    "2. **Record your results** in the cells below\n",
    "3. **Continue to Notebook 02** for GPU communication tests\n",
    "4. **Use the results** for your LinkedIn article!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c298fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¥ Part 8: NCCL Tests - What Your Inference Stack Actually Uses\n",
    "\n",
    "All the tests above (`ib_write_bw`, `iperf3`) measure raw network performance. But inference frameworks don't talk to the network directlyâ€”they use **NCCL** (NVIDIA Collective Communications Library).\n",
    "\n",
    "### What is NCCL?\n",
    "\n",
    "NCCL is NVIDIA's library for GPU-to-GPU communication. When you run tensor parallelism in vLLM, TensorRT-LLM, or any multi-GPU PyTorch workload, NCCL handles the actual data movement.\n",
    "\n",
    "**Why test NCCL separately?**\n",
    "- NCCL uses RDMA when available (InfiniBand or RoCE)\n",
    "- It optimizes communication patterns (all-reduce, all-gather, etc.)\n",
    "- The performance you see in NCCL tests = performance your inference gets\n",
    "\n",
    "```\n",
    "Your Inference Code\n",
    "       â†“\n",
    "   PyTorch / vLLM / TensorRT-LLM\n",
    "       â†“\n",
    "     NCCL  â† This is what we're testing\n",
    "       â†“\n",
    "   InfiniBand (RDMA)\n",
    "       â†“\n",
    "   Other GPUs\n",
    "```\n",
    "\n",
    "### What NCCL Operations Matter for Inference?\n",
    "\n",
    "| Operation | What it does | When it's used |\n",
    "|-----------|--------------|----------------|\n",
    "| **All-Reduce** | Sum values across all GPUs | Tensor parallelism (every layer!) |\n",
    "| **All-Gather** | Collect data from all GPUs | Gathering outputs |\n",
    "| **Broadcast** | Send from one GPU to all | Distributing inputs |\n",
    "| **Send/Recv** | Point-to-point transfer | Pipeline parallelism, KV-cache movement |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342ff9b",
   "metadata": {},
   "source": [
    "### Step 8.1: Check NCCL Installation and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e758f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking NCCL environment...\n",
      "\n",
      "âŒ nccl-tests not found. Install with:\n",
      "\n",
      "    # Clone and build nccl-tests\n",
      "    git clone https://github.com/NVIDIA/nccl-tests.git\n",
      "    cd nccl-tests\n",
      "    make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi\n",
      "\n",
      "    # Or if you have CUDA in a custom location:\n",
      "    make MPI=1 CUDA_HOME=/usr/local/cuda MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi\n",
      "\n",
      "    # Add to PATH\n",
      "    export PATH=$PATH:$(pwd)/build\n",
      "    \n",
      "\n",
      "ğŸ” Checking PyTorch NCCL backend...\n",
      "âŒ PyTorch not installed\n"
     ]
    }
   ],
   "source": [
    "# Check NCCL version and if it detects InfiniBand\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸ” Checking NCCL environment...\\n\")\n",
    "\n",
    "# Check if nccl-tests is available\n",
    "nccl_test_path = subprocess.run(\"which all_reduce_perf\", shell=True, capture_output=True, text=True)\n",
    "if nccl_test_path.returncode == 0:\n",
    "    print(f\"âœ… nccl-tests found: {nccl_test_path.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"âŒ nccl-tests not found. Install with:\")\n",
    "    print(\"\"\"\n",
    "    # Clone and build nccl-tests\n",
    "    git clone https://github.com/NVIDIA/nccl-tests.git\n",
    "    cd nccl-tests\n",
    "    make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi\n",
    "    \n",
    "    # Or if you have CUDA in a custom location:\n",
    "    make MPI=1 CUDA_HOME=/usr/local/cuda MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi\n",
    "    \n",
    "    # Add to PATH\n",
    "    export PATH=$PATH:$(pwd)/build\n",
    "    \"\"\")\n",
    "\n",
    "# Check PyTorch NCCL\n",
    "print(\"\\nğŸ” Checking PyTorch NCCL backend...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… GPU count: {torch.cuda.device_count()}\")\n",
    "        print(f\"âœ… NCCL available: {torch.distributed.is_nccl_available()}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950d05f",
   "metadata": {},
   "source": [
    "### Step 8.2: Run NCCL All-Reduce Test (Single Node)\n",
    "\n",
    "First, let's test NCCL communication between GPUs on the SAME machine. This establishes a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89b3e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Running NCCL All-Reduce test (single node)...\n",
      "This tests GPU-to-GPU communication WITHIN this machine.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'run_command' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mThis tests GPU-to-GPU communication WITHIN this machine.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run all_reduce_perf if available\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43mrun_command\u001b[49m(\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mall_reduce_perf -b 8 -e 128M -f 2 -g 2\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mNCCL All-Reduce Performance (2 GPUs, single node)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33mIf all_reduce_perf is not found, you can test with PyTorch directly.\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33mSee the next cell for a PyTorch-based NCCL test.\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_command' is not defined"
     ]
    }
   ],
   "source": [
    "# NCCL All-Reduce test on a single node (between local GPUs)\n",
    "# This uses NVLink/PCIe, not InfiniBand\n",
    "\n",
    "print(\"ğŸ”„ Running NCCL All-Reduce test (single node)...\")\n",
    "print(\"This tests GPU-to-GPU communication WITHIN this machine.\\n\")\n",
    "\n",
    "# Run all_reduce_perf if available\n",
    "result = run_command(\n",
    "    \"all_reduce_perf -b 8 -e 128M -f 2 -g 2\", \n",
    "    \"NCCL All-Reduce Performance (2 GPUs, single node)\"\n",
    ")\n",
    "\n",
    "if not result:\n",
    "    print(\"\"\"\n",
    "If all_reduce_perf is not found, you can test with PyTorch directly.\n",
    "See the next cell for a PyTorch-based NCCL test.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdefb36",
   "metadata": {},
   "source": [
    "### Step 8.3: Run NCCL Multi-Node Test (Over InfiniBand)\n",
    "\n",
    "**This is the real test** - communication between GPUs on DIFFERENT machines over InfiniBand.\n",
    "\n",
    "You need to run this with `mpirun` to coordinate processes across both machines.\n",
    "\n",
    "#### Understanding the mpirun Command\n",
    "\n",
    "```bash\n",
    "mpirun -np 4 \\\n",
    "    --host spark1:2,spark2:2 \\\n",
    "    -x NCCL_DEBUG=INFO \\\n",
    "    -x NCCL_IB_DISABLE=0 \\\n",
    "    all_reduce_perf -b 8 -e 128M -f 2 -g 1\n",
    "```\n",
    "\n",
    "**What each part means:**\n",
    "\n",
    "| Part | Meaning |\n",
    "|------|---------|\n",
    "| `-np 4` | Total number of processes (2 per machine Ã— 2 machines) |\n",
    "| `--host spark1:2,spark2:2` | Run 2 processes on spark1, 2 on spark2 |\n",
    "| `-x NCCL_DEBUG=INFO` | Show NCCL's transport selection (important!) |\n",
    "| `-x NCCL_IB_DISABLE=0` | Ensure InfiniBand is enabled |\n",
    "| `-b 8 -e 128M -f 2` | Test sizes from 8 bytes to 128MB, doubling each time |\n",
    "| `-g 1` | Each process uses 1 GPU |\n",
    "\n",
    "#### What to Look For in the Output\n",
    "\n",
    "**1. Transport Selection (in NCCL_DEBUG output):**\n",
    "```\n",
    "NCCL INFO NET/IB : Using [0]mlx5_0:1/IB/SHARP [1]mlx5_1:1/IB/SHARP\n",
    "```\n",
    "This means NCCL found and is using InfiniBand! \n",
    "\n",
    "If you see `NET/Socket` instead, it's falling back to TCP (bad).\n",
    "\n",
    "**2. Bandwidth Results:**\n",
    "```\n",
    "#       size    time   algbw   busbw\n",
    "     8388608   0.51   16.5    31.0\n",
    "```\n",
    "- **algbw** = Algorithm bandwidth (what the operation achieved)\n",
    "- **busbw** = Bus bandwidth (normalized for the operation type)\n",
    "\n",
    "For all-reduce on 4 GPUs over InfiniBand, expect **busbw of 10-40 GB/s** depending on message size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6561c3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OTHER_MACHINE_IB_IP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msocket\u001b[39;00m\n\u001b[32m      4\u001b[39m hostname = socket.gethostname()\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m other_host = \u001b[43mOTHER_MACHINE_IB_IP\u001b[49m  \u001b[38;5;66;03m# Use IP if hostnames aren't in /etc/hosts\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸ“‹ NCCL MULTI-NODE TEST INSTRUCTIONS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'OTHER_MACHINE_IB_IP' is not defined"
     ]
    }
   ],
   "source": [
    "# Generate the mpirun command for multi-node NCCL test\n",
    "import socket\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "other_host = OTHER_MACHINE_IB_IP  # Use IP if hostnames aren't in /etc/hosts\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹ NCCL MULTI-NODE TEST INSTRUCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "This test requires running mpirun from a terminal (not this notebook)\n",
    "because it needs to coordinate processes across both machines.\n",
    "\n",
    "PREREQUISITE: Passwordless SSH between machines\n",
    "  ssh-keygen -t rsa  # if you don't have keys\n",
    "  ssh-copy-id {other_host}\n",
    "\n",
    "STEP 1: Run this command from {hostname}:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "mpirun -np 4 \\\\\n",
    "    --host {hostname}:2,{other_host}:2 \\\\\n",
    "    --mca btl_tcp_if_include enp1s0f0np0 \\\\\n",
    "    -x NCCL_DEBUG=INFO \\\\\n",
    "    -x NCCL_IB_DISABLE=0 \\\\\n",
    "    -x NCCL_IB_HCA=mlx5_0,mlx5_1 \\\\\n",
    "    all_reduce_perf -b 8 -e 128M -f 2 -g 1\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "STEP 2: Look for these key lines in the output:\n",
    "\n",
    "  âœ… GOOD - Using InfiniBand:\n",
    "     \"NCCL INFO NET/IB : Using [0]mlx5_0:1/IB\"\n",
    "  \n",
    "  âŒ BAD - Falling back to TCP:\n",
    "     \"NCCL INFO NET/Socket\"\n",
    "\n",
    "STEP 3: Check the busbw (bus bandwidth) column:\n",
    "  - 10-20 GB/s = Working, but might be suboptimal\n",
    "  - 20-40 GB/s = Good InfiniBand performance\n",
    "  - < 5 GB/s = Probably using TCP fallback\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265cea4",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding NCCL Test Results\n",
    "\n",
    "**Sample Output Explained:**\n",
    "\n",
    "```\n",
    "#                                                       out-of-place                       in-place          \n",
    "#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n",
    "#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       \n",
    "           8             2     float     sum      -1    33.22    0.00    0.00      0    32.88    0.00    0.00      0\n",
    "         256             64    float     sum      -1    34.56    0.01    0.01      0    34.12    0.01    0.01      0\n",
    "        4096           1024    float     sum      -1    36.44    0.11    0.21      0    35.89    0.11    0.22      0\n",
    "       65536          16384    float     sum      -1    48.23    1.36    2.55      0    47.67    1.37    2.58      0\n",
    "     1048576         262144    float     sum      -1    89.45   11.72   21.98      0    88.92   11.79   22.11      0\n",
    "    16777216        4194304    float     sum      -1   523.67   32.04   60.08      0   521.23   32.19   60.36      0\n",
    "   134217728       33554432    float     sum      -1  3845.2    34.91   65.45      0  3832.1    35.03   65.68      0\n",
    "```\n",
    "\n",
    "**Key Columns:**\n",
    "| Column | What it means |\n",
    "|--------|---------------|\n",
    "| `size` | Message size in bytes |\n",
    "| `time` | Time to complete operation (microseconds) |\n",
    "| `algbw` | Algorithm bandwidth = size / time |\n",
    "| `busbw` | Bus bandwidth = algbw Ã— (2Ã—(n-1)/n) for all-reduce |\n",
    "\n",
    "**Why busbw > algbw?**\n",
    "\n",
    "All-reduce sends data in both directions. For 4 GPUs, the bus bandwidth correction factor is 1.875Ã—. This gives you a better measure of actual link utilization.\n",
    "\n",
    "**Healthy Results for InfiniBand:**\n",
    "- Small messages (< 1KB): Low bandwidth (latency-bound) - this is normal\n",
    "- Large messages (> 1MB): busbw should reach **20-40+ GB/s** over InfiniBand\n",
    "- If busbw stays below 5 GB/s even for large messages, NCCL isn't using InfiniBand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d1b11",
   "metadata": {},
   "source": [
    "### Step 8.4: Alternative - PyTorch NCCL Test\n",
    "\n",
    "If nccl-tests isn't installed, you can verify NCCL over InfiniBand with PyTorch directly.\n",
    "\n",
    "Save this as `nccl_test.py` and run with `torchrun`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc416fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple PyTorch NCCL benchmark script\n",
    "nccl_test_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple NCCL All-Reduce benchmark using PyTorch.\n",
    "Tests GPU-to-GPU communication bandwidth.\n",
    "\n",
    "Run with:\n",
    "  # Single node (2 GPUs):\n",
    "  torchrun --nproc_per_node=2 nccl_test.py\n",
    "  \n",
    "  # Multi-node (requires MASTER_ADDR, MASTER_PORT):\n",
    "  # On node 1:\n",
    "  torchrun --nproc_per_node=2 --nnodes=2 --node_rank=0 \\\\\n",
    "           --master_addr=<node1_ip> --master_port=29500 nccl_test.py\n",
    "  # On node 2:\n",
    "  torchrun --nproc_per_node=2 --nnodes=2 --node_rank=1 \\\\\n",
    "           --master_addr=<node1_ip> --master_port=29500 nccl_test.py\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def benchmark_all_reduce(size_mb, num_iterations=100):\n",
    "    \"\"\"Benchmark all-reduce for a given tensor size.\"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Create tensor\n",
    "    num_elements = (size_mb * 1024 * 1024) // 4  # float32 = 4 bytes\n",
    "    tensor = torch.randn(num_elements, device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        dist.all_reduce(tensor)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_iterations):\n",
    "        dist.all_reduce(tensor)\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    # All-reduce moves 2*(n-1)/n * size data\n",
    "    bytes_moved = size_mb * 1024 * 1024 * 2 * (world_size - 1) / world_size\n",
    "    total_bytes = bytes_moved * num_iterations\n",
    "    bandwidth_gbps = (total_bytes / elapsed) / 1e9\n",
    "    \n",
    "    return bandwidth_gbps, elapsed / num_iterations * 1000  # GB/s, ms\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"NCCL All-Reduce Benchmark\")\n",
    "        print(f\"World size: {world_size} GPUs\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"{'Size':>10} {'Bandwidth':>15} {'Latency':>15}\")\n",
    "        print(f\"{'(MB)':>10} {'(GB/s)':>15} {'(ms)':>15}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    for size_mb in [1, 8, 32, 128, 512]:\n",
    "        bw, latency = benchmark_all_reduce(size_mb)\n",
    "        if rank == 0:\n",
    "            print(f\"{size_mb:>10} {bw:>15.2f} {latency:>15.3f}\")\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\\\nExpected bandwidth over InfiniBand: 20-40+ GB/s for large messages\")\n",
    "        print(\"If seeing < 5 GB/s, check NCCL_DEBUG=INFO output for transport type\")\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the script\n",
    "script_path = \"/home/nvidia/src/github.com/elizabetht/spark/infiniband-tutorial/nccl_test.py\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(nccl_test_script)\n",
    "\n",
    "print(f\"âœ… Saved NCCL test script to: {script_path}\")\n",
    "print(\"\"\"\n",
    "To run single-node test (2 GPUs on this machine):\n",
    "  torchrun --nproc_per_node=2 nccl_test.py\n",
    "\n",
    "To run multi-node test over InfiniBand:\n",
    "  # On machine 1 (master):\n",
    "  NCCL_DEBUG=INFO torchrun --nproc_per_node=2 --nnodes=2 --node_rank=0 \\\\\n",
    "      --master_addr={0} --master_port=29500 nccl_test.py\n",
    "  \n",
    "  # On machine 2:\n",
    "  NCCL_DEBUG=INFO torchrun --nproc_per_node=2 --nnodes=2 --node_rank=1 \\\\\n",
    "      --master_addr={0} --master_port=29500 nccl_test.py\n",
    "\"\"\".format(OTHER_MACHINE_IB_IP if 'OTHER_MACHINE_IB_IP' in dir() else '<master_ip>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcf250",
   "metadata": {},
   "source": [
    "### ğŸ”§ Troubleshooting NCCL Over InfiniBand\n",
    "\n",
    "**NCCL isn't using InfiniBand?** Check these environment variables:\n",
    "\n",
    "```bash\n",
    "# Force NCCL to use InfiniBand\n",
    "export NCCL_IB_DISABLE=0\n",
    "\n",
    "# Specify which InfiniBand devices to use  \n",
    "export NCCL_IB_HCA=mlx5_0,mlx5_1\n",
    "\n",
    "# Enable debug output to see transport selection\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_DEBUG_SUBSYS=INIT,NET\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|---------|--------------|-----|\n",
    "| \"NET/Socket\" in debug | IB not detected | Check `ibstat`, ensure ports are Active |\n",
    "| Low bandwidth (< 5 GB/s) | Using TCP fallback | Set `NCCL_IB_DISABLE=0` |\n",
    "| \"No route to host\" | Network config | Ensure IB IPs are reachable (`ping`) |\n",
    "| \"Bootstrap failed\" | Firewall | Open ports 29500+ or disable firewall |\n",
    "\n",
    "### Step 8.5: Record Your NCCL Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your NCCL test results\n",
    "nccl_results = {\n",
    "    \"test_type\": \"all_reduce_perf / PyTorch\",  # Which test did you run?\n",
    "    \"num_gpus\": 4,  # Total GPUs across all nodes\n",
    "    \"num_nodes\": 2,  # Number of machines\n",
    "    \"transport_used\": \"IB\",  # Check NCCL_DEBUG output: \"IB\" or \"Socket\"?\n",
    "    \"bandwidth_128mb_gbps\": 0,  # busbw for 128MB message\n",
    "    \"notes\": \"\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š NCCL Test Results:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in nccl_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "Fill in your results above after running the NCCL tests!\n",
    "\n",
    "Key question: Did NCCL_DEBUG show \"NET/IB\" or \"NET/Socket\"?\n",
    "- NET/IB = InfiniBand is working âœ…\n",
    "- NET/Socket = Falling back to TCP âŒ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24135551",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Record Your Results Here\n",
    "\n",
    "Fill in your actual test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your results here for reference\n",
    "my_results = {\n",
    "    \"machine_1_hostname\": \"spark-01\",  # Fill in\n",
    "    \"machine_2_hostname\": \"spark-02\",  # Fill in\n",
    "    \"infiniband_bandwidth_gbps\":  96.3,  # From ib_write_bw test\n",
    "    \"infiniband_latency_us\": 1.96,  # From ib_write_lat test\n",
    "    \"ethernet_bandwidth_gbps\": 42.25,  # From iperf3 over ethernet (if tested)\n",
    "    \"notes\": \"\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Your recorded results:\")\n",
    "for key, value in my_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
