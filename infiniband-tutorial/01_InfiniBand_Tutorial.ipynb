{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1e6409",
   "metadata": {},
   "source": [
    "# ğŸš€ RDMA Networking Tutorial for DGX Spark Systems\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "This notebook will teach you:\n",
    "1. **What is RDMA?** - The basics explained simply (InfiniBand vs RoCE)\n",
    "2. **How to check your hardware** - See what's connected\n",
    "3. **Run speed tests** - Compare RDMA vs TCP/IP\n",
    "4. **Understand the results** - Why RDMA matters for AI/ML\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810d0dc2",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 0: Install Required Packages\n",
    "\n",
    "Before we can use InfiniBand diagnostic tools, we need to install them. Run the cells below to install all necessary packages.\n",
    "\n",
    "**What we're installing:**\n",
    "- `infiniband-diags` - Diagnostic tools (`ibstat`, `ibv_devinfo`, `perfquery`, etc.)\n",
    "- `rdma-core` - Core RDMA libraries\n",
    "- `ibverbs-utils` - InfiniBand verbs utilities\n",
    "- `perftest` - Performance testing tools (`ib_write_bw`, `ib_write_lat`)\n",
    "- `iperf3` - Network bandwidth testing (works on both Ethernet and InfiniBand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76b6c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.1: Install InfiniBand diagnostic and performance tools\n",
    "# \n",
    "# âš ï¸ IMPORTANT: Run these commands in a TERMINAL (not in this notebook)\n",
    "# The notebook cannot prompt for your sudo password.\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘  ğŸ“¦ INSTALLATION INSTRUCTIONS                                        â•‘\n",
    "â•‘                                                                      â•‘\n",
    "â•‘  Copy and paste these commands into a terminal window:               â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Step 1: Update package list\n",
    "sudo apt-get update\n",
    "\n",
    "# Step 2: Install InfiniBand tools\n",
    "sudo apt-get install -y infiniband-diags rdma-core ibverbs-utils perftest iperf3\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "After running the commands above in a terminal, come back here and run\n",
    "the NEXT cell (Step 0.2) to verify the installation was successful.\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98966c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0.2: Verify installation - check that all tools are available\n",
    "\n",
    "import shutil\n",
    "\n",
    "tools_to_check = [\n",
    "    (\"ibstat\", \"InfiniBand status tool\"),\n",
    "    (\"ibv_devinfo\", \"InfiniBand device info\"),\n",
    "    (\"ib_write_bw\", \"Bandwidth test tool\"),\n",
    "    (\"ib_write_lat\", \"Latency test tool\"),\n",
    "    (\"iperf3\", \"Network bandwidth tester\"),\n",
    "    (\"perfquery\", \"Performance query tool\"),\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Checking installed tools...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_found = True\n",
    "for tool, description in tools_to_check:\n",
    "    path = shutil.which(tool)\n",
    "    if path:\n",
    "        print(f\"âœ… {tool:15} - {description}\")\n",
    "    else:\n",
    "        print(f\"âŒ {tool:15} - NOT FOUND\")\n",
    "        all_found = False\n",
    "\n",
    "print(\"=\" * 50)\n",
    "if all_found:\n",
    "    print(\"ğŸ‰ All tools installed! You're ready to proceed.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Some tools missing. Re-run the installation cell above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd4009f",
   "metadata": {},
   "source": [
    "### âš ï¸ Troubleshooting Installation\n",
    "\n",
    "If installation fails, try running these commands manually in a terminal:\n",
    "\n",
    "```bash\n",
    "# Update and install\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y infiniband-diags rdma-core ibverbs-utils perftest iperf3\n",
    "\n",
    "# On some systems, you may also need:\n",
    "sudo apt-get install -y libibverbs-dev librdmacm-dev\n",
    "```\n",
    "\n",
    "**Common issues:**\n",
    "- \"Package not found\" â†’ Your system may use different package names. Try `apt search infiniband`\n",
    "- \"Permission denied\" â†’ Make sure you have sudo access\n",
    "- Already installed â†’ Great! Move on to Part 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7842ac",
   "metadata": {},
   "source": [
    "## ğŸ“š Part 1: Understanding RDMA, InfiniBand, and RoCE\n",
    "\n",
    "### What is RDMA?\n",
    "\n",
    "**RDMA (Remote Direct Memory Access)** lets network cards read/write directly to application memoryâ€”bypassing the CPU entirely. This is what makes high-speed GPU networking possible.\n",
    "\n",
    "There are two main ways to get RDMA:\n",
    "\n",
    "| Technology | InfiniBand | RoCE (RDMA over Converged Ethernet) |\n",
    "|------------|------------|-------------------------------------|\n",
    "| Physical Layer | InfiniBand fabric | Standard Ethernet |\n",
    "| Speed | 100-400 Gbps | 100-400 Gbps |\n",
    "| Latency | ~1-2 microseconds | ~1-2 microseconds |\n",
    "| Switches Required | InfiniBand switches | Regular Ethernet switches |\n",
    "| Configuration | Works out of the box | Needs PFC/ECN on switches |\n",
    "| **DGX Spark uses** | âŒ | âœ… **Yes** |\n",
    "\n",
    "### Wait, DGX Spark Uses RoCE?\n",
    "\n",
    "Yes! If you run `ibv_devinfo`, you'll see device names like `roceP2p1s0f0`. That `roce` prefix tells you this is RoCE, not native InfiniBand.\n",
    "\n",
    "**The good news**: It doesn't matter for your workloads. Same RDMA benefits, same tools (`ib_write_bw`, etc.), same performance. NCCL doesn't care whether it's InfiniBand or RoCEâ€”it just sees RDMA.\n",
    "\n",
    "### Why Does This Matter for AI/ML?\n",
    "\n",
    "When running large AI models across multiple GPUs/machines:\n",
    "- GPUs need to **constantly share data** (KV-cache, activations, model shards)\n",
    "- Slow connections = GPUs waiting = wasted expensive hardware\n",
    "- RDMA keeps GPUs fed with data at maximum speed\n",
    "\n",
    "### Your Setup: 2 DGX Spark Boxes + 2 RoCE Links\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           RoCE/RDMA Links         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   DGX Spark 1   â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚   DGX Spark 2   â”‚\n",
    "â”‚                 â”‚ â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• â”‚                 â”‚\n",
    "â”‚  [GPU][GPU]     â”‚      (Super fast connection!)    â”‚  [GPU][GPU]     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Note**: The tools are called `ib_*` (e.g., `ib_write_bw`) because they use the InfiniBand verbs APIâ€”but they work identically with RoCE.\n",
    "\n",
    "---\n",
    "\n",
    "Now let's check what hardware you have! Run the cells below one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b7891",
   "metadata": {},
   "source": [
    "## ğŸ” Part 2: Check Your System\n",
    "\n",
    "First, let's import the libraries we need and create some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd2fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports - run this first!\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def run_command(cmd, description=\"\"):\n",
    "    \"\"\"Run a shell command and display the output nicely\"\"\"\n",
    "    if description:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“‹ {description}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Command: {cmd}\\n\")\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "        if result.stdout:\n",
    "            print(result.stdout)\n",
    "        if result.stderr and result.returncode != 0:\n",
    "            print(f\"âš ï¸ Error: {result.stderr}\")\n",
    "        return result.returncode == 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â±ï¸ Command timed out\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"âœ… Helper functions loaded! Ready to explore your system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48245af1",
   "metadata": {},
   "source": [
    "### Step 2.1: What machine am I on?\n",
    "\n",
    "Let's first check which DGX Spark box you're currently using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6775a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your hostname and basic system info\n",
    "run_command(\"hostname\", \"Your machine's name (hostname)\")\n",
    "run_command(\"uname -a\", \"Operating system details\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d15f2",
   "metadata": {},
   "source": [
    "### Step 2.2: Do I have InfiniBand hardware?\n",
    "\n",
    "Let's check if InfiniBand devices are detected on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for InfiniBand devices\n",
    "run_command(\"lspci | grep -i 'infiniband\\|mellanox\\|connectx'\", \n",
    "            \"Looking for InfiniBand/Mellanox hardware in PCI devices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936d9f3",
   "metadata": {},
   "source": [
    "**What to expect:** You should see something like \"Mellanox ConnectX-7\" or similar. This is your RDMA network card (used for both InfiniBand and RoCE)!\n",
    "\n",
    "### Step 2.3: Check RDMA Device Status\n",
    "\n",
    "Now let's see the status of your RDMA connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01433081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check InfiniBand status - this is the main command!\n",
    "run_command(\"ibstat\", \"InfiniBand device status (ibstat)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821fd4f",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding ibstat Output\n",
    "\n",
    "Here's what to look for:\n",
    "\n",
    "| Field | Good Value | Meaning |\n",
    "|-------|------------|----------|\n",
    "| **State** | `Active` | Cable connected and working! |\n",
    "| **State** | `Down` | No cable or other end not connected |\n",
    "| **Physical state** | `LinkUp` | Physical connection established |\n",
    "| **Rate** | `100` or higher | Speed in Gbps |\n",
    "\n",
    "**If you see `Active` and `LinkUp` â†’ Your InfiniBand cables are connected correctly! ğŸ‰**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38cdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More detailed view of InfiniBand devices\n",
    "run_command(\"ibv_devinfo\", \"Detailed InfiniBand device information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fab0ab",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding ibv_devinfo Output\n",
    "\n",
    "Look at the device names in the output above. You'll see something like:\n",
    "\n",
    "- `roceP2p1s0f0` â†’ **This is RoCE** (RDMA over Converged Ethernet)\n",
    "- `mlx5_0` â†’ Generic Mellanox/NVIDIA device name\n",
    "- `ib0` â†’ Traditional InfiniBand device name (rare on modern systems)\n",
    "\n",
    "**If you see `roce` in the device name**: Your system uses RoCE, not native InfiniBand. This is normal for DGX Spark! All the same tools work, and performance is identical.\n",
    "\n",
    "The key fields to check:\n",
    "| Field | What to Look For |\n",
    "|-------|------------------|\n",
    "| `transport` | `InfiniBand` (even for RoCE, the API is the same) |\n",
    "| `link_layer` | `Ethernet` (for RoCE) or `InfiniBand` (for native IB) |\n",
    "| `state` | `PORT_ACTIVE` = ready to use |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d951074",
   "metadata": {},
   "source": [
    "### Step 2.4: Check Network Interfaces\n",
    "\n",
    "InfiniBand creates network interfaces just like Ethernet. Let's see all your network interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83eecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all network interfaces\n",
    "run_command(\"ip link show\", \"All network interfaces on this system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad00637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show IP addresses assigned to interfaces\n",
    "run_command(\"ip addr show\", \"IP addresses assigned to each interface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b9f11",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Network Interfaces\n",
    "\n",
    "You'll likely see several types:\n",
    "\n",
    "| Interface Name | Type | Description |\n",
    "|----------------|------|-------------|\n",
    "| `lo` | Loopback | Internal (127.0.0.1) |\n",
    "| `eth0`, `enp...` | Ethernet | Regular network cable |\n",
    "| `ib0`, `ibp...` | **InfiniBand** | Traditional IB interface names (rare now) |\n",
    "| `enp*np*` (e.g., `enp1s0f0np0`) | **RoCE/RDMA** | Predictable naming for RDMA interfaces |\n",
    "| `docker0`, `br-...` | Virtual | Docker/container networks |\n",
    "\n",
    "**On DGX Spark**: Look for interfaces with the `np` suffix (like `enp1s0f0np0`) - these are your RoCE/RDMA interfaces. They look like Ethernet interfaces but support RDMA!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9639045",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¯ Part 3: Understanding Your Two DGX Spark Boxes\n",
    "\n",
    "Before we run tests, we need to know:\n",
    "1. The **IP address** of THIS machine (on the RoCE/RDMA interface)\n",
    "2. The **IP address** of the OTHER machine (on the RoCE/RDMA interface)\n",
    "\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4750472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find InfiniBand IP addresses specifically\n",
    "print(\"Looking for InfiniBand interfaces (ib0, ib1, enp*np*, etc.)...\\n\")\n",
    "run_command(\"ip addr show | grep -E -A 2 'ib[0-9]|enp.*np[0-9]'\", \"InfiniBand interface IP addresses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7a29b",
   "metadata": {},
   "source": [
    "### ğŸ“ Write Down Your IPs!\n",
    "\n",
    "Fill in these values based on what you see above:\n",
    "\n",
    "**This machine (DGX Spark 2):**\n",
    "- Hostname: `_spark-02___________`\n",
    "- InfiniBand IP (e.g., enp1s0f0np0): `_192.168.100.11___________`\n",
    "\n",
    "**Other machine (DGX Spark 1):**\n",
    "- Hostname: `_spark-01___________`  \n",
    "- InfiniBand IP (e.g., enp1s0f0np0): `192.168.100.10____________`\n",
    "\n",
    "*(You'll need to run this notebook on BOTH machines to get both IPs, or SSH to the other machine)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bdedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE THIS: Set the IP of the OTHER DGX Spark box\n",
    "# After running the cells above on both machines, fill in the other machine's InfiniBand IP\n",
    "\n",
    "OTHER_MACHINE_IB_IP = \"192.168.100.10\"  # Example: \"192.168.1.2\" - FILL THIS IN!\n",
    "\n",
    "if not OTHER_MACHINE_IB_IP:\n",
    "    print(\"âš ï¸ Please set OTHER_MACHINE_IB_IP above!\")\n",
    "    print(\"   Run the previous cell on your other DGX Spark to find its InfiniBand IP.\")\n",
    "else:\n",
    "    print(f\"âœ… Will test connection to: {OTHER_MACHINE_IB_IP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f004a9f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ§ª Part 4: Basic Connectivity Test\n",
    "\n",
    "Let's make sure the two machines can talk to each other over InfiniBand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46f581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ping test over InfiniBand\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    run_command(f\"ping -c 5 {OTHER_MACHINE_IB_IP}\", \n",
    "                f\"Pinging other DGX Spark at {OTHER_MACHINE_IB_IP}\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb31ed6",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Ping Results\n",
    "\n",
    "Look at the **time** values:\n",
    "- `time=0.1 ms` â†’ Excellent! This is InfiniBand speed\n",
    "- `time=1-5 ms` â†’ Good, still fast\n",
    "- `time=10+ ms` â†’ Might be going through Ethernet instead\n",
    "\n",
    "**InfiniBand ping times should be very low (under 1ms typically)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89dc4af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š Part 5: Speed Tests - The Fun Part!\n",
    "\n",
    "Now let's measure how fast InfiniBand really is! We'll use tools called `ib_write_bw` and `ib_write_lat`.\n",
    "\n",
    "### How These Tests Work\n",
    "\n",
    "```\n",
    "Machine 1 (Server)          Machine 2 (Client)\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Listening...   â”‚ â—„â”€â”€â”€â”€â”€â”€ â”‚  Connect & Test â”‚\n",
    "â”‚  Ready to       â”‚ â”€â”€â”€â”€â”€â”€â–º â”‚  Send data back â”‚\n",
    "â”‚  receive data   â”‚         â”‚  and forth      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Important:** You need to run commands on BOTH machines!\n",
    "1. Start the **server** on one machine first\n",
    "2. Then start the **client** on the other machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f40ac",
   "metadata": {},
   "source": [
    "### Step 5.1: Bandwidth Test\n",
    "\n",
    "**Bandwidth** = How much data can flow per second (like water through a pipe)\n",
    "\n",
    "#### On Machine 1 (Server) - Run this in a terminal:\n",
    "```bash\n",
    "ib_write_bw\n",
    "```\n",
    "\n",
    "#### On Machine 2 (Client) - Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069ca9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BANDWIDTH TEST - Run this AFTER starting server on the other machine!\n",
    "# On the OTHER machine, first run: ib_write_bw\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"â³ Running bandwidth test... (takes about 10 seconds)\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE you ran 'ib_write_bw' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_bw {OTHER_MACHINE_IB_IP}\", \n",
    "                \"InfiniBand Bandwidth Test\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4065415",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Bandwidth Results\n",
    "\n",
    "The output looks something like this:\n",
    "```\n",
    "---------------------------------------------------------------------------------------\n",
    " #bytes     #iterations    BW peak[MB/sec]    BW average[MB/sec]   MsgRate[Mpps]\n",
    " 65536      5000           12045.23           12032.45             0.183521\n",
    "---------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "**Look at the `BW average[MB/sec]` column** - this is your sustained bandwidth.\n",
    "\n",
    "#### Converting MB/sec to Gbps:\n",
    "$$\\text{Gbps} = \\frac{\\text{MB/sec} \\times 8}{1000}$$\n",
    "\n",
    "**Why Ã— 8 and Ã· 1000?**\n",
    "\n",
    "| Step | What it does | Reason |\n",
    "|------|--------------|--------|\n",
    "| **Ã— 8** | Converts **Bytes** â†’ **bits** | 1 Byte = 8 bits |\n",
    "| **Ã· 1000** | Converts **Mega** â†’ **Giga** | 1 Giga = 1000 Mega |\n",
    "\n",
    "```\n",
    "MB/sec  â†’  Mb/sec  â†’  Gb/sec\n",
    "        Ã—8         Ã·1000\n",
    "```\n",
    "\n",
    "- **Benchmark tools** report in **Bytes** (B) - what you actually transfer\n",
    "- **Network specs** advertise in **bits** (b) - industry standard\n",
    "\n",
    "This is why a \"100 Gbps\" InfiniBand link shows ~12,000 MB/sec in benchmarks - same speed, different units!\n",
    "\n",
    "#### Reference Table:\n",
    "\n",
    "| BW average (MB/sec) | Calculation | Speed (Gbps) | Hardware |\n",
    "|---------------------|-------------|--------------|----------|\n",
    "| ~12,000 MB/sec | 12000 Ã— 8 Ã· 1000 | **~96 Gbps** | ConnectX-6 (100G) |\n",
    "| ~24,000 MB/sec | 24000 Ã— 8 Ã· 1000 | **~192 Gbps** | ConnectX-6 HDR (200G) |\n",
    "| ~48,000 MB/sec | 48000 Ã— 8 Ã· 1000 | **~384 Gbps** | ConnectX-7 NDR (400G) |\n",
    "\n",
    "**Quick mental math:** Divide MB/sec by 125 to get Gbps  \n",
    "Example: 12,000 Ã· 125 = **96 Gbps** âœ…\n",
    "\n",
    "**For comparison:**\n",
    "- WiFi 6: ~150 MB/sec (~1.2 Gbps)\n",
    "- Gigabit Ethernet: ~125 MB/sec (1 Gbps)\n",
    "- 10G Ethernet: ~1,250 MB/sec (10 Gbps)\n",
    "\n",
    "**InfiniBand is typically 10-100x faster than standard networking!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b74a8",
   "metadata": {},
   "source": [
    "### ğŸ“– ib_write_bw vs ib_send_bw - What's the Difference?\n",
    "\n",
    "You might see different RDMA bandwidth tests mentioned. Here's what they do:\n",
    "\n",
    "| Test | RDMA Operation | How it Works |\n",
    "|------|----------------|--------------|\n",
    "| `ib_write_bw` | **RDMA Write** | One-sided: sender writes directly to receiver's memory |\n",
    "| `ib_send_bw` | **RDMA Send** | Two-sided: receiver must post receive buffers first |\n",
    "| `ib_read_bw` | **RDMA Read** | One-sided: reader pulls data from remote memory |\n",
    "\n",
    "#### RDMA Write (`ib_write_bw`) - One-Sided\n",
    "\n",
    "```\n",
    "Sender                          Receiver\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ App  â”‚ \"Write to addr X\"      â”‚ App  â”‚\n",
    "â”‚      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      â”‚ (CPU not notified!)\n",
    "â”‚ RAM  â”‚                        â”‚ RAM  â”‚ â† Data just appears\n",
    "â””â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "The sender writes directly to the receiver's memory. The receiver's CPU is never involvedâ€”data just appears. **Lowest overhead, best for raw performance testing.**\n",
    "\n",
    "#### RDMA Send (`ib_send_bw`) - Two-Sided\n",
    "\n",
    "```\n",
    "Sender                          Receiver\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”                        â”Œâ”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ App  â”‚ \"Send this data\"       â”‚ App  â”‚ (must pre-post receives)\n",
    "â”‚      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      â”‚ â† Gets completion event\n",
    "â”‚ RAM  â”‚                        â”‚ RAM  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "The receiver must post receive buffers in advance and gets notified when data arrives. **More like traditional message passing (MPI-style).**\n",
    "\n",
    "#### Which Test to Use?\n",
    "\n",
    "| Scenario | Recommended Test |\n",
    "|----------|------------------|\n",
    "| Validating InfiniBand hardware | `ib_write_bw` (standard) |\n",
    "| MPI-style workload simulation | `ib_send_bw` |\n",
    "| Storage/NVMe-oF patterns | `ib_write_bw` or `ib_read_bw` |\n",
    "| General validation | Both should hit near line-rate |\n",
    "\n",
    "**Performance difference is typically minimal** - both should achieve similar bandwidth. `ib_write_bw` may show slightly higher numbers due to zero receive-side overhead.\n",
    "\n",
    "**The full perftest suite:**\n",
    "- `ib_write_bw` / `ib_write_lat` - RDMA Write operations\n",
    "- `ib_read_bw` / `ib_read_lat` - RDMA Read operations\n",
    "- `ib_send_bw` / `ib_send_lat` - RDMA Send operations\n",
    "- `ib_atomic_bw` / `ib_atomic_lat` - Atomic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52028377",
   "metadata": {},
   "source": [
    "### Step 5.2: Latency Test\n",
    "\n",
    "**Latency** = How long it takes for a single message to arrive (like response time)\n",
    "\n",
    "#### On Machine 1 (Server) - Run this in a terminal:\n",
    "```bash\n",
    "ib_write_lat\n",
    "```\n",
    "\n",
    "#### On Machine 2 (Client) - Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb941db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATENCY TEST - Run this AFTER starting server on the other machine!\n",
    "# On the OTHER machine, first run: ib_write_lat\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"â³ Running latency test... (takes about 10 seconds)\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE you ran 'ib_write_lat' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_lat {OTHER_MACHINE_IB_IP}\", \n",
    "                \"InfiniBand Latency Test\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a91145",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Latency Results\n",
    "\n",
    "Look for the **t_avg** (average time) column:\n",
    "\n",
    "| Latency | What it means |\n",
    "|---------|---------------|\n",
    "| 1-2 Î¼s (microseconds) | Excellent! InfiniBand shining |\n",
    "| 5-10 Î¼s | Good |\n",
    "| 50+ Î¼s | Something might be wrong |\n",
    "\n",
    "**For comparison:**\n",
    "- InfiniBand: ~1-2 microseconds\n",
    "- Ethernet (local): ~50-200 microseconds  \n",
    "- WiFi: ~1,000-10,000 microseconds (1-10 ms)\n",
    "\n",
    "**InfiniBand latency is 50-100x lower than Ethernet!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0765c8c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”— Part 5.5: Single Cable vs Dual Cable Experiment\n",
    "\n",
    "Currently, you have **one InfiniBand cable** connecting the two DGX Spark boxes. Let's see what happens when you add a **second cable**!\n",
    "\n",
    "### Why Use Two Cables?\n",
    "\n",
    "```\n",
    "SINGLE CABLE:                          DUAL CABLE:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    1 cable    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   2 cables   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Spark 1 â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚  Spark 2 â”‚    â”‚  Spark 1 â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚  Spark 2 â”‚\n",
    "â”‚          â”‚              â”‚          â”‚    â”‚          â”‚â•â•â•â•â•â•â•â•â•â•â•â•â•â•â”‚          â”‚\n",
    "â”‚ Port 1   â”‚              â”‚ Port 1   â”‚    â”‚ Port 1   â”‚              â”‚ Port 1   â”‚\n",
    "â”‚ Port 2 â—‹ â”‚  (unused)    â”‚ â—‹ Port 2 â”‚    â”‚ Port 2   â”‚              â”‚ Port 2   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "   ~100 Gbps                                  ~200 Gbps (aggregated)\n",
    "```\n",
    "\n",
    "**Benefits of dual cables:**\n",
    "- **Double bandwidth** for large data transfers\n",
    "- **Redundancy** - if one cable fails, traffic continues\n",
    "- **Lower latency** for parallel workloads (multiple streams)\n",
    "\n",
    "### Step 5.5.1: Record Your Single Cable Results\n",
    "\n",
    "Before adding the second cable, record your current performance:\n",
    "\n",
    "| Metric | Single Cable Result |\n",
    "|--------|---------------------|\n",
    "| `ib_write_bw` (MB/sec) | 88.261 Gbps_____________ |\n",
    "| `ib_write_lat` (Î¼s) | 1.99 us_____________ |\n",
    "| `iperf3` (Gbps) | 35 Gbps_____________ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c414bd",
   "metadata": {},
   "source": [
    "### Step 5.5.2: Connect the Second Cable\n",
    "\n",
    "**Physical Setup:**\n",
    "1. Power down is NOT required (InfiniBand supports hot-plug)\n",
    "2. Connect the second cable between the unused ports\n",
    "3. Wait ~10 seconds for the link to come up\n",
    "\n",
    "**Verify both links are active:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee55d5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that BOTH InfiniBand ports are now active\n",
    "run_command(\"ibstat\", \"Check both InfiniBand ports status\")\n",
    "\n",
    "# You should see TWO ports with State: Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81399ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find IP addresses for BOTH InfiniBand interfaces\n",
    "print(\"Looking for both InfiniBand interfaces...\\n\")\n",
    "run_command(\"ip addr show | grep -E -A 2 'enp.*np[0-9]'\", \"Both InfiniBand interface IPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c159e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the second interface IPs (if not already assigned)\n",
    "# You'll need to set IPs on BOTH machines for the second interface\n",
    "\n",
    "# Example IP scheme:\n",
    "#   Machine 1: enp1s0f0np0 = 192.168.100.10, enp1s0f1np1 = 192.168.101.10\n",
    "#   Machine 2: enp1s0f0np0 = 192.168.100.11, enp1s0f1np1 = 192.168.101.11\n",
    "\n",
    "OTHER_MACHINE_IB_IP_2 = \"192.168.200.12\"  # Fill in the second interface IP of the other machine\n",
    "\n",
    "print(\"Set OTHER_MACHINE_IB_IP_2 to the second InfiniBand interface IP of the other machine\")\n",
    "print(f\"  First interface IP:  {OTHER_MACHINE_IB_IP}\")\n",
    "print(f\"  Second interface IP: {OTHER_MACHINE_IB_IP_2 if OTHER_MACHINE_IB_IP_2 else '(not set)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0ad675",
   "metadata": {},
   "source": [
    "### Step 5.5.3: Test Bandwidth on Each Link Individually\n",
    "\n",
    "First, test each cable separately to confirm both are working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bandwidth on FIRST cable (same as before)\n",
    "# On OTHER machine, run: ib_write_bw\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"ğŸ”— Testing FIRST InfiniBand link...\")\n",
    "    print(\"ğŸ”´ Run 'ib_write_bw' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_bw {OTHER_MACHINE_IB_IP}\", \n",
    "                f\"Bandwidth Test - Link 1 ({OTHER_MACHINE_IB_IP})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test bandwidth on SECOND cable\n",
    "# On OTHER machine, run: ib_write_bw -d rocep1s0f1  (or the second device name)\n",
    "\n",
    "if OTHER_MACHINE_IB_IP_2:\n",
    "    print(\"ğŸ”— Testing SECOND InfiniBand link...\")\n",
    "    print(\"ğŸ”´ Run 'ib_write_bw -d rocep1s0f1' on the other machine first!\\n\")\n",
    "    run_command(f\"ib_write_bw -d rocep1s0f1 {OTHER_MACHINE_IB_IP_2}\", \n",
    "                f\"Bandwidth Test - Link 2 ({OTHER_MACHINE_IB_IP_2})\")\n",
    "else:\n",
    "    print(\"âŒ Set OTHER_MACHINE_IB_IP_2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b811a434",
   "metadata": {},
   "source": [
    "### Step 5.5.4: Test Aggregated Bandwidth (Both Links Simultaneously)\n",
    "\n",
    "Now the real test - run traffic on BOTH cables at the same time!\n",
    "\n",
    "**Method 1: Parallel ib_write_bw tests**\n",
    "\n",
    "This requires running tests in parallel. Open two terminals on each machine:\n",
    "\n",
    "```bash\n",
    "# On Machine 1 (server) - Terminal 1:\n",
    "ib_write_bw -d rocep1s0f0\n",
    "\n",
    "# On Machine 1 (server) - Terminal 2:\n",
    "ib_write_bw -d rocep1s0f1 -p 18516\n",
    "\n",
    "# On Machine 2 (client) - Terminal 1:\n",
    "ib_write_bw -d rocep1s0f0 192.168.100.10\n",
    "\n",
    "# On Machine 2 (client) - Terminal 2:\n",
    "ib_write_bw -d rocep1s0f1 192.168.200.12 -p 18516\n",
    "```\n",
    "\n",
    "**Method 2: iperf3 with parallel streams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07929ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run parallel iperf3 tests on both interfaces\n",
    "# Start servers on the other machine first:\n",
    "#   iperf3 -s -p 5201\n",
    "#   iperf3 -s -p 5202\n",
    "\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def run_iperf_test(ip, port, results, index):\n",
    "    \"\"\"Run iperf3 test and store result\"\"\"\n",
    "    try:\n",
    "        cmd = f\"iperf3 -c {ip} -p {port} -t 10 -J\"  # JSON output\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)\n",
    "        results[index] = result.stdout\n",
    "    except Exception as e:\n",
    "        results[index] = str(e)\n",
    "\n",
    "if OTHER_MACHINE_IB_IP and OTHER_MACHINE_IB_IP_2:\n",
    "    print(\"ğŸ”— Running PARALLEL iperf3 tests on BOTH links...\")\n",
    "    print(\"ğŸ”´ Start two iperf3 servers on the other machine:\")\n",
    "    print(\"   Terminal 1: iperf3 -s -p 5201\")\n",
    "    print(\"   Terminal 2: iperf3 -s -p 5202\\n\")\n",
    "    \n",
    "    results = [None, None]\n",
    "    \n",
    "    # Create threads for parallel execution\n",
    "    t1 = threading.Thread(target=run_iperf_test, args=(OTHER_MACHINE_IB_IP, 5201, results, 0))\n",
    "    t2 = threading.Thread(target=run_iperf_test, args=(OTHER_MACHINE_IB_IP_2, 5202, results, 1))\n",
    "    \n",
    "    print(\"Starting parallel tests...\")\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    \n",
    "    print(\"\\nâœ… Both tests complete!\")\n",
    "    print(\"Check the individual results above and add them together for total bandwidth.\")\n",
    "else:\n",
    "    print(\"âŒ Set both OTHER_MACHINE_IB_IP and OTHER_MACHINE_IB_IP_2 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd23a44",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding Dual Cable Results\n",
    "\n",
    "**Expected Results:**\n",
    "\n",
    "| Configuration | Bandwidth (ib_write_bw) | Bandwidth (iperf3) |\n",
    "|--------------|-------------------------|---------------------|\n",
    "| Single Cable | ~12,000 MB/sec (~96 Gbps) | ~35 Gbps |\n",
    "| Dual Cable (each) | ~12,000 MB/sec each | ~35 Gbps each |\n",
    "| Dual Cable (total) | **~24,000 MB/sec (~192 Gbps)** | **~70 Gbps** |\n",
    "\n",
    "**Key Insights:**\n",
    "\n",
    "1. **Linear Scaling**: With two independent links, bandwidth approximately doubles\n",
    "2. **Latency Unchanged**: Adding cables doesn't reduce latency for single messages\n",
    "3. **Parallelism Required**: To use both links, workloads must send traffic on both interfaces\n",
    "4. **NCCL Awareness**: NCCL (used by PyTorch/TensorFlow) automatically uses multiple InfiniBand ports when available\n",
    "\n",
    "### Step 5.5.5: Record Your Dual Cable Results\n",
    "\n",
    "| Metric | Single Cable | Dual Cable (Total) | Improvement |\n",
    "|--------|--------------|-------------------|-------------|\n",
    "| `ib_write_bw` (MB/sec) | 88.252 | 176.5 | 2x |\n",
    "| `iperf3` (Gbps) | _________ | _________ | _____x |\n",
    "| `ib_write_lat` (Î¼s) | 1.98 | 4 | 2x |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f497b906",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ†š Part 6: Compare InfiniBand vs Ethernet\n",
    "\n",
    "Let's do a side-by-side comparison using `iperf3` (works on both InfiniBand and Ethernet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfcd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if iperf3 is installed\n",
    "run_command(\"which iperf3\", \"Checking for iperf3\")\n",
    "run_command(\"iperf3 --version\", \"iperf3 version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f9ca8",
   "metadata": {},
   "source": [
    "### Step 6.1: Test InfiniBand with iperf3\n",
    "\n",
    "#### On Machine 1 (Server) - Run in terminal:\n",
    "```bash\n",
    "iperf3 -s\n",
    "```\n",
    "\n",
    "#### âš ï¸ Troubleshooting: \"Address already in use\"\n",
    "\n",
    "If you get the error `unable to start listener for connections: Address already in use`, the default port 5201 is busy. Try one of these:\n",
    "\n",
    "```bash\n",
    "# Option 1: Kill existing iperf3 process\n",
    "pkill iperf3\n",
    "iperf3 -s\n",
    "\n",
    "# Option 2: Use a different port (e.g., 5202)\n",
    "iperf3 -s -p 5202\n",
    "# Then on client use: iperf3 -c <IP> -p 5202\n",
    "```\n",
    "\n",
    "#### On Machine 2 (Client) - Run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iperf3 test over InfiniBand\n",
    "# On the OTHER machine, first run: iperf3 -s\n",
    "\n",
    "if OTHER_MACHINE_IB_IP:\n",
    "    print(\"â³ Running iperf3 bandwidth test over InfiniBand...\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE you ran 'iperf3 -s' on the other machine first!\\n\")\n",
    "    run_command(f\"iperf3 -c {OTHER_MACHINE_IB_IP} -p 5202 -t 10\", \n",
    "                \"iperf3 Bandwidth Test (10 seconds)\")\n",
    "else:\n",
    "    print(\"âŒ Please set OTHER_MACHINE_IB_IP first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daebf88",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding iperf3 vs ib_write_bw Results\n",
    "\n",
    "**Why is iperf3 showing ~35 Gbps instead of 100+ Gbps?**\n",
    "\n",
    "This is **expected behavior**! Here's why:\n",
    "\n",
    "| Tool | Protocol | What it tests | Expected Speed |\n",
    "|------|----------|---------------|----------------|\n",
    "| `ib_write_bw` | **Native RDMA** | True InfiniBand capability | 100-400 Gbps |\n",
    "| `iperf3` | **TCP/IP over IPoIB** | IP networking over InfiniBand | 20-50 Gbps |\n",
    "\n",
    "```\n",
    "ib_write_bw (RDMA):              iperf3 (TCP/IP):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Application   â”‚              â”‚   Application   â”‚\n",
    "â”‚        â†“        â”‚              â”‚        â†“        â”‚\n",
    "â”‚   Direct RDMA   â”‚ â† Fast!      â”‚   TCP/IP Stack  â”‚ â† Overhead!\n",
    "â”‚        â†“        â”‚              â”‚        â†“        â”‚\n",
    "â”‚  InfiniBand HW  â”‚              â”‚  IPoIB Layer    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚        â†“        â”‚\n",
    "                                 â”‚  InfiniBand HW  â”‚\n",
    "                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key insight:** \n",
    "- **iperf3 at ~35 Gbps** = Your IPoIB (IP-over-InfiniBand) is working well\n",
    "- **ib_write_bw at 100+ Gbps** = Your native RDMA is working at full speed\n",
    "\n",
    "**AI/ML frameworks (PyTorch, TensorFlow) use NCCL which leverages native RDMA, not TCP/IP!**\n",
    "\n",
    "ğŸ’¡ **To improve iperf3 results**, try parallel streams (still won't match RDMA):\n",
    "```bash\n",
    "# Server: iperf3 -s\n",
    "# Client: iperf3 -c <IP> -P 8 -t 10   # 8 parallel streams\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43846ea3",
   "metadata": {},
   "source": [
    "### Step 6.2: Now Test Regular Ethernet (for comparison)\n",
    "\n",
    "If your machines also have Ethernet connections, you can compare!\n",
    "\n",
    "First, find the Ethernet IP of the other machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a263643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Ethernet interfaces (not InfiniBand)\n",
    "run_command(\"ip addr show | grep -A 2 -E 'eth|enp'\", \"Ethernet interface IP addresses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bf31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Set the Ethernet IP of the other machine for comparison\n",
    "OTHER_MACHINE_ETH_IP = \"192.168.100.10\"  # Example: \"10.0.0.2\" - FILL THIS IN!\n",
    "\n",
    "if OTHER_MACHINE_ETH_IP:\n",
    "    print(\"â³ Running iperf3 bandwidth test over Ethernet...\")\n",
    "    print(\"\\nğŸ”´ MAKE SURE 'iperf3 -s' is still running on the other machine!\\n\")\n",
    "    run_command(f\"iperf3 -c {OTHER_MACHINE_ETH_IP} -p 5202 -t 10\", \n",
    "                \"iperf3 Bandwidth Test over ETHERNET\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Set OTHER_MACHINE_ETH_IP to compare Ethernet vs InfiniBand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c298fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ”¥ Part 7: NCCL Tests - What Your Inference Stack Actually Uses\n",
    "\n",
    "All the tests above (`ib_write_bw`, `iperf3`) measure raw network performance. But inference frameworks don't talk to the network directlyâ€”they use **NCCL** (NVIDIA Collective Communications Library).\n",
    "\n",
    "### What is NCCL?\n",
    "\n",
    "NCCL is NVIDIA's library for GPU-to-GPU communication. When you run tensor parallelism in vLLM, TensorRT-LLM, or any multi-GPU PyTorch workload, NCCL handles the actual data movement.\n",
    "\n",
    "**Why test NCCL separately?**\n",
    "- NCCL uses RDMA when available (InfiniBand or RoCE)\n",
    "- It optimizes communication patterns (all-reduce, all-gather, etc.)\n",
    "- The performance you see in NCCL tests = performance your inference gets\n",
    "\n",
    "```\n",
    "Your Inference Code\n",
    "       â†“\n",
    "   PyTorch / vLLM / TensorRT-LLM\n",
    "       â†“\n",
    "     NCCL  â† This is what we're testing\n",
    "       â†“\n",
    "   RoCE (RDMA over Converged Ethernet)\n",
    "       â†“\n",
    "   Other GPUs\n",
    "```\n",
    "\n",
    "### Available nccl-tests Benchmarks\n",
    "\n",
    "| Test | Operation | Use Case |\n",
    "|------|-----------|----------|\n",
    "| `all_gather_perf` | Collect data from all GPUs to all GPUs | Tensor parallelism (gathering sharded outputs) |\n",
    "| `all_reduce_perf` | Sum/reduce across all GPUs | Gradient synchronization, tensor parallelism |\n",
    "| `broadcast_perf` | One GPU sends to all others | Distributing inputs, model weights |\n",
    "| `reduce_scatter_perf` | Reduce then scatter results | Data parallelism, ZeRO (Zero Redundancy Optimizer) |\n",
    "| `sendrecv_perf` | Point-to-point transfers | Pipeline parallelism, KV-cache movement |\n",
    "\n",
    "> ğŸ’¡ For this tutorial, we use `all_gather_perf` as the primary benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1342ff9b",
   "metadata": {},
   "source": [
    "### Step 7.1: Check NCCL Installation and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NCCL version and if it detects InfiniBand\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸ” Checking NCCL environment...\\n\")\n",
    "\n",
    "# Check if nccl-tests is available\n",
    "nccl_test_path = subprocess.run(\"which all_reduce_perf\", shell=True, capture_output=True, text=True)\n",
    "if nccl_test_path.returncode == 0:\n",
    "    print(f\"âœ… nccl-tests found: {nccl_test_path.stdout.strip()}\")\n",
    "else:\n",
    "    print(\"âŒ nccl-tests not found. Install with:\")\n",
    "    print(\"\"\"\n",
    "    # Clone and build nccl-tests\n",
    "    git clone https://github.com/NVIDIA/nccl-tests.git\n",
    "    cd nccl-tests\n",
    "    make MPI=1 MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi\n",
    "    \n",
    "    # Or if you have CUDA in a custom location:\n",
    "    make MPI=1 CUDA_HOME=/usr/local/cuda MPI_HOME=/usr/lib/x86_64-linux-gnu/openmpi\n",
    "    \n",
    "    # Add to PATH\n",
    "    export PATH=$PATH:$(pwd)/build\n",
    "    \"\"\")\n",
    "\n",
    "# Check PyTorch NCCL\n",
    "print(\"\\nğŸ” Checking PyTorch NCCL backend...\")\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"âœ… PyTorch version: {torch.__version__}\")\n",
    "    print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"âœ… GPU count: {torch.cuda.device_count()}\")\n",
    "        print(f\"âœ… NCCL available: {torch.distributed.is_nccl_available()}\")\n",
    "except ImportError:\n",
    "    print(\"âŒ PyTorch not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d950d05f",
   "metadata": {},
   "source": [
    "### Step 8.2: Run NCCL All-Reduce Test (Single Node)\n",
    "\n",
    "First, let's test NCCL communication between GPUs on the SAME machine. This establishes a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCCL All-Reduce test on a single node (between local GPUs)\n",
    "# This uses NVLink/PCIe, not InfiniBand\n",
    "\n",
    "print(\"ğŸ”„ Running NCCL All-Reduce test (single node)...\")\n",
    "print(\"This tests GPU-to-GPU communication WITHIN this machine.\\n\")\n",
    "\n",
    "# Run all_reduce_perf if available\n",
    "result = run_command(\n",
    "    \"all_reduce_perf -b 8 -e 128M -f 2 -g 2\", \n",
    "    \"NCCL All-Reduce Performance (2 GPUs, single node)\"\n",
    ")\n",
    "\n",
    "if not result:\n",
    "    print(\"\"\"\n",
    "If all_reduce_perf is not found, you can test with PyTorch directly.\n",
    "See the next cell for a PyTorch-based NCCL test.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdefb36",
   "metadata": {},
   "source": [
    "### Step 8.3: Run NCCL Multi-Node Test (Over RoCE/RDMA)\n",
    "\n",
    "**This is the real test** - communication between GPUs on DIFFERENT machines over RoCE.\n",
    "\n",
    "You need to run this with `mpirun` to coordinate processes across both machines.\n",
    "\n",
    "#### Understanding the mpirun Command\n",
    "\n",
    "For DGX Spark with 1 GPU per node, use this command format:\n",
    "\n",
    "```bash\n",
    "mpirun -np 2 -H 192.168.200.12:1,192.168.200.13:1 \\\n",
    "    --mca plm_rsh_agent \"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\" \\\n",
    "    -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH \\\n",
    "    -x NCCL_DEBUG=INFO \\\n",
    "    $HOME/src/github.com/NVIDIA/nccl-tests/build/all_gather_perf\n",
    "```\n",
    "\n",
    "#### mpirun Command Options Reference\n",
    "\n",
    "| Option | Meaning |\n",
    "|--------|---------|\n",
    "| `-np 2` | Total number of MPI (Message Passing Interface) processes (1 per GPU across both nodes) |\n",
    "| `-H host:n` | Run n processes on each host (use IP addresses if hostnames aren't in /etc/hosts) |\n",
    "| `--mca` | MCA (Modular Component Architecture) parameter; configures MPI internals |\n",
    "| `btl_tcp_if_include` | Network interface for MPI byte transfer layer (data transfer) |\n",
    "| `oob_tcp_if_include` | Network interface for out-of-band MPI control messages (process startup, coordination) |\n",
    "| `plm_rsh_agent` | PLM (Process Lifecycle Management) remote shell command for launching processes; default is `ssh`, can add options to skip host key prompts |\n",
    "| `-x VAR` | Export environment variable to remote processes |\n",
    "\n",
    "#### nccl-tests Options\n",
    "\n",
    "| Option | Meaning |\n",
    "|--------|---------|\n",
    "| `-b 8` | Start message size (8 bytes) |\n",
    "| `-e 128M` | End message size (128 MB) |\n",
    "| `-f 2` | Step factor: multiply size by 2 each iteration |\n",
    "| `-g 1` | GPUs per process |\n",
    "| `-n 20` | Number of iterations (default: 20) |\n",
    "| `-w 5` | Warmup iterations (default: 5) |\n",
    "| `-c 1` | Check correctness of results |\n",
    "\n",
    "#### What to Look For in the Output\n",
    "\n",
    "**1. Transport Selection (in NCCL_DEBUG output):**\n",
    "```\n",
    "NCCL INFO NET/IB : Using [0]rocep1s0f0:1/RoCE [1]rocep1s0f1:1/RoCE [2]roceP2p1s0f0:1/RoCE [3]roceP2p1s0f1:1/RoCE\n",
    "NCCL INFO Using network IB\n",
    "```\n",
    "`NET/IB` confirms NCCL found and is using RDMA over the RoCE interfaces.\n",
    "\n",
    "If you see `NET/Socket` instead, it's falling back to TCP (requires troubleshooting).\n",
    "\n",
    "**2. Channels established over RDMA:**\n",
    "```\n",
    "NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/4\n",
    "NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/5\n",
    "```\n",
    "\n",
    "**3. Bandwidth Results:**\n",
    "```\n",
    "#       size      time   algbw   busbw\n",
    "#        (B)      (us)  (GB/s)  (GB/s)\n",
    "    33554432   1020.46   32.88   16.44\n",
    "# Avg bus bandwidth    : 15.9891\n",
    "```\n",
    "- **algbw** = Algorithm bandwidth (raw throughput: message_size / time)\n",
    "- **busbw** = Bus bandwidth (normalized for the algorithm's data movement pattern)\n",
    "\n",
    "For all-gather on 2 GPUs over dual 100G RoCE links, expect **busbw of 15-22 GB/s** for large messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6561c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the mpirun command for multi-node NCCL test\n",
    "import os\n",
    "\n",
    "# Configuration - update these for your setup\n",
    "SPARK1_IP = \"192.168.200.12\"  # spark-01 RoCE IP\n",
    "SPARK2_IP = \"192.168.200.13\"  # spark-02 RoCE IP\n",
    "NCCL_TESTS_PATH = os.path.expandvars(\"$HOME/src/github.com/NVIDIA/nccl-tests/build\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ“‹ NCCL MULTI-NODE TEST INSTRUCTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "This test requires running mpirun from a terminal (not this notebook)\n",
    "because it needs to coordinate processes across both machines.\n",
    "\n",
    "PREREQUISITE: SSH access between machines (password or key-based)\n",
    "  ssh-keygen -t rsa  # if you don't have keys\n",
    "  ssh-copy-id {SPARK2_IP}\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "TEST 1: All-Gather (default message sizes)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "Run from spark-01:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"mpirun -np 2 -H {SPARK1_IP}:1,{SPARK2_IP}:1 \\\\\n",
    "    --mca plm_rsh_agent \"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\" \\\\\n",
    "    -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH \\\\\n",
    "    -x NCCL_DEBUG=INFO \\\\\n",
    "    {NCCL_TESTS_PATH}/all_gather_perf\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "TEST 2: All-Gather with 16 GB message (maximum throughput test)\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"mpirun -np 2 -H {SPARK1_IP}:1,{SPARK2_IP}:1 \\\\\n",
    "    --mca plm_rsh_agent \"ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no\" \\\\\n",
    "    -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH \\\\\n",
    "    -x NCCL_DEBUG=INFO \\\\\n",
    "    {NCCL_TESTS_PATH}/all_gather_perf -b 16G -e 16G -f 2\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "WHAT TO LOOK FOR IN THE OUTPUT\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "âœ… GOOD - Using RDMA (RoCE):\n",
    "   \"NCCL INFO NET/IB : Using [0]rocep1s0f0:1/RoCE [1]rocep1s0f1:1/RoCE...\"\n",
    "   \"NCCL INFO Using network IB\"\n",
    "\n",
    "âŒ BAD - Falling back to TCP:\n",
    "   \"NCCL INFO NET/Socket\"\n",
    "\n",
    "EXPECTED BANDWIDTH (busbw column):\n",
    "  - 32 MB message:  ~16 GB/s\n",
    "  - 16 GB message:  ~22 GB/s (dual 100G links saturated)\n",
    "  - < 5 GB/s for large messages = RDMA not working, troubleshoot!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265cea4",
   "metadata": {},
   "source": [
    "### ğŸ“– Understanding NCCL Test Results\n",
    "\n",
    "#### Key Indicators in NCCL_DEBUG Output\n",
    "\n",
    "When running with `NCCL_DEBUG=INFO`, look for these four key indicators:\n",
    "\n",
    "**1. Both GPUs detected across nodes:**\n",
    "```\n",
    "#  Rank  0 Group  0 Pid 144333 on   spark-01 device  0 [000f:01:00] NVIDIA GB10\n",
    "#  Rank  1 Group  0 Pid  69466 on   spark-02 device  0 [000f:01:00] NVIDIA GB10\n",
    "```\n",
    "This confirms NCCL sees both GPUs. The `[000f:01:00]` is the PCIe bus ID.\n",
    "\n",
    "**2. RDMA transport selected (not TCP fallback):**\n",
    "```\n",
    "NCCL INFO NET/IB : Using [0]rocep1s0f0:1/RoCE [1]rocep1s0f1:1/RoCE [2]roceP2p1s0f0:1/RoCE [3]roceP2p1s0f1:1/RoCE\n",
    "NCCL INFO Using network IB\n",
    "```\n",
    "`NET/IB` confirms NCCL is using RDMA over the RoCE interfaces. All four interfaces are detected (dual-port on each node).\n",
    "\n",
    "âš ï¸ **If you see `NET/Socket` instead**, NCCL is falling back to TCP. This requires troubleshooting:\n",
    "- Check that RoCE interfaces have IP addresses configured\n",
    "- Verify `ibstat` shows ports as Active\n",
    "- Ensure no firewall is blocking RDMA traffic\n",
    "\n",
    "**3. Channels established over RDMA:**\n",
    "```\n",
    "NCCL INFO Channel 00/0 : 0[0] -> 1[0] [send] via NET/IB/4\n",
    "NCCL INFO Channel 01/0 : 0[0] -> 1[0] [send] via NET/IB/5\n",
    "```\n",
    "NCCL creates multiple channels for parallelism. `NET/IB/4` and `NET/IB/5` refer to different RoCE interfaces, showing NCCL is load-balancing across both 100G links.\n",
    "\n",
    "**4. Performance results show expected bandwidth:**\n",
    "```\n",
    "#       size      time   algbw   busbw\n",
    "#        (B)      (us)  (GB/s)  (GB/s)\n",
    "    33554432   1020.46   32.88   16.44\n",
    "# Avg bus bandwidth    : 15.9891\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Actual Results from DGX Spark\n",
    "\n",
    "**32 MB message:**\n",
    "```\n",
    "#       size      time   algbw   busbw\n",
    "#        (B)      (us)  (GB/s)  (GB/s)\n",
    "    33554432   1020.46   32.88   16.44\n",
    "# Avg bus bandwidth    : 15.9891\n",
    "```\n",
    "\n",
    "**16 GB message (maximum throughput):**\n",
    "```\n",
    "#       size      time   algbw   busbw\n",
    "#        (B)      (us)  (GB/s)  (GB/s)\n",
    " 17179869184   387603   44.32   22.16\n",
    "# Avg bus bandwidth    : 21.9736\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Understanding the Columns\n",
    "\n",
    "| Column | What it means |\n",
    "|--------|---------------|\n",
    "| `size` | Message size in bytes |\n",
    "| `time` | Time to complete operation (microseconds) |\n",
    "| `algbw` | Algorithm bandwidth = size / time (raw throughput) |\n",
    "| `busbw` | Bus bandwidth = algbw Ã— (N-1)/N for all-gather (actual link utilization) |\n",
    "\n",
    "**Why busbw differs from algbw?**\n",
    "\n",
    "For all-gather with N GPUs, each GPU sends its data to all other GPUs. The bus bandwidth formula `busbw = algbw Ã— (N-1)/N` normalizes for this pattern, giving a better measure of actual network utilization.\n",
    "\n",
    "With 2 GPUs: busbw = algbw Ã— 0.5. So if algbw is 44 GB/s, busbw is ~22 GB/s.\n",
    "\n",
    "---\n",
    "\n",
    "#### Expected Performance Benchmarks\n",
    "\n",
    "| Message Size | Expected busbw | Notes |\n",
    "|--------------|----------------|-------|\n",
    "| < 1 KB | Low | Latency-bound, not meaningful for bandwidth |\n",
    "| 32 MB | ~16 GB/s | Single 100G link saturated |\n",
    "| 16 GB | ~22 GB/s | Both 100G links working together |\n",
    "| < 5 GB/s (large msg) | âš ï¸ Problem | NCCL likely not using RDMA |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d1b11",
   "metadata": {},
   "source": [
    "### Step 7.4: Alternative - PyTorch NCCL Test\n",
    "\n",
    "If nccl-tests isn't installed, you can verify NCCL over InfiniBand with PyTorch directly.\n",
    "\n",
    "Save this as `nccl_test.py` and run with `torchrun`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc416fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple PyTorch NCCL benchmark script\n",
    "nccl_test_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple NCCL All-Reduce benchmark using PyTorch.\n",
    "Tests GPU-to-GPU communication bandwidth.\n",
    "\n",
    "Run with:\n",
    "  # Single node (2 GPUs):\n",
    "  torchrun --nproc_per_node=2 nccl_test.py\n",
    "  \n",
    "  # Multi-node (requires MASTER_ADDR, MASTER_PORT):\n",
    "  # On node 1:\n",
    "  torchrun --nproc_per_node=2 --nnodes=2 --node_rank=0 \\\\\n",
    "           --master_addr=<node1_ip> --master_port=29500 nccl_test.py\n",
    "  # On node 2:\n",
    "  torchrun --nproc_per_node=2 --nnodes=2 --node_rank=1 \\\\\n",
    "           --master_addr=<node1_ip> --master_port=29500 nccl_test.py\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "def benchmark_all_reduce(size_mb, num_iterations=100):\n",
    "    \"\"\"Benchmark all-reduce for a given tensor size.\"\"\"\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    device = torch.device(f\"cuda:{rank % torch.cuda.device_count()}\")\n",
    "    \n",
    "    # Create tensor\n",
    "    num_elements = (size_mb * 1024 * 1024) // 4  # float32 = 4 bytes\n",
    "    tensor = torch.randn(num_elements, device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        dist.all_reduce(tensor)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_iterations):\n",
    "        dist.all_reduce(tensor)\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    # Calculate bandwidth\n",
    "    # All-reduce moves 2*(n-1)/n * size data\n",
    "    bytes_moved = size_mb * 1024 * 1024 * 2 * (world_size - 1) / world_size\n",
    "    total_bytes = bytes_moved * num_iterations\n",
    "    bandwidth_gbps = (total_bytes / elapsed) / 1e9\n",
    "    \n",
    "    return bandwidth_gbps, elapsed / num_iterations * 1000  # GB/s, ms\n",
    "\n",
    "def main():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(f\"\\\\n{'='*60}\")\n",
    "        print(f\"NCCL All-Reduce Benchmark\")\n",
    "        print(f\"World size: {world_size} GPUs\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"{'Size':>10} {'Bandwidth':>15} {'Latency':>15}\")\n",
    "        print(f\"{'(MB)':>10} {'(GB/s)':>15} {'(ms)':>15}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    for size_mb in [1, 8, 32, 128, 512]:\n",
    "        bw, latency = benchmark_all_reduce(size_mb)\n",
    "        if rank == 0:\n",
    "            print(f\"{size_mb:>10} {bw:>15.2f} {latency:>15.3f}\")\n",
    "    \n",
    "    if rank == 0:\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\\\nExpected bandwidth over InfiniBand: 20-40+ GB/s for large messages\")\n",
    "        print(\"If seeing < 5 GB/s, check NCCL_DEBUG=INFO output for transport type\")\n",
    "    \n",
    "    dist.destroy_process_group()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the script\n",
    "script_path = \"/home/nvidia/src/github.com/elizabetht/spark/infiniband-tutorial/nccl_test.py\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(nccl_test_script)\n",
    "\n",
    "print(f\"âœ… Saved NCCL test script to: {script_path}\")\n",
    "print(\"\"\"\n",
    "To run single-node test (2 GPUs on this machine):\n",
    "  torchrun --nproc_per_node=2 nccl_test.py\n",
    "\n",
    "To run multi-node test over InfiniBand:\n",
    "  # On machine 1 (master):\n",
    "  NCCL_DEBUG=INFO torchrun --nproc_per_node=2 --nnodes=2 --node_rank=0 \\\\\n",
    "      --master_addr={0} --master_port=29500 nccl_test.py\n",
    "  \n",
    "  # On machine 2:\n",
    "  NCCL_DEBUG=INFO torchrun --nproc_per_node=2 --nnodes=2 --node_rank=1 \\\\\n",
    "      --master_addr={0} --master_port=29500 nccl_test.py\n",
    "\"\"\".format(OTHER_MACHINE_IB_IP if 'OTHER_MACHINE_IB_IP' in dir() else '<master_ip>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bcf250",
   "metadata": {},
   "source": [
    "### ğŸ”§ Troubleshooting NCCL Over InfiniBand\n",
    "\n",
    "**NCCL isn't using InfiniBand?** Check these environment variables:\n",
    "\n",
    "```bash\n",
    "# Force NCCL to use InfiniBand\n",
    "export NCCL_IB_DISABLE=0\n",
    "\n",
    "# Specify which InfiniBand devices to use  \n",
    "export NCCL_IB_HCA=mlx5_0,mlx5_1\n",
    "\n",
    "# Enable debug output to see transport selection\n",
    "export NCCL_DEBUG=INFO\n",
    "export NCCL_DEBUG_SUBSYS=INIT,NET\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "\n",
    "| Symptom | Likely Cause | Fix |\n",
    "|---------|--------------|-----|\n",
    "| \"NET/Socket\" in debug | IB not detected | Check `ibstat`, ensure ports are Active |\n",
    "| Low bandwidth (< 5 GB/s) | Using TCP fallback | Set `NCCL_IB_DISABLE=0` |\n",
    "| \"No route to host\" | Network config | Ensure IB IPs are reachable (`ping`) |\n",
    "| \"Bootstrap failed\" | Firewall | Open ports 29500+ or disable firewall |\n",
    "\n",
    "### Step 7.5: Record Your NCCL Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your NCCL test results\n",
    "nccl_results = {\n",
    "    \"test_type\": \"all_reduce_perf / PyTorch\",  # Which test did you run?\n",
    "    \"num_gpus\": 4,  # Total GPUs across all nodes\n",
    "    \"num_nodes\": 2,  # Number of machines\n",
    "    \"transport_used\": \"IB\",  # Check NCCL_DEBUG output: \"IB\" or \"Socket\"?\n",
    "    \"bandwidth_128mb_gbps\": 0,  # busbw for 128MB message\n",
    "    \"notes\": \"\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š NCCL Test Results:\")\n",
    "print(\"-\" * 40)\n",
    "for key, value in nccl_results.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"-\" * 40)\n",
    "print(\"\"\"\n",
    "Fill in your results above after running the NCCL tests!\n",
    "\n",
    "Key question: Did NCCL_DEBUG show \"NET/IB\" or \"NET/Socket\"?\n",
    "- NET/IB = InfiniBand is working âœ…\n",
    "- NET/Socket = Falling back to TCP âŒ\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24135551",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Record Your Results Here\n",
    "\n",
    "Fill in your actual test results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a20cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record your results here for reference\n",
    "my_results = {\n",
    "    \"machine_1_hostname\": \"spark-01\",  # Fill in\n",
    "    \"machine_2_hostname\": \"spark-02\",  # Fill in\n",
    "    \"infiniband_bandwidth_gbps\":  96.3,  # From ib_write_bw test\n",
    "    \"infiniband_latency_us\": 1.96,  # From ib_write_lat test\n",
    "    \"ethernet_bandwidth_gbps\": 42.25,  # From iperf3 over ethernet (if tested)\n",
    "    \"notes\": \"\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“Š Your recorded results:\")\n",
    "for key, value in my_results.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
