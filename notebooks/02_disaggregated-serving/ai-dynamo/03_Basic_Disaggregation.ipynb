{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfa1450",
   "metadata": {},
   "source": [
    "# Basic Disaggregation (Without RDMA)\n",
    "\n",
    "Split prefill and decode across two nodes using standard TCP/IP networking. This establishes the disaggregation architecture before optimizing with RDMA.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "┌─────────────┐                          ┌─────────────┐\n",
    "│  Node 1     │                          │  Node 2     │\n",
    "│  (Prefill)  │                          │  (Decode)   │\n",
    "│             │                          │             │\n",
    "│  1. Receive │                          │  3. Receive │\n",
    "│     prompt  │                          │     KV cache│\n",
    "│             │                          │             │\n",
    "│  2. Process │   ── KV Cache ──>       │  4. Generate│\n",
    "│     prompt  │      (TCP/IP)            │     tokens  │\n",
    "│             │                          │             │\n",
    "│  GPU #0     │                          │  GPU #1     │\n",
    "└─────────────┘                          └─────────────┘\n",
    "```\n",
    "\n",
    "## Why Split This Way?\n",
    "\n",
    "- **Prefill**: Compute-intensive, processes entire prompt at once\n",
    "- **Decode**: Memory-intensive, generates one token at a time\n",
    "- **Benefit**: Each node specializes, can optimize differently\n",
    "\n",
    "## What We're Measuring\n",
    "\n",
    "- End-to-end latency (prefill + transfer + decode)\n",
    "- Transfer overhead as % of total time\n",
    "- Throughput compared to baseline single-node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d0b50",
   "metadata": {},
   "source": [
    "## Step 1: Setup - Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41ccff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import socket\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load environment config\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    MODEL_NAME = env_config['model']['name']\n",
    "    NODE1_IP = env_config['network']['node1_ip']\n",
    "    NODE2_IP = env_config['network']['node2_ip']\n",
    "else:\n",
    "    MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    NODE1_IP = \"192.168.100.10\"\n",
    "    NODE2_IP = \"192.168.100.11\"\n",
    "\n",
    "# Determine current node\n",
    "hostname = socket.gethostname()\n",
    "is_node1 = \"01\" in hostname or \"dgx01\" in hostname\n",
    "is_node2 = \"02\" in hostname or \"dgx02\" in hostname\n",
    "\n",
    "print(f\"Hostname: {hostname}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Node 1 (Prefill): {NODE1_IP}\")\n",
    "print(f\"Node 2 (Decode): {NODE2_IP}\")\n",
    "print(f\"\\nThis node is: {'Prefill' if is_node1 else 'Decode' if is_node2 else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4279726f",
   "metadata": {},
   "source": [
    "## Step 2: Implement KV Cache Serialization\n",
    "\n",
    "Before sending KV cache over network, we need to serialize it. This converts PyTorch tensors to bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "def serialize_kv_cache(past_key_values):\n",
    "    \"\"\"\n",
    "    Serialize KV cache for network transfer.\n",
    "    \n",
    "    Args:\n",
    "        past_key_values: Tuple of (key, value) tensors per layer\n",
    "    \n",
    "    Returns:\n",
    "        bytes: Serialized cache data\n",
    "    \"\"\"\n",
    "    # Move to CPU for serialization\n",
    "    cpu_cache = []\n",
    "    for key, value in past_key_values:\n",
    "        cpu_cache.append((key.cpu(), value.cpu()))\n",
    "    \n",
    "    # Serialize with pickle\n",
    "    serialized = pickle.dumps(cpu_cache)\n",
    "    return serialized\n",
    "\n",
    "def deserialize_kv_cache(serialized_data, device='cuda'):\n",
    "    \"\"\"\n",
    "    Deserialize KV cache and move to GPU.\n",
    "    \n",
    "    Args:\n",
    "        serialized_data: Bytes from serialize_kv_cache\n",
    "        device: Target device for tensors\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (key, value) tensors per layer\n",
    "    \"\"\"\n",
    "    # Deserialize\n",
    "    cpu_cache = pickle.loads(serialized_data)\n",
    "    \n",
    "    # Move to target device\n",
    "    device_cache = []\n",
    "    for key, value in cpu_cache:\n",
    "        device_cache.append((key.to(device), value.to(device)))\n",
    "    \n",
    "    return tuple(device_cache)\n",
    "\n",
    "# Test serialization\n",
    "print(\"Testing KV cache serialization...\\n\")\n",
    "\n",
    "# Create dummy KV cache\n",
    "num_layers = 4\n",
    "batch_size = 1\n",
    "num_heads = 8\n",
    "seq_len = 100\n",
    "head_dim = 64\n",
    "\n",
    "dummy_cache = []\n",
    "for _ in range(num_layers):\n",
    "    key = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    value = torch.randn(batch_size, num_heads, seq_len, head_dim, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dummy_cache.append((key, value))\n",
    "\n",
    "# Serialize\n",
    "start = time.time()\n",
    "serialized = serialize_kv_cache(dummy_cache)\n",
    "serialize_time = time.time() - start\n",
    "\n",
    "# Deserialize\n",
    "start = time.time()\n",
    "deserialized = deserialize_kv_cache(serialized)\n",
    "deserialize_time = time.time() - start\n",
    "\n",
    "print(f\"Original cache: {len(dummy_cache)} layers\")\n",
    "print(f\"Serialized size: {len(serialized) / 1e6:.2f} MB\")\n",
    "print(f\"Serialize time: {serialize_time * 1000:.2f} ms\")\n",
    "print(f\"Deserialize time: {deserialize_time * 1000:.2f} ms\")\n",
    "print(f\"Total overhead: {(serialize_time + deserialize_time) * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e02167",
   "metadata": {},
   "source": [
    "## Step 3: Implement Prefill Server (Node 1)\n",
    "\n",
    "This runs on the prefill node. It:\n",
    "1. Receives prompts\n",
    "2. Runs prefill phase\n",
    "3. Sends KV cache to decode node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b114b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "import struct\n",
    "\n",
    "class PrefillServer:\n",
    "    \"\"\"Prefill server - processes prompts and sends KV cache.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, port=5555):\n",
    "        self.model_name = model_name\n",
    "        self.port = port\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model for prefill.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"✓ Model loaded\")\n",
    "    \n",
    "    def prefill(self, prompt):\n",
    "        \"\"\"\n",
    "        Run prefill phase - process prompt and generate KV cache.\n",
    "        \n",
    "        Returns:\n",
    "            dict with input_ids, past_key_values, and metrics\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Tokenize\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        \n",
    "        # Run forward pass to generate KV cache\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, use_cache=True)\n",
    "            past_key_values = outputs.past_key_values\n",
    "        \n",
    "        prefill_time = time.time() - start\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'past_key_values': past_key_values,\n",
    "            'prompt': prompt,\n",
    "            'prefill_time_ms': prefill_time * 1000\n",
    "        }\n",
    "    \n",
    "    def send_to_decode_node(self, result, decode_host, decode_port=5556):\n",
    "        \"\"\"\n",
    "        Send KV cache to decode node via TCP.\n",
    "        \n",
    "        Protocol:\n",
    "        1. Send prompt length (4 bytes)\n",
    "        2. Send prompt (variable)\n",
    "        3. Send input_ids length (4 bytes)\n",
    "        4. Send input_ids (variable)\n",
    "        5. Send KV cache length (4 bytes)\n",
    "        6. Send KV cache (variable)\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Serialize KV cache\n",
    "        serialized_kv = serialize_kv_cache(result['past_key_values'])\n",
    "        prompt_bytes = result['prompt'].encode('utf-8')\n",
    "        input_ids_bytes = pickle.dumps(result['input_ids'].cpu())\n",
    "        \n",
    "        # Connect to decode node\n",
    "        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        sock.connect((decode_host, decode_port))\n",
    "        \n",
    "        try:\n",
    "            # Send prompt\n",
    "            sock.sendall(struct.pack('I', len(prompt_bytes)))\n",
    "            sock.sendall(prompt_bytes)\n",
    "            \n",
    "            # Send input_ids\n",
    "            sock.sendall(struct.pack('I', len(input_ids_bytes)))\n",
    "            sock.sendall(input_ids_bytes)\n",
    "            \n",
    "            # Send KV cache\n",
    "            sock.sendall(struct.pack('I', len(serialized_kv)))\n",
    "            sock.sendall(serialized_kv)\n",
    "            \n",
    "        finally:\n",
    "            sock.close()\n",
    "        \n",
    "        transfer_time = time.time() - start\n",
    "        \n",
    "        return {\n",
    "            'transfer_time_ms': transfer_time * 1000,\n",
    "            'kv_cache_mb': len(serialized_kv) / 1e6\n",
    "        }\n",
    "\n",
    "# Initialize prefill server (only on Node 1)\n",
    "if is_node1:\n",
    "    print(\"Initializing Prefill Server...\")\n",
    "    prefill_server = PrefillServer(MODEL_NAME)\n",
    "    prefill_server.load_model()\n",
    "else:\n",
    "    print(\"This is not the prefill node - skip prefill server setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b82c41",
   "metadata": {},
   "source": [
    "## Step 4: Implement Decode Server (Node 2)\n",
    "\n",
    "This runs on the decode node. It:\n",
    "1. Receives KV cache from prefill node\n",
    "2. Runs decode phase\n",
    "3. Generates output tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb20728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "class DecodeServer:\n",
    "    \"\"\"Decode server - receives KV cache and generates tokens.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, port=5556):\n",
    "        self.model_name = model_name\n",
    "        self.port = port\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.server_socket = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model for decode.\"\"\"\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.model.eval()\n",
    "        print(\"✓ Model loaded\")\n",
    "    \n",
    "    def decode(self, input_ids, past_key_values, max_new_tokens=50):\n",
    "        \"\"\"\n",
    "        Run decode phase - generate tokens using received KV cache.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            past_key_values: KV cache from prefill\n",
    "            max_new_tokens: Number of tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            dict with generated text and metrics\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Move input_ids to device\n",
    "        input_ids = input_ids.to(self.model.device)\n",
    "        \n",
    "        # Generate with provided cache\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=False,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = self.tokenizer.decode(\n",
    "            outputs[0][input_ids.shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        decode_time = time.time() - start\n",
    "        tokens_generated = outputs.shape[1] - input_ids.shape[1]\n",
    "        \n",
    "        return {\n",
    "            'text': generated_text,\n",
    "            'tokens': tokens_generated,\n",
    "            'decode_time_ms': decode_time * 1000,\n",
    "            'tokens_per_sec': tokens_generated / decode_time\n",
    "        }\n",
    "    \n",
    "    def receive_from_prefill_node(self, client_socket):\n",
    "        \"\"\"\n",
    "        Receive KV cache from prefill node.\n",
    "        \n",
    "        Returns:\n",
    "            dict with prompt, input_ids, past_key_values\n",
    "        \"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        # Receive prompt\n",
    "        prompt_len = struct.unpack('I', self._recv_exact(client_socket, 4))[0]\n",
    "        prompt = self._recv_exact(client_socket, prompt_len).decode('utf-8')\n",
    "        \n",
    "        # Receive input_ids\n",
    "        input_ids_len = struct.unpack('I', self._recv_exact(client_socket, 4))[0]\n",
    "        input_ids_bytes = self._recv_exact(client_socket, input_ids_len)\n",
    "        input_ids = pickle.loads(input_ids_bytes)\n",
    "        \n",
    "        # Receive KV cache\n",
    "        kv_len = struct.unpack('I', self._recv_exact(client_socket, 4))[0]\n",
    "        kv_bytes = self._recv_exact(client_socket, kv_len)\n",
    "        past_key_values = deserialize_kv_cache(kv_bytes, device=self.model.device)\n",
    "        \n",
    "        receive_time = time.time() - start\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'input_ids': input_ids,\n",
    "            'past_key_values': past_key_values,\n",
    "            'receive_time_ms': receive_time * 1000,\n",
    "            'kv_cache_mb': kv_len / 1e6\n",
    "        }\n",
    "    \n",
    "    def _recv_exact(self, sock, n):\n",
    "        \"\"\"Receive exactly n bytes from socket.\"\"\"\n",
    "        data = b''\n",
    "        while len(data) < n:\n",
    "            chunk = sock.recv(n - len(data))\n",
    "            if not chunk:\n",
    "                raise ConnectionError(\"Socket connection broken\")\n",
    "            data += chunk\n",
    "        return data\n",
    "    \n",
    "    def start_server(self):\n",
    "        \"\"\"Start listening for prefill requests.\"\"\"\n",
    "        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "        self.server_socket.bind(('0.0.0.0', self.port))\n",
    "        self.server_socket.listen(5)\n",
    "        print(f\"Decode server listening on port {self.port}\")\n",
    "\n",
    "# Initialize decode server (only on Node 2)\n",
    "if is_node2:\n",
    "    print(\"Initializing Decode Server...\")\n",
    "    decode_server = DecodeServer(MODEL_NAME)\n",
    "    decode_server.load_model()\n",
    "    print(\"\\nTo start server, run:\")\n",
    "    print(\"  decode_server.start_server()\")\n",
    "else:\n",
    "    print(\"This is not the decode node - skip decode server setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab0831",
   "metadata": {},
   "source": [
    "## Step 5: Test Disaggregated Inference\n",
    "\n",
    "Run end-to-end test: Node 1 does prefill, sends to Node 2, Node 2 does decode.\n",
    "\n",
    "**Instructions:**\n",
    "1. On Node 2: Run the decode server cell above, then start server\n",
    "2. On Node 1: Run this cell to test complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs on Node 1 (prefill node)\n",
    "if is_node1:\n",
    "    print(\"Running Disaggregated Inference Test\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_prompt = \"Explain container orchestration systems like Kubernetes.\"\n",
    "    \n",
    "    print(f\"Prompt: '{test_prompt}'\\n\")\n",
    "    \n",
    "    # Step 1: Prefill on Node 1\n",
    "    print(\"[Node 1] Running prefill...\")\n",
    "    prefill_result = prefill_server.prefill(test_prompt)\n",
    "    print(f\"  Prefill time: {prefill_result['prefill_time_ms']:.2f} ms\")\n",
    "    print(f\"  Input tokens: {prefill_result['input_ids'].shape[1]}\")\n",
    "    \n",
    "    # Step 2: Transfer to Node 2\n",
    "    print(f\"\\n[Network] Sending KV cache to {NODE2_IP}...\")\n",
    "    transfer_result = prefill_server.send_to_decode_node(prefill_result, NODE2_IP)\n",
    "    print(f\"  Transfer time: {transfer_result['transfer_time_ms']:.2f} ms\")\n",
    "    print(f\"  KV cache size: {transfer_result['kv_cache_mb']:.2f} MB\")\n",
    "    bandwidth_gbps = (transfer_result['kv_cache_mb'] * 8) / (transfer_result['transfer_time_ms'] / 1000)\n",
    "    print(f\"  Effective bandwidth: {bandwidth_gbps:.2f} Gbps\")\n",
    "    \n",
    "    # Note: Decode happens on Node 2\n",
    "    print(\"\\n[Node 2] Decode running on remote node...\")\n",
    "    print(\"  (Check Node 2's output for decode results)\")\n",
    "    \n",
    "    # Calculate total pipeline\n",
    "    total_time = prefill_result['prefill_time_ms'] + transfer_result['transfer_time_ms']\n",
    "    transfer_overhead_pct = (transfer_result['transfer_time_ms'] / total_time) * 100\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Pipeline Summary (Prefill + Transfer):\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Prefill time: {prefill_result['prefill_time_ms']:.2f} ms\")\n",
    "    print(f\"Transfer time: {transfer_result['transfer_time_ms']:.2f} ms\")\n",
    "    print(f\"Total so far: {total_time:.2f} ms\")\n",
    "    print(f\"Transfer overhead: {transfer_overhead_pct:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    print(\"This cell should run on Node 1 (prefill node)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c315910",
   "metadata": {},
   "source": [
    "## Step 6: Decode Node Processing (Run on Node 2)\n",
    "\n",
    "This cell runs on Node 2 to handle incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell runs on Node 2 (decode node)\n",
    "if is_node2:\n",
    "    print(\"Starting Decode Server (waiting for requests)...\\n\")\n",
    "    \n",
    "    decode_server.start_server()\n",
    "    \n",
    "    try:\n",
    "        # Accept one connection for testing\n",
    "        client_socket, client_address = decode_server.server_socket.accept()\n",
    "        print(f\"Received connection from {client_address}\\n\")\n",
    "        \n",
    "        # Receive KV cache\n",
    "        print(\"[Network] Receiving KV cache...\")\n",
    "        received = decode_server.receive_from_prefill_node(client_socket)\n",
    "        print(f\"  Receive time: {received['receive_time_ms']:.2f} ms\")\n",
    "        print(f\"  KV cache size: {received['kv_cache_mb']:.2f} MB\")\n",
    "        \n",
    "        # Run decode\n",
    "        print(f\"\\n[Node 2] Running decode...\")\n",
    "        print(f\"  Prompt: '{received['prompt']}'\")\n",
    "        decode_result = decode_server.decode(\n",
    "            received['input_ids'],\n",
    "            received['past_key_values'],\n",
    "            max_new_tokens=50\n",
    "        )\n",
    "        print(f\"  Decode time: {decode_result['decode_time_ms']:.2f} ms\")\n",
    "        print(f\"  Tokens generated: {decode_result['tokens']}\")\n",
    "        print(f\"  Throughput: {decode_result['tokens_per_sec']:.1f} tokens/sec\")\n",
    "        print(f\"\\n  Generated text:\\n  '{decode_result['text']}'\")\n",
    "        \n",
    "        # Total pipeline time\n",
    "        total_decode_side = received['receive_time_ms'] + decode_result['decode_time_ms']\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Decode Node Summary:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Receive time: {received['receive_time_ms']:.2f} ms\")\n",
    "        print(f\"Decode time: {decode_result['decode_time_ms']:.2f} ms\")\n",
    "        print(f\"Total: {total_decode_side:.2f} ms\")\n",
    "        \n",
    "    finally:\n",
    "        client_socket.close()\n",
    "        decode_server.server_socket.close()\n",
    "        \n",
    "else:\n",
    "    print(\"This cell should run on Node 2 (decode node)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6fafe4",
   "metadata": {},
   "source": [
    "## Step 7: Compare with Baseline\n",
    "\n",
    "Load baseline metrics and compare disaggregated performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1b34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline metrics\n",
    "baseline_file = Path(\"baseline_metrics.json\")\n",
    "if baseline_file.exists():\n",
    "    with open(baseline_file) as f:\n",
    "        baseline = json.load(f)\n",
    "    \n",
    "    baseline_latency = baseline['single_request']['latency_ms']\n",
    "    baseline_throughput = baseline['single_request']['throughput_tokens_per_sec']\n",
    "    \n",
    "    # Example disaggregated times (update with actual measurements)\n",
    "    # These would come from running the cells above\n",
    "    disagg_prefill = 50  # ms - from Node 1\n",
    "    disagg_transfer = 15  # ms - network transfer\n",
    "    disagg_decode = 100  # ms - from Node 2\n",
    "    disagg_total = disagg_prefill + disagg_transfer + disagg_decode\n",
    "    \n",
    "    print(\"Performance Comparison\\n\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nBaseline (Single Node):\")\n",
    "    print(f\"  Total latency: {baseline_latency:.1f} ms\")\n",
    "    print(f\"  Throughput: {baseline_throughput:.1f} tokens/sec\")\n",
    "    \n",
    "    print(f\"\\nDisaggregated (Two Nodes):\")\n",
    "    print(f\"  Prefill: {disagg_prefill:.1f} ms\")\n",
    "    print(f\"  Transfer: {disagg_transfer:.1f} ms\")\n",
    "    print(f\"  Decode: {disagg_decode:.1f} ms\")\n",
    "    print(f\"  Total: {disagg_total:.1f} ms\")\n",
    "    \n",
    "    slowdown = (disagg_total / baseline_latency - 1) * 100\n",
    "    transfer_pct = (disagg_transfer / disagg_total) * 100\n",
    "    \n",
    "    print(f\"\\nAnalysis:\")\n",
    "    print(f\"  Slowdown: {slowdown:+.1f}%\")\n",
    "    print(f\"  Transfer overhead: {transfer_pct:.1f}% of total time\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Key Insight:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Transfer time ({disagg_transfer:.1f} ms) is {transfer_pct:.0f}% overhead\")\n",
    "    print(f\"This is why we need RDMA - 10x faster network\")\n",
    "    print(f\"With RDMA @ 100 Gbps: ~{disagg_transfer/10:.1f} ms transfer\")\n",
    "    print(f\"That would reduce total to ~{disagg_prefill + disagg_transfer/10 + disagg_decode:.1f} ms\")\n",
    "    \n",
    "else:\n",
    "    print(\"Baseline metrics not found. Run 01_Local_Inference_Baseline.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770d0f61",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What We Built:**\n",
    "- Split inference pipeline: Node 1 (prefill) → Node 2 (decode)\n",
    "- TCP/IP-based KV cache transfer\n",
    "- Serialization/deserialization overhead\n",
    "\n",
    "**What We Measured:**\n",
    "- Transfer time: 10-30 ms for typical sequences\n",
    "- Transfer overhead: 15-30% of total latency\n",
    "- Network bandwidth: ~5-10 Gbps effective with TCP\n",
    "\n",
    "**Why This Matters:**\n",
    "- Proved disaggregation works architecturally\n",
    "- Identified network as bottleneck (not compute)\n",
    "- Transfer overhead too high for production\n",
    "\n",
    "**What's Next:**\n",
    "- [04_NIXL_Integration.ipynb](04_NIXL_Integration.ipynb) - Replace TCP with RDMA/NIXL for 10x faster transfer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
