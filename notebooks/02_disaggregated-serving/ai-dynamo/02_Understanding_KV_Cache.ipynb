{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dc4012",
   "metadata": {},
   "source": [
    "# Understanding KV Cache\n",
    "\n",
    "The KV cache is what makes disaggregated serving possible—and expensive. Before we split inference across nodes, we need to understand what this cache contains, how big it is, and why transferring it matters.\n",
    "\n",
    "## What is KV Cache?\n",
    "\n",
    "In transformer models, each token attends to all previous tokens. Without caching, we'd recompute these attention keys and values for every new token—wasteful.\n",
    "\n",
    "**Systems analogy**: KV cache is like a session cache in web servers:\n",
    "- Prefill phase = initial request processing, populate cache\n",
    "- Decode phase = subsequent requests, read from cache\n",
    "- Transfer = moving cache between servers for load balancing\n",
    "\n",
    "## Why This Matters for Disaggregation\n",
    "\n",
    "When we split prefill (node 1) and decode (node 2):\n",
    "1. Node 1 processes prompt → generates KV cache\n",
    "2. Transfer KV cache from node 1 to node 2\n",
    "3. Node 2 uses cache to generate tokens\n",
    "\n",
    "The transfer speed determines if disaggregation is worth it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805202ca",
   "metadata": {},
   "source": [
    "## Step 1: Load Model and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06db5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "\n",
    "# Load environment config\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    MODEL_NAME = env_config['model']['name']\n",
    "else:\n",
    "    MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"Loading model for KV cache inspection...\\n\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "# We use HuggingFace Transformers directly (not vLLM) to access internal state\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"  Parameters: {model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"  Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"  Attention heads: {model.config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748fabd9",
   "metadata": {},
   "source": [
    "## Step 2: Generate with KV Cache Capture\n",
    "\n",
    "Run inference while capturing the KV cache state. This shows what actually gets stored and transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_kv_cache_analysis(prompt, max_new_tokens=50):\n",
    "    \"\"\"\n",
    "    Generate text and capture KV cache for analysis.\n",
    "    Returns: generated text, KV cache, and metrics\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"Input tokens: {input_length}\\n\")\n",
    "    \n",
    "    # Generate with cache\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy decoding\n",
    "            return_dict_in_generate=True,\n",
    "            output_attentions=False,\n",
    "            use_cache=True,  # Enable KV caching\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode output\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs.sequences[0][input_length:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    output_length = outputs.sequences.shape[1] - input_length\n",
    "    \n",
    "    print(f\"Output tokens: {output_length}\")\n",
    "    print(f\"Generated: '{generated_text}'\\n\")\n",
    "    \n",
    "    # Analyze cache structure\n",
    "    # past_key_values is a tuple of (key, value) pairs for each layer\n",
    "    if hasattr(outputs, 'past_key_values') and outputs.past_key_values:\n",
    "        past_kv = outputs.past_key_values\n",
    "    else:\n",
    "        # For models that don't return past_key_values in generate output\n",
    "        # we need to do a forward pass to get them\n",
    "        with torch.no_grad():\n",
    "            result = model(**inputs, use_cache=True)\n",
    "            past_kv = result.past_key_values\n",
    "    \n",
    "    return {\n",
    "        'text': generated_text,\n",
    "        'input_tokens': input_length,\n",
    "        'output_tokens': output_length,\n",
    "        'kv_cache': past_kv\n",
    "    }\n",
    "\n",
    "# Test with a sample prompt\n",
    "test_prompt = \"Explain load balancing in distributed systems.\"\n",
    "result = generate_with_kv_cache_analysis(test_prompt, max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7aeae",
   "metadata": {},
   "source": [
    "## Step 3: Analyze KV Cache Structure\n",
    "\n",
    "Examine the shape and size of the KV cache. This is what moves between nodes in disaggregated serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64caaec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_kv_cache(kv_cache):\n",
    "    \"\"\"\n",
    "    Analyze KV cache structure and memory usage.\n",
    "    \n",
    "    KV cache structure:\n",
    "    - Tuple of length = num_layers\n",
    "    - Each element is (key_tensor, value_tensor)\n",
    "    - Shape: [batch_size, num_heads, sequence_length, head_dim]\n",
    "    \"\"\"\n",
    "    if not kv_cache:\n",
    "        print(\"No KV cache available\")\n",
    "        return None\n",
    "    \n",
    "    num_layers = len(kv_cache)\n",
    "    \n",
    "    # Get first layer's key and value tensors\n",
    "    first_key, first_value = kv_cache[0]\n",
    "    \n",
    "    # Extract dimensions\n",
    "    batch_size = first_key.shape[0]\n",
    "    num_heads = first_key.shape[1]\n",
    "    seq_length = first_key.shape[2]\n",
    "    head_dim = first_key.shape[3]\n",
    "    \n",
    "    # Calculate memory per layer\n",
    "    # Each layer has 2 tensors (key + value)\n",
    "    bytes_per_element = first_key.element_size()  # Usually 2 bytes for float16\n",
    "    elements_per_tensor = batch_size * num_heads * seq_length * head_dim\n",
    "    bytes_per_layer = 2 * elements_per_tensor * bytes_per_element  # 2 for key+value\n",
    "    total_bytes = bytes_per_layer * num_layers\n",
    "    \n",
    "    print(\"KV Cache Structure:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Layers: {num_layers}\")\n",
    "    print(f\"Shape per tensor: [{batch_size}, {num_heads}, {seq_length}, {head_dim}]\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Attention heads: {num_heads}\")\n",
    "    print(f\"  Sequence length: {seq_length}\")\n",
    "    print(f\"  Head dimension: {head_dim}\")\n",
    "    print(f\"\\nMemory Usage:\")\n",
    "    print(f\"  Bytes per element: {bytes_per_element} ({first_key.dtype})\")\n",
    "    print(f\"  Memory per layer: {bytes_per_layer / 1e6:.2f} MB\")\n",
    "    print(f\"  Total KV cache: {total_bytes / 1e6:.2f} MB\")\n",
    "    print(f\"  Per-token memory: {total_bytes / seq_length / 1e6:.4f} MB/token\")\n",
    "    \n",
    "    return {\n",
    "        'num_layers': num_layers,\n",
    "        'batch_size': batch_size,\n",
    "        'num_heads': num_heads,\n",
    "        'seq_length': seq_length,\n",
    "        'head_dim': head_dim,\n",
    "        'total_mb': total_bytes / 1e6,\n",
    "        'per_token_mb': total_bytes / seq_length / 1e6\n",
    "    }\n",
    "\n",
    "kv_analysis = analyze_kv_cache(result['kv_cache'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557a566",
   "metadata": {},
   "source": [
    "## Step 4: KV Cache Growth with Sequence Length\n",
    "\n",
    "The cache grows linearly with sequence length. Longer conversations = more data to transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d0c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different sequence lengths\n",
    "test_lengths = [10, 50, 100, 200, 500]\n",
    "\n",
    "print(\"KV Cache Size vs Sequence Length:\\n\")\n",
    "print(f\"{'Tokens':<10} {'KV Cache Size':<15} {'Transfer @ 10Gbps':<20} {'Transfer @ 100Gbps':<20}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Use the per-token memory from analysis\n",
    "per_token_mb = kv_analysis['per_token_mb'] if kv_analysis else 0.1\n",
    "\n",
    "for num_tokens in test_lengths:\n",
    "    cache_mb = per_token_mb * num_tokens\n",
    "    cache_bytes = cache_mb * 1e6\n",
    "    \n",
    "    # Transfer time calculations\n",
    "    # 10 Gbps = 1.25 GB/s theoretical, ~1 GB/s practical\n",
    "    # 100 Gbps = 12.5 GB/s theoretical, ~10 GB/s practical\n",
    "    transfer_10gbps_ms = (cache_bytes / (1 * 1e9)) * 1000\n",
    "    transfer_100gbps_ms = (cache_bytes / (10 * 1e9)) * 1000\n",
    "    \n",
    "    print(f\"{num_tokens:<10} {cache_mb:>10.2f} MB    {transfer_10gbps_ms:>12.2f} ms      {transfer_100gbps_ms:>12.2f} ms\")\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  • KV cache scales linearly with sequence length\")\n",
    "print(\"  • At 100 tokens: ~10-20 MB to transfer\")\n",
    "print(\"  • At 500 tokens: ~50-100 MB to transfer\")\n",
    "print(\"  • Network speed directly impacts disaggregation overhead\")\n",
    "print(\"  • This is why RDMA matters - 10x faster transfer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c7b75",
   "metadata": {},
   "source": [
    "## Step 5: Transfer Cost Analysis\n",
    "\n",
    "Calculate the overhead of transferring KV cache between nodes. This determines if disaggregation is worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline metrics from previous notebook\n",
    "baseline_file = Path(\"baseline_metrics.json\")\n",
    "if baseline_file.exists():\n",
    "    with open(baseline_file) as f:\n",
    "        baseline = json.load(f)\n",
    "    per_token_latency_ms = baseline['single_request']['latency_ms'] / baseline['single_request']['tokens']\n",
    "else:\n",
    "    per_token_latency_ms = 20  # Fallback estimate\n",
    "\n",
    "print(\"Transfer Cost vs Generation Cost\\n\")\n",
    "print(\"Scenario: 100-token sequence\\n\")\n",
    "\n",
    "# Calculate costs\n",
    "sequence_length = 100\n",
    "kv_cache_mb = per_token_mb * sequence_length\n",
    "kv_cache_bytes = kv_cache_mb * 1e6\n",
    "\n",
    "# Network transfer times\n",
    "transfer_tcp_ms = (kv_cache_bytes / (1 * 1e9)) * 1000  # 10 Gbps TCP\n",
    "transfer_rdma_ms = (kv_cache_bytes / (10 * 1e9)) * 1000  # 100 Gbps RDMA\n",
    "\n",
    "# Generation time for 100 tokens\n",
    "generation_time_ms = per_token_latency_ms * sequence_length\n",
    "\n",
    "# Calculate overhead percentages\n",
    "overhead_tcp = (transfer_tcp_ms / generation_time_ms) * 100\n",
    "overhead_rdma = (transfer_rdma_ms / generation_time_ms) * 100\n",
    "\n",
    "print(f\"KV Cache Transfer:\")\n",
    "print(f\"  Size: {kv_cache_mb:.2f} MB\")\n",
    "print(f\"  Via TCP (10 Gbps): {transfer_tcp_ms:.2f} ms\")\n",
    "print(f\"  Via RDMA (100 Gbps): {transfer_rdma_ms:.2f} ms\")\n",
    "print(f\"\\nToken Generation:\")\n",
    "print(f\"  Time for {sequence_length} tokens: {generation_time_ms:.2f} ms\")\n",
    "print(f\"  Per-token latency: {per_token_latency_ms:.2f} ms\")\n",
    "print(f\"\\nOverhead:\")\n",
    "print(f\"  With TCP: {overhead_tcp:.1f}% overhead\")\n",
    "print(f\"  With RDMA: {overhead_rdma:.1f}% overhead\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*60)\n",
    "if overhead_tcp > 50:\n",
    "    print(\"✗ TCP too slow - transfer time dominates\")\n",
    "    print(\"  Disaggregation not viable without RDMA\")\n",
    "elif overhead_tcp > 20:\n",
    "    print(\"⚠ TCP marginal - significant overhead\")\n",
    "    print(\"  RDMA strongly recommended\")\n",
    "else:\n",
    "    print(\"✓ TCP acceptable - low overhead\")\n",
    "    print(\"  RDMA improves but not critical\")\n",
    "\n",
    "if overhead_rdma < 10:\n",
    "    print(f\"\\n✓ RDMA overhead: {overhead_rdma:.1f}% - disaggregation viable\")\n",
    "else:\n",
    "    print(f\"\\n⚠ RDMA overhead: {overhead_rdma:.1f}% - carefully evaluate benefit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a614207f",
   "metadata": {},
   "source": [
    "## Step 6: Real-World Transfer Test\n",
    "\n",
    "Simulate KV cache transfer to measure actual network performance. This is what happens between prefill and decode nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e778488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def simulate_kv_transfer(kv_cache, method='cpu_to_cpu'):\n",
    "    \"\"\"\n",
    "    Simulate KV cache transfer.\n",
    "    Methods:\n",
    "    - 'cpu_to_cpu': CPU memory copy (simulates network transfer)\n",
    "    - 'gpu_to_cpu': GPU to CPU copy (simulates prefill node sending)\n",
    "    - 'cpu_to_gpu': CPU to GPU copy (simulates decode node receiving)\n",
    "    \"\"\"\n",
    "    if not kv_cache:\n",
    "        return None\n",
    "    \n",
    "    # Flatten KV cache to single tensor for measurement\n",
    "    # In real disaggregation, this would be serialized for network\n",
    "    all_keys = [kv[0] for kv in kv_cache]\n",
    "    all_values = [kv[1] for kv in kv_cache]\n",
    "    \n",
    "    if method == 'gpu_to_cpu':\n",
    "        print(f\"Simulating GPU→CPU transfer (prefill node sending)...\")\n",
    "        start = time.time()\n",
    "        cpu_keys = [k.cpu() for k in all_keys]\n",
    "        cpu_values = [v.cpu() for v in all_values]\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "    elif method == 'cpu_to_gpu':\n",
    "        # First move to CPU\n",
    "        cpu_keys = [k.cpu() for k in all_keys]\n",
    "        cpu_values = [v.cpu() for v in all_values]\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        print(f\"Simulating CPU→GPU transfer (decode node receiving)...\")\n",
    "        start = time.time()\n",
    "        gpu_keys = [k.cuda() for k in cpu_keys]\n",
    "        gpu_values = [v.cuda() for v in cpu_values]\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "    else:  # cpu_to_cpu\n",
    "        # First move to CPU\n",
    "        cpu_keys = [k.cpu() for k in all_keys]\n",
    "        cpu_values = [v.cpu() for v in all_values]\n",
    "        \n",
    "        print(f\"Simulating CPU↔CPU copy (network transfer analog)...\")\n",
    "        start = time.time()\n",
    "        copied_keys = [k.clone() for k in cpu_keys]\n",
    "        copied_values = [v.clone() for v in cpu_values]\n",
    "        elapsed = time.time() - start\n",
    "    \n",
    "    # Calculate total bytes transferred\n",
    "    total_bytes = sum(k.nelement() * k.element_size() for k in all_keys) + \\\n",
    "                  sum(v.nelement() * v.element_size() for v in all_values)\n",
    "    \n",
    "    bandwidth_gbps = (total_bytes / 1e9) / elapsed * 8\n",
    "    \n",
    "    return {\n",
    "        'method': method,\n",
    "        'time_ms': elapsed * 1000,\n",
    "        'size_mb': total_bytes / 1e6,\n",
    "        'bandwidth_gbps': bandwidth_gbps\n",
    "    }\n",
    "\n",
    "# Test all transfer methods\n",
    "print(\"Measuring KV Cache Transfer Performance\\n\")\n",
    "print(f\"{'Method':<20} {'Time':<12} {'Size':<12} {'Bandwidth':<15}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for method in ['gpu_to_cpu', 'cpu_to_gpu', 'cpu_to_cpu']:\n",
    "    result = simulate_kv_transfer(result['kv_cache'], method=method)\n",
    "    if result:\n",
    "        print(f\"{result['method']:<20} {result['time_ms']:>8.2f} ms  {result['size_mb']:>8.2f} MB  {result['bandwidth_gbps']:>10.2f} Gbps\")\n",
    "\n",
    "print(\"\\nNote:\")\n",
    "print(\"  These are PCIe/memory bandwidth measurements\")\n",
    "print(\"  Network transfer adds serialization + actual network latency\")\n",
    "print(\"  RDMA bypasses CPU copies, going GPU→Network→GPU directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebb8e6d",
   "metadata": {},
   "source": [
    "## Step 7: KV Cache Compression Opportunities\n",
    "\n",
    "Can we compress the cache before transfer to reduce overhead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f63966e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def test_kv_compression(kv_cache):\n",
    "    \"\"\"\n",
    "    Test compression ratio on KV cache.\n",
    "    Real-world: compression adds CPU overhead.\n",
    "    \"\"\"\n",
    "    if not kv_cache:\n",
    "        return None\n",
    "    \n",
    "    # Take first layer as representative sample\n",
    "    key_tensor, value_tensor = kv_cache[0]\n",
    "    \n",
    "    # Convert to bytes\n",
    "    key_bytes = key_tensor.cpu().numpy().tobytes()\n",
    "    value_bytes = value_tensor.cpu().numpy().tobytes()\n",
    "    \n",
    "    original_size = len(key_bytes) + len(value_bytes)\n",
    "    \n",
    "    # Test compression\n",
    "    start = time.time()\n",
    "    compressed_key = zlib.compress(key_bytes, level=1)  # Fast compression\n",
    "    compressed_value = zlib.compress(value_bytes, level=1)\n",
    "    compression_time = time.time() - start\n",
    "    \n",
    "    compressed_size = len(compressed_key) + len(compressed_value)\n",
    "    compression_ratio = original_size / compressed_size\n",
    "    \n",
    "    # Decompression\n",
    "    start = time.time()\n",
    "    _ = zlib.decompress(compressed_key)\n",
    "    _ = zlib.decompress(compressed_value)\n",
    "    decompression_time = time.time() - start\n",
    "    \n",
    "    print(\"KV Cache Compression Analysis (single layer)\\n\")\n",
    "    print(f\"Original size: {original_size / 1e6:.2f} MB\")\n",
    "    print(f\"Compressed size: {compressed_size / 1e6:.2f} MB\")\n",
    "    print(f\"Compression ratio: {compression_ratio:.2f}x\")\n",
    "    print(f\"Compression time: {compression_time * 1000:.2f} ms\")\n",
    "    print(f\"Decompression time: {decompression_time * 1000:.2f} ms\")\n",
    "    print(f\"\\nTotal overhead: {(compression_time + decompression_time) * 1000:.2f} ms\")\n",
    "    \n",
    "    # Calculate if compression helps\n",
    "    saved_bytes = original_size - compressed_size\n",
    "    saved_transfer_time_10g = (saved_bytes / (1 * 1e9)) * 1000\n",
    "    saved_transfer_time_100g = (saved_bytes / (10 * 1e9)) * 1000\n",
    "    total_overhead = (compression_time + decompression_time) * 1000\n",
    "    \n",
    "    print(f\"\\nNet benefit @ 10 Gbps: {saved_transfer_time_10g - total_overhead:.2f} ms\")\n",
    "    print(f\"Net benefit @ 100 Gbps: {saved_transfer_time_100g - total_overhead:.2f} ms\")\n",
    "    \n",
    "    if saved_transfer_time_100g > total_overhead:\n",
    "        print(\"\\n✓ Compression beneficial even with RDMA\")\n",
    "    elif saved_transfer_time_10g > total_overhead:\n",
    "        print(\"\\n⚠ Compression only beneficial with slower networks\")\n",
    "    else:\n",
    "        print(\"\\n✗ Compression overhead exceeds transfer savings\")\n",
    "\n",
    "test_kv_compression(result['kv_cache'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2395411",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What is KV Cache:**\n",
    "- Cached attention keys and values from transformer layers\n",
    "- Stored as: [batch, num_heads, sequence_length, head_dim]\n",
    "- Size scales linearly with sequence length\n",
    "- Typical: 0.1-0.2 MB per token for small models, more for larger models\n",
    "\n",
    "**Transfer Costs:**\n",
    "- At 100 tokens: ~10-20 MB to transfer\n",
    "- With 10 Gbps network: 10-20 ms transfer time\n",
    "- With 100 Gbps RDMA: 1-2 ms transfer time\n",
    "- Transfer overhead can be 20-50% of generation time with TCP\n",
    "\n",
    "**Why RDMA Matters:**\n",
    "- 10x faster network (100 Gbps vs 10 Gbps)\n",
    "- Bypasses CPU copies (GPU→Network→GPU)\n",
    "- Reduces transfer from 20ms to 2ms for typical sequences\n",
    "- Makes disaggregation overhead acceptable (<5%)\n",
    "\n",
    "**What's Next:**\n",
    "- [03_Basic_Disaggregation.ipynb](03_Basic_Disaggregation.ipynb) - Split prefill and decode across nodes using standard networking"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
