{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f39894",
   "metadata": {},
   "source": [
    "# Environment Setup for Disaggregated Serving\n",
    "\n",
    "Before building a disaggregated inference system, we need a working baseline environment. This notebook sets up two DGX Spark nodes for distributed LLM inference.\n",
    "\n",
    "## What We're Setting Up\n",
    "\n",
    "- **Node 1 (dgx01)**: Primary inference node\n",
    "- **Node 2 (dgx02)**: Secondary inference node\n",
    "- **Network**: RDMA-capable InfiniBand link between nodes\n",
    "- **Software**: PyTorch, vLLM, monitoring tools\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Disaggregated serving splits the inference pipeline across multiple machines. Before we optimize with RDMA and cache-aware routing, we need working compute and networking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d9715",
   "metadata": {},
   "source": [
    "## Step 1: Verify Node Configuration\n",
    "\n",
    "Check both nodes are accessible and have the expected hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ded05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: spark-01\n",
      "GPUs: 1x NVIDIA GB10\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import socket\n",
    "\n",
    "def get_hostname():\n",
    "    return socket.gethostname()\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Get GPU count and model from nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,count', '--format=csv,noheader'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        gpu_info = result.stdout.strip().split('\\n')\n",
    "        return len(gpu_info), gpu_info[0].split(',')[0] if gpu_info else \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return 0, str(e)\n",
    "\n",
    "hostname = get_hostname()\n",
    "gpu_count, gpu_model = get_gpu_info()\n",
    "\n",
    "print(f\"Hostname: {hostname}\")\n",
    "print(f\"GPUs: {gpu_count}x {gpu_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae4ac5",
   "metadata": {},
   "source": [
    "## Step 2: Check InfiniBand Network\n",
    "\n",
    "Verify RDMA-capable interfaces are present and active. This is what enables fast KV cache transfer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6da0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InfiniBand Devices:\n",
      "  roceP2p1s0f0: Active\n",
      "  roceP2p1s0f1: Active\n",
      "  rocep1s0f0: Active\n",
      "  rocep1s0f1: Active\n"
     ]
    }
   ],
   "source": [
    "def check_ib_devices():\n",
    "    \"\"\"List InfiniBand devices using ibstat\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ibstat', '-l'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        devices = result.stdout.strip().split('\\n')\n",
    "        return [d for d in devices if d]\n",
    "    except subprocess.CalledProcessError:\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        return [\"ibstat not found - install rdma-core\"]\n",
    "\n",
    "def check_ib_link_state(device):\n",
    "    \"\"\"Check if InfiniBand device is active\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ibstat', device],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        # Look for \"State: Active\" in output\n",
    "        return \"State: Active\" in result.stdout\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"InfiniBand Devices:\")\n",
    "ib_devices = check_ib_devices()\n",
    "for device in ib_devices:\n",
    "    state = \"Active\" if check_ib_link_state(device) else \"Down\"\n",
    "    print(f\"  {device}: {state}\")\n",
    "\n",
    "if not ib_devices:\n",
    "    print(\"  No InfiniBand devices found\")\n",
    "    print(\"  Disaggregated serving will use TCP/IP (slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8c0ee",
   "metadata": {},
   "source": [
    "## Step 3: Test Network Connectivity\n",
    "\n",
    "Ping the other node to verify basic network connectivity. Update the IP address for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c05f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connectivity to remote node (192.168.100.11)...\n",
      "✓ Remote node reachable (avg latency: 1.012 ms)\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these IPs for your setup\n",
    "NODE1_IP = \"192.168.100.10\"  # dgx01\n",
    "NODE2_IP = \"192.168.100.11\"  # dgx02\n",
    "\n",
    "def ping_host(ip_address, count=4):\n",
    "    \"\"\"Test network connectivity to remote host\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ping', '-c', str(count), ip_address],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        # Extract average latency from ping output\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'avg' in line:\n",
    "                # Format: min/avg/max/mdev = 0.123/0.456/0.789/0.012 ms\n",
    "                avg_latency = line.split('=')[1].strip().split('/')[1]\n",
    "                return True, f\"{avg_latency} ms\"\n",
    "        return result.returncode == 0, \"success\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "current_node = get_hostname()\n",
    "remote_ip = NODE2_IP if \"01\" in current_node else NODE1_IP\n",
    "\n",
    "print(f\"Testing connectivity to remote node ({remote_ip})...\")\n",
    "success, latency = ping_host(remote_ip)\n",
    "\n",
    "if success:\n",
    "    print(f\"✓ Remote node reachable (avg latency: {latency})\")\n",
    "else:\n",
    "    print(f\"✗ Cannot reach remote node: {latency}\")\n",
    "    print(\"  Check network configuration and IP addresses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654aaa6",
   "metadata": {},
   "source": [
    "## Step 4: Install Core Dependencies\n",
    "\n",
    "Install PyTorch and vLLM for baseline inference. We'll add RDMA libraries later when we optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53af46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installed packages:\n",
      "\n",
      "✓ torch: PyTorch (deep learning framework)\n",
      "✓ vllm: vLLM (high-performance LLM serving)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ transformers: HuggingFace Transformers (model loading)\n",
      "\n",
      "✓ All core packages installed\n"
     ]
    }
   ],
   "source": [
    "# Check if packages are already installed\n",
    "def check_package(package_name):\n",
    "    \"\"\"Check if a Python package is installed\"\"\"\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages = {\n",
    "    'torch': 'PyTorch (deep learning framework)',\n",
    "    'vllm': 'vLLM (high-performance LLM serving)',\n",
    "    'transformers': 'HuggingFace Transformers (model loading)',\n",
    "}\n",
    "\n",
    "print(\"Checking installed packages:\\n\")\n",
    "missing = []\n",
    "for pkg, description in packages.items():\n",
    "    installed = check_package(pkg)\n",
    "    status = \"✓\" if installed else \"✗\"\n",
    "    print(f\"{status} {pkg}: {description}\")\n",
    "    if not installed:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nMissing packages: {', '.join(missing)}\")\n",
    "    print(\"Run: pip install transformers\")\n",
    "    print(\"Run: pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 --index-url https://download.pytorch.org/whl/cu130\")\n",
    "    print(\"Run: pip install vllm==0.13.0 --extra-index-url https://wheels.vllm.ai/0.13.0/cu130\")\n",
    "else:\n",
    "    print(\"\\n✓ All core packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568caf2c",
   "metadata": {},
   "source": [
    "## Step 5: Verify PyTorch GPU Access\n",
    "\n",
    "Confirm PyTorch can see and use the GPUs. This is our compute layer for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e0ae48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_HOME: /usr/local/cuda-13.0\n",
      "LD_LIBRARY_PATH: /usr/local/cuda-13.0/lib64\n",
      "\n",
      "✓ CUDA 13.0 environment configured\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA environment variables for PyTorch\n",
    "import os\n",
    "\n",
    "# CUDA 13.0 paths for DGX Spark\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda-13.0'\n",
    "cuda_lib_path = '/usr/local/cuda-13.0/lib64'\n",
    "\n",
    "if 'LD_LIBRARY_PATH' in os.environ:\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{cuda_lib_path}:{os.environ['LD_LIBRARY_PATH']}\"\n",
    "else:\n",
    "    os.environ['LD_LIBRARY_PATH'] = cuda_lib_path\n",
    "\n",
    "print(f\"CUDA_HOME: {os.environ['CUDA_HOME']}\")\n",
    "print(f\"LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}\")\n",
    "print(\"\\n✓ CUDA 13.0 environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b174617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPUs visible to PyTorch: 1\n",
      "\n",
      "GPU 0: NVIDIA GB10 (128.5 GB)\n",
      "\n",
      "Testing GPU compute...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU compute working\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"GPUs visible to PyTorch: {gpu_count}\\n\")\n",
    "    \n",
    "    for i in range(gpu_count):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        memory_gb = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "        print(f\"GPU {i}: {name} ({memory_gb:.1f} GB)\")\n",
    "    \n",
    "    # Quick compute test\n",
    "    print(\"\\nTesting GPU compute...\")\n",
    "    x = torch.randn(1000, 1000, device='cuda')\n",
    "    y = torch.matmul(x, x)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"✓ GPU compute working\")\n",
    "else:\n",
    "    print(\"✗ CUDA not available\")\n",
    "    print(\"\\nIMPORTANT: Restart the kernel and run cells in order:\")\n",
    "    print(\"1. Kernel → Restart Kernel\")\n",
    "    print(\"2. Run Step 5 cells again (environment setup, then this test)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee10ab0",
   "metadata": {},
   "source": [
    "## Step 6: Download Test Model\n",
    "\n",
    "Download a small model (TinyLlama-1.1B) for testing. This runs fast enough to iterate quickly while learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ea4592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Cache directory: /home/nvidia/.cache/huggingface/hub\n",
      "\n",
      "✓ Model files found in cache\n",
      "  If you want to test download, delete cache directory\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "CACHE_DIR = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "\n",
    "def check_model_cached(model_name):\n",
    "    \"\"\"Check if model is already downloaded\"\"\"\n",
    "    # HuggingFace cache uses hashed directory names\n",
    "    # We'll check if cache dir exists and has any models\n",
    "    if not CACHE_DIR.exists():\n",
    "        return False\n",
    "    \n",
    "    # Look for any cached models\n",
    "    model_dirs = list(CACHE_DIR.glob(\"models--*\"))\n",
    "    return len(model_dirs) > 0\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\\n\")\n",
    "\n",
    "if check_model_cached(MODEL_NAME):\n",
    "    print(\"✓ Model files found in cache\")\n",
    "    print(\"  If you want to test download, delete cache directory\")\n",
    "else:\n",
    "    print(\"Model not cached. Downloading...\")\n",
    "    print(\"  This will happen automatically on first inference\")\n",
    "    print(\"  Or run: huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505c3ae",
   "metadata": {},
   "source": [
    "## Step 7: Environment Summary\n",
    "\n",
    "Collect all configuration details for reference in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "939af176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying remote node at 192.168.100.11...\n",
      "✓ Remote node: spark-02\n",
      "  GPUs: 1x NVIDIA GB10\n",
      "  IB devices: roceP2p1s0f0, roceP2p1s0f1, rocep1s0f0, rocep1s0f1\n",
      "\n",
      "Environment Configuration:\n",
      "{\n",
      "  \"timestamp\": \"2026-02-04T18:07:23.159361\",\n",
      "  \"hostname\": \"spark-01\",\n",
      "  \"gpus\": {\n",
      "    \"count\": 1,\n",
      "    \"model\": \"NVIDIA GB10\"\n",
      "  },\n",
      "  \"nodes\": {\n",
      "    \"spark-01\": {\n",
      "      \"ip\": \"192.168.100.10\",\n",
      "      \"gpus\": {\n",
      "        \"count\": 1,\n",
      "        \"model\": \"NVIDIA GB10\"\n",
      "      },\n",
      "      \"ib_devices\": [\n",
      "        \"roceP2p1s0f0\",\n",
      "        \"roceP2p1s0f1\",\n",
      "        \"rocep1s0f0\",\n",
      "        \"rocep1s0f1\"\n",
      "      ]\n",
      "    },\n",
      "    \"spark-02\": {\n",
      "      \"ip\": \"192.168.100.11\",\n",
      "      \"gpus\": {\n",
      "        \"count\": 1,\n",
      "        \"model\": \"NVIDIA GB10\"\n",
      "      },\n",
      "      \"ib_devices\": [\n",
      "        \"roceP2p1s0f0\",\n",
      "        \"roceP2p1s0f1\",\n",
      "        \"rocep1s0f0\",\n",
      "        \"rocep1s0f1\"\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"network\": {\n",
      "    \"node1_ip\": \"192.168.100.10\",\n",
      "    \"node2_ip\": \"192.168.100.11\",\n",
      "    \"ib_devices\": [\n",
      "      \"roceP2p1s0f0\",\n",
      "      \"roceP2p1s0f1\",\n",
      "      \"rocep1s0f0\",\n",
      "      \"rocep1s0f1\"\n",
      "    ]\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"name\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
      "    \"cache_dir\": \"/home/nvidia/.cache/huggingface/hub\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Configuration saved to: environment_config.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_remote_info(remote_ip):\n",
    "    \"\"\"Query remote node hostname, GPU, and InfiniBand configuration via SSH\"\"\"\n",
    "    try:\n",
    "        # Get hostname\n",
    "        hostname_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{remote_ip}', 'hostname'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        # Get GPU info\n",
    "        gpu_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{remote_ip}', \n",
    "             'nvidia-smi --query-gpu=name,count --format=csv,noheader'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        # Get InfiniBand devices\n",
    "        ib_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{remote_ip}', 'ibstat -l'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if hostname_result.returncode == 0 and gpu_result.returncode == 0:\n",
    "            hostname = hostname_result.stdout.strip()\n",
    "            gpu_info = gpu_result.stdout.strip().split('\\n')\n",
    "            gpu_count = len(gpu_info)\n",
    "            gpu_model = gpu_info[0].split(',')[0] if gpu_info else \"Unknown\"\n",
    "            \n",
    "            # Parse IB devices\n",
    "            if ib_result.returncode == 0:\n",
    "                ib_devices = [d for d in ib_result.stdout.strip().split('\\n') if d]\n",
    "            else:\n",
    "                ib_devices = [\"ibstat not found\"]\n",
    "            \n",
    "            return hostname, gpu_count, gpu_model, ib_devices\n",
    "        else:\n",
    "            return None, None, \"SSH failed\", []\n",
    "    except Exception as e:\n",
    "        return None, None, str(e), []\n",
    "\n",
    "# Gather local node information\n",
    "local_node = get_hostname()\n",
    "env_config = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hostname\": local_node,  # For backward compatibility\n",
    "    \"gpus\": {\n",
    "        \"count\": gpu_count,\n",
    "        \"model\": gpu_model\n",
    "    },\n",
    "    \"nodes\": {\n",
    "        local_node: {\n",
    "            \"ip\": NODE1_IP if \"01\" in local_node else NODE2_IP,\n",
    "            \"gpus\": {\n",
    "                \"count\": gpu_count,\n",
    "                \"model\": gpu_model\n",
    "            },\n",
    "            \"ib_devices\": ib_devices\n",
    "        }\n",
    "    },\n",
    "    \"network\": {\n",
    "        \"node1_ip\": NODE1_IP,\n",
    "        \"node2_ip\": NODE2_IP,\n",
    "        \"ib_devices\": ib_devices\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": MODEL_NAME,\n",
    "        \"cache_dir\": str(CACHE_DIR)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Try to query remote node\n",
    "remote_ip = NODE2_IP if \"01\" in local_node else NODE1_IP\n",
    "print(f\"Querying remote node at {remote_ip}...\")\n",
    "remote_hostname, remote_gpu_count, remote_gpu_model, remote_ib_devices = get_remote_info(remote_ip)\n",
    "\n",
    "if remote_hostname:\n",
    "    env_config[\"nodes\"][remote_hostname] = {\n",
    "        \"ip\": remote_ip,\n",
    "        \"gpus\": {\n",
    "            \"count\": remote_gpu_count,\n",
    "            \"model\": remote_gpu_model\n",
    "        },\n",
    "        \"ib_devices\": remote_ib_devices\n",
    "    }\n",
    "    print(f\"✓ Remote node: {remote_hostname}\")\n",
    "    print(f\"  GPUs: {remote_gpu_count}x {remote_gpu_model}\")\n",
    "    print(f\"  IB devices: {', '.join(remote_ib_devices) if remote_ib_devices else 'none'}\")\n",
    "else:\n",
    "    print(f\"✗ Could not query remote node: {remote_gpu_model}\")\n",
    "    print(\"  Check SSH connectivity or run this notebook on both nodes\")\n",
    "\n",
    "# Save to file for reference\n",
    "config_file = Path(\"environment_config.json\")\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(env_config, f, indent=2)\n",
    "\n",
    "print(\"\\nEnvironment Configuration:\")\n",
    "print(json.dumps(env_config, indent=2))\n",
    "print(f\"\\nConfiguration saved to: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea5d1a",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Environment is ready. The next notebook ([01_Local_Inference_Baseline.ipynb](01_Local_Inference_Baseline.ipynb)) will run single-node inference to establish baseline performance.\n",
    "\n",
    "**What we measured here:**\n",
    "- Hardware configuration (GPUs, network)\n",
    "- Network latency between nodes\n",
    "- Software availability (PyTorch, vLLM)\n",
    "\n",
    "**What we'll measure next:**\n",
    "- Throughput (tokens/sec) for local inference\n",
    "- Latency (ms) per request\n",
    "- GPU memory usage patterns\n",
    "\n",
    "This baseline is what we'll compare against when we add disaggregation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
