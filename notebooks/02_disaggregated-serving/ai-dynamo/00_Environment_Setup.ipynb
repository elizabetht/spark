{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28f39894",
   "metadata": {},
   "source": [
    "# Environment Setup for Disaggregated Serving\n",
    "\n",
    "Before building a disaggregated inference system, we need a working baseline environment. This notebook sets up two DGX Spark nodes for distributed LLM inference.\n",
    "\n",
    "## What We're Setting Up\n",
    "\n",
    "- **Node 1 (dgx01)**: Primary inference node\n",
    "- **Node 2 (dgx02)**: Secondary inference node\n",
    "- **Network**: RDMA-capable InfiniBand link between nodes\n",
    "- **Software**: PyTorch, vLLM, monitoring tools\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Disaggregated serving splits the inference pipeline across multiple machines. Before we optimize with RDMA and cache-aware routing, we need working compute and networking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d9715",
   "metadata": {},
   "source": [
    "## Step 1: Verify Node Configuration\n",
    "\n",
    "Check both nodes are accessible and have the expected hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ded05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hostname: spark-01\n",
      "GPUs: 1x NVIDIA GB10\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import socket\n",
    "\n",
    "def get_hostname():\n",
    "    return socket.gethostname()\n",
    "\n",
    "def get_gpu_info():\n",
    "    \"\"\"Get GPU count and model from nvidia-smi\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,count', '--format=csv,noheader'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        gpu_info = result.stdout.strip().split('\\n')\n",
    "        return len(gpu_info), gpu_info[0].split(',')[0] if gpu_info else \"Unknown\"\n",
    "    except Exception as e:\n",
    "        return 0, str(e)\n",
    "\n",
    "hostname = get_hostname()\n",
    "gpu_count, gpu_model = get_gpu_info()\n",
    "\n",
    "print(f\"Hostname: {hostname}\")\n",
    "print(f\"GPUs: {gpu_count}x {gpu_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae4ac5",
   "metadata": {},
   "source": [
    "## Step 2: Check InfiniBand Network\n",
    "\n",
    "Verify RDMA-capable interfaces are present and active. This is what enables fast KV cache transfer later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6da0df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "InfiniBand Devices:\n",
      "  roceP2p1s0f0: Active\n",
      "  roceP2p1s0f1: Active\n",
      "  rocep1s0f0: Active\n",
      "  rocep1s0f1: Active\n"
     ]
    }
   ],
   "source": [
    "def check_ib_devices():\n",
    "    \"\"\"List InfiniBand devices using ibstat\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ibstat', '-l'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        devices = result.stdout.strip().split('\\n')\n",
    "        return [d for d in devices if d]\n",
    "    except subprocess.CalledProcessError:\n",
    "        return []\n",
    "    except FileNotFoundError:\n",
    "        return [\"ibstat not found - install rdma-core\"]\n",
    "\n",
    "def check_ib_link_state(device):\n",
    "    \"\"\"Check if InfiniBand device is active\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ibstat', device],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True\n",
    "        )\n",
    "        # Look for \"State: Active\" in output\n",
    "        return \"State: Active\" in result.stdout\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "print(\"InfiniBand Devices:\")\n",
    "ib_devices = check_ib_devices()\n",
    "for device in ib_devices:\n",
    "    state = \"Active\" if check_ib_link_state(device) else \"Down\"\n",
    "    print(f\"  {device}: {state}\")\n",
    "\n",
    "if not ib_devices:\n",
    "    print(\"  No InfiniBand devices found\")\n",
    "    print(\"  Disaggregated serving will use TCP/IP (slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf8c0ee",
   "metadata": {},
   "source": [
    "## Step 3: Test Network Connectivity\n",
    "\n",
    "Ping the other node to verify basic network connectivity. Update the IP address for your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c05f59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connectivity to remote node (192.168.100.11)...\n",
      "✓ Remote node reachable (avg latency: 1.140 ms)\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these IPs for your setup\n",
    "NODE1_IP = \"192.168.100.10\"  # dgx01\n",
    "NODE2_IP = \"192.168.100.11\"  # dgx02\n",
    "\n",
    "def ping_host(ip_address, count=4):\n",
    "    \"\"\"Test network connectivity to remote host\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['ping', '-c', str(count), ip_address],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        # Extract average latency from ping output\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'avg' in line:\n",
    "                # Format: min/avg/max/mdev = 0.123/0.456/0.789/0.012 ms\n",
    "                avg_latency = line.split('=')[1].strip().split('/')[1]\n",
    "                return True, f\"{avg_latency} ms\"\n",
    "        return result.returncode == 0, \"success\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "current_node = get_hostname()\n",
    "remote_ip = NODE2_IP if \"01\" in current_node else NODE1_IP\n",
    "\n",
    "print(f\"Testing connectivity to remote node ({remote_ip})...\")\n",
    "success, latency = ping_host(remote_ip)\n",
    "\n",
    "if success:\n",
    "    print(f\"✓ Remote node reachable (avg latency: {latency})\")\n",
    "else:\n",
    "    print(f\"✗ Cannot reach remote node: {latency}\")\n",
    "    print(\"  Check network configuration and IP addresses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6654aaa6",
   "metadata": {},
   "source": [
    "## Step 4: Install Core Dependencies\n",
    "\n",
    "Install PyTorch and vLLM for baseline inference. We'll add RDMA libraries later when we optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53af46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installed packages:\n",
      "\n",
      "✓ torch: PyTorch (deep learning framework)\n",
      "✗ vllm: vLLM (high-performance LLM serving)\n",
      "✗ transformers: HuggingFace Transformers (model loading)\n",
      "\n",
      "Missing packages: vllm, transformers\n",
      "Run: pip install torch==2.9.1 --index-url https://download.pytorch.org/whl/cu130 vllm==0.15.0 --extra-index-url https://wheels.vllm.ai/0.15.0/cu130 transformers\n"
     ]
    }
   ],
   "source": [
    "# Check if packages are already installed\n",
    "def check_package(package_name):\n",
    "    \"\"\"Check if a Python package is installed\"\"\"\n",
    "    try:\n",
    "        __import__(package_name)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "packages = {\n",
    "    'torch': 'PyTorch (deep learning framework)',\n",
    "    'vllm': 'vLLM (high-performance LLM serving)',\n",
    "    'transformers': 'HuggingFace Transformers (model loading)',\n",
    "}\n",
    "\n",
    "print(\"Checking installed packages:\\n\")\n",
    "missing = []\n",
    "for pkg, description in packages.items():\n",
    "    installed = check_package(pkg)\n",
    "    status = \"✓\" if installed else \"✗\"\n",
    "    print(f\"{status} {pkg}: {description}\")\n",
    "    if not installed:\n",
    "        missing.append(pkg)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nMissing packages: {', '.join(missing)}\")\n",
    "    print(\"Run: pip install torch==2.9.1 --index-url https://download.pytorch.org/whl/cu130 vllm==0.15.0 --extra-index-url https://wheels.vllm.ai/0.15.0/cu130 transformers\")\n",
    "else:\n",
    "    print(\"\\n✓ All core packages installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568caf2c",
   "metadata": {},
   "source": [
    "## Step 5: Verify PyTorch GPU Access\n",
    "\n",
    "Confirm PyTorch can see and use the GPUs. This is our compute layer for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b174617",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    \n",
    "    cuda_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {cuda_available}\")\n",
    "    \n",
    "    if cuda_available:\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        print(f\"GPUs visible to PyTorch: {gpu_count}\\n\")\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "            name = torch.cuda.get_device_name(i)\n",
    "            memory_gb = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
    "            print(f\"GPU {i}: {name} ({memory_gb:.1f} GB)\")\n",
    "        \n",
    "        # Quick compute test\n",
    "        print(\"\\nTesting GPU compute...\")\n",
    "        x = torch.randn(1000, 1000, device='cuda')\n",
    "        y = torch.matmul(x, x)\n",
    "        torch.cuda.synchronize()\n",
    "        print(\"✓ GPU compute working\")\n",
    "    else:\n",
    "        print(\"✗ CUDA not available - check GPU drivers\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed. Run: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee10ab0",
   "metadata": {},
   "source": [
    "## Step 6: Download Test Model\n",
    "\n",
    "Download a small model (TinyLlama-1.1B) for testing. This runs fast enough to iterate quickly while learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ea4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "CACHE_DIR = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "\n",
    "def check_model_cached(model_name):\n",
    "    \"\"\"Check if model is already downloaded\"\"\"\n",
    "    # HuggingFace cache uses hashed directory names\n",
    "    # We'll check if cache dir exists and has any models\n",
    "    if not CACHE_DIR.exists():\n",
    "        return False\n",
    "    \n",
    "    # Look for any cached models\n",
    "    model_dirs = list(CACHE_DIR.glob(\"models--*\"))\n",
    "    return len(model_dirs) > 0\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Cache directory: {CACHE_DIR}\\n\")\n",
    "\n",
    "if check_model_cached(MODEL_NAME):\n",
    "    print(\"✓ Model files found in cache\")\n",
    "    print(\"  If you want to test download, delete cache directory\")\n",
    "else:\n",
    "    print(\"Model not cached. Downloading...\")\n",
    "    print(\"  This will happen automatically on first inference\")\n",
    "    print(\"  Or run: huggingface-cli download TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f505c3ae",
   "metadata": {},
   "source": [
    "## Step 7: Environment Summary\n",
    "\n",
    "Collect all configuration details for reference in later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939af176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Gather environment information\n",
    "env_config = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hostname\": get_hostname(),\n",
    "    \"gpus\": {\n",
    "        \"count\": gpu_count,\n",
    "        \"model\": gpu_model\n",
    "    },\n",
    "    \"network\": {\n",
    "        \"node1_ip\": NODE1_IP,\n",
    "        \"node2_ip\": NODE2_IP,\n",
    "        \"ib_devices\": ib_devices\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": MODEL_NAME,\n",
    "        \"cache_dir\": str(CACHE_DIR)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to file for reference\n",
    "config_file = Path(\"environment_config.json\")\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(env_config, f, indent=2)\n",
    "\n",
    "print(\"Environment Configuration:\")\n",
    "print(json.dumps(env_config, indent=2))\n",
    "print(f\"\\nConfiguration saved to: {config_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ea5d1a",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Environment is ready. The next notebook ([01_Local_Inference_Baseline.ipynb](01_Local_Inference_Baseline.ipynb)) will run single-node inference to establish baseline performance.\n",
    "\n",
    "**What we measured here:**\n",
    "- Hardware configuration (GPUs, network)\n",
    "- Network latency between nodes\n",
    "- Software availability (PyTorch, vLLM)\n",
    "\n",
    "**What we'll measure next:**\n",
    "- Throughput (tokens/sec) for local inference\n",
    "- Latency (ms) per request\n",
    "- GPU memory usage patterns\n",
    "\n",
    "This baseline is what we'll compare against when we add disaggregation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
