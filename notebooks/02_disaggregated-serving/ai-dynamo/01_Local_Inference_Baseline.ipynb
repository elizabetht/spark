{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7d3764",
   "metadata": {},
   "source": [
    "# Local Inference Baseline\n",
    "\n",
    "Before optimizing with disaggregation, we need to measure single-node performance. This is the \"before\" measurement.\n",
    "\n",
    "## What We're Measuring\n",
    "\n",
    "- **Throughput**: Tokens generated per second\n",
    "- **Latency**: Time from request to first token (TTFT) and per-token latency\n",
    "- **Memory**: GPU memory usage patterns\n",
    "- **Bottlenecks**: Where time is spent (prefill vs decode)\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Disaggregated serving claims to improve throughput by splitting prefill and decode. To evaluate that claim, we need honest baseline numbers from a well-configured single-node setup using vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab4d7",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a734dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from environment_config.json\n",
      "Hostname: spark-01\n",
      "GPUs: 1x NVIDIA GB10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load configuration from previous notebook\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    print(f\"Loaded config from {config_file}\")\n",
    "    print(f\"Hostname: {env_config['hostname']}\")\n",
    "    print(f\"GPUs: {env_config['gpus']['count']}x {env_config['gpus']['model']}\")\n",
    "else:\n",
    "    print(\"Config file not found. Run 00_Environment_Setup.ipynb first\")\n",
    "    env_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7828b",
   "metadata": {},
   "source": [
    "## Step 2: Initialize vLLM Engine\n",
    "\n",
    "vLLM is a high-performance inference engine with continuous batching and PagedAttention. This is our baseline—not naive sequential inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1fd53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for: models--meta-llama--Llama-3.1-8B-Instruct\n",
      "Found: 1 matches\n",
      "✓ Found cached model at /home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "Loading model from local cache (offline mode)\n",
      "Loading model...\n",
      "INFO 02-04 22:44:09 [utils.py:253] non-default args: {'tokenizer': '/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', 'disable_log_stats': True, 'model': '/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-04 22:44:09 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
      "INFO 02-04 22:44:09 [model.py:1661] Using max model len 131072\n",
      "INFO 02-04 22:44:09 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m INFO 02-04 22:44:09 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', speculative_config=None, tokenizer='/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m /home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     (8.0) - (12.0)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     \n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] EngineCore failed to start.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     self._init_executor()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     self.driver_worker.init_device()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py\", line 326, in init_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     self.worker.init_device()  # type: ignore\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 216, in init_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     current_platform.set_device(self.device)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/platforms/cuda.py\", line 123, in set_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     torch.cuda.set_device(device)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866]     torch._C._cuda_setDevice(device)\n",
      "ERROR 02-04 22:44:10 [core.py:866] torch.AcceleratorError: CUDA error: out of memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m \u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m ERROR 02-04 22:44:10 [core.py:866] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     self.run()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 870, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     raise e\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 857, in run_engine_core\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 637, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     super().__init__(\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 102, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 101, in __init__\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     self._init_executor()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     self.driver_worker.init_device()\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/worker/worker_base.py\", line 326, in init_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 216, in init_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     current_platform.set_device(self.device)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/platforms/cuda.py\", line 123, in set_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     torch.cuda.set_device(device)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m   File \"/home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py\", line 567, in set_device\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m     torch._C._cuda_setDevice(device)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m torch.AcceleratorError: CUDA error: out of memory\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m Search for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=910524)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m start_time = time.time()\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Use the actual snapshot path to force offline loading\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Explicitly set tokenizer path too\u001b[39;49;00m\n\u001b[32m     63\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m load_time = time.time() - start_time\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Model loaded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:351\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, structured_outputs_config, profiler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    316\u001b[39m engine_args = EngineArgs(\n\u001b[32m    317\u001b[39m     model=model,\n\u001b[32m    318\u001b[39m     runner=runner,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs,\n\u001b[32m    347\u001b[39m )\n\u001b[32m    349\u001b[39m log_non_default_args(engine_args)\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    356\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:183\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    180\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:109\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, aggregate_engine_logging, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: StatLoggerManager | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:93\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     89\u001b[39m         vllm_config, executor_class, log_stats\n\u001b[32m     90\u001b[39m     )\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:648\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    646\u001b[39m     \u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor], log_stats: \u001b[38;5;28mbool\u001b[39m\n\u001b[32m    647\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[EngineCoreOutputs | \u001b[38;5;167;01mException\u001b[39;00m]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:477\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    474\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    476\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:903\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    902\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/vllm/v1/engine/utils.py:960\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    958\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    959\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    961\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    962\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    963\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m     )\n\u001b[32m    966\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    967\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set offline mode BEFORE any HuggingFace operations\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "\n",
    "# Set CUDA/Triton compilation environment variables\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '12.0'  # Use 12.0 as base for compatibility\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Check if model is cached locally\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "cache_dir_parent = Path.home() / \".cache\" / \"huggingface\"\n",
    "model_slug = MODEL_NAME.replace(\"/\", \"--\")\n",
    "\n",
    "# Try both possible locations\n",
    "model_cache = list(cache_dir.glob(f\"models--{model_slug}*\"))\n",
    "if not model_cache:\n",
    "    model_cache = list(cache_dir_parent.glob(f\"models--{model_slug}*\"))\n",
    "\n",
    "print(f\"Looking for: models--{model_slug}\")\n",
    "print(f\"Found: {len(model_cache)} matches\")\n",
    "\n",
    "if model_cache:\n",
    "    # Find the actual model files in the snapshots directory\n",
    "    cache_base = model_cache[0]\n",
    "    snapshots_dir = cache_base / \"snapshots\"\n",
    "    \n",
    "    if snapshots_dir.exists():\n",
    "        # Get the first (and usually only) snapshot\n",
    "        snapshot_dirs = list(snapshots_dir.iterdir())\n",
    "        if snapshot_dirs:\n",
    "            model_path = str(snapshot_dirs[0])\n",
    "            print(f\"✓ Found cached model at {model_path}\")\n",
    "        else:\n",
    "            model_path = str(cache_base)\n",
    "            print(f\"✓ Found cached model at {model_path} (no snapshots)\")\n",
    "    else:\n",
    "        model_path = str(cache_base)\n",
    "        print(f\"✓ Found cached model at {model_path}\")\n",
    "    \n",
    "    print(f\"Loading model from local cache (offline mode)\")\n",
    "else:\n",
    "    print(f\"✗ Model not cached\")\n",
    "    raise FileNotFoundError(f\"Model {MODEL_NAME} not found in cache and no internet access\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the actual snapshot path to force offline loading\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,\n",
    "    tokenizer=model_path  # Explicitly set tokenizer path too\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"✓ Model loaded in {load_time:.2f} seconds\")\n",
    "\n",
    "# Check GPU memory usage after model load\n",
    "if torch.cuda.is_available():\n",
    "    # Force synchronization to ensure all GPU operations complete\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # PyTorch memory tracking (may show 0 if vLLM uses its own allocator)\n",
    "    allocated_mb = torch.cuda.memory_allocated() / 1e6\n",
    "    reserved_mb = torch.cuda.memory_reserved() / 1e6\n",
    "    \n",
    "    # Get actual GPU memory usage via nvidia-smi\n",
    "    import subprocess\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True, text=True, check=True\n",
    "        )\n",
    "        gpu_memory_used = int(result.stdout.strip().split('\\n')[0])\n",
    "        print(f\"GPU Memory (nvidia-smi): {gpu_memory_used} MB used\")\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if allocated_mb > 0:\n",
    "        print(f\"GPU Memory (PyTorch): {allocated_mb:.0f} MB allocated, {reserved_mb:.0f} MB reserved\")\n",
    "    else:\n",
    "        print(f\"Note: PyTorch memory tracking shows 0 MB (vLLM uses its own memory allocator)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49720282",
   "metadata": {},
   "source": [
    "## Step 3: Single Request Latency Test\n",
    "\n",
    "Measure time from request submission to response completion for a single prompt. This shows best-case latency with no batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b075b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Explain how HTTP load balancers work in 3 sentences.'\n",
      "\n",
      "Running single request...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 499.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.90s/it, est. speed input: 2.03 toks/s, output: 14.49 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Total latency: 6916.3 ms\n",
      "  Tokens generated: 100\n",
      "  Throughput: 14.5 tokens/sec\n",
      "  Per-token latency: 69.2 ms/token\n",
      "\n",
      "Output:\n",
      " An HTTP load balancer distributes incoming HTTP traffic across multiple servers to improve responsiveness, reliability, and scalability. It does this by routing each incoming request to the server that is best suited to handle it, based on factors such as server load, response time, and availability. By distributing the load across multiple servers, an HTTP load balancer helps to prevent any one server from becoming overwhelmed and ensures that users can access the application or service without interruption.\n",
      "What is the primary function of an HTTP load balancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"Explain how HTTP load balancers work in 3 sentences.\"\n",
    "\n",
    "# Sampling parameters - control output length\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,  # Deterministic output\n",
    "    max_tokens=100,   # Limit output length\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "print(f\"Prompt: '{test_prompt}'\\n\")\n",
    "print(\"Running single request...\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate([test_prompt], sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Extract results\n",
    "output_text = outputs[0].outputs[0].text\n",
    "tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "latency_ms = (end - start) * 1000\n",
    "tokens_per_sec = tokens_generated / (end - start)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total latency: {latency_ms:.1f} ms\")\n",
    "print(f\"  Tokens generated: {tokens_generated}\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"  Per-token latency: {latency_ms/tokens_generated:.1f} ms/token\")\n",
    "print(f\"\\nOutput:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe28ea",
   "metadata": {},
   "source": [
    "## Continuous Batching\n",
    "\n",
    "Before we test batch processing, understand what makes vLLM's batching strategy different.\n",
    "\n",
    "vLLM uses **continuous batching**—a dynamic scheduling technique that maximizes GPU utilization.\n",
    "\n",
    "**Traditional Static Batching:**\n",
    "- Batch of 8 requests arrives\n",
    "- Process all 8 until every request finishes (wait for slowest request)\n",
    "- Only then start next batch\n",
    "- GPU sits idle while waiting for stragglers\n",
    "\n",
    "**Continuous Batching (vLLM):**\n",
    "- Request A finishes at token 50 → immediately replace with new Request I\n",
    "- Request B finishes at token 75 → immediately replace with new Request J\n",
    "- GPU stays saturated because new work fills vacant slots\n",
    "- Each iteration processes whoever is still generating\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Traditional Static Batching:\n",
    "Time →\n",
    "Batch 1: [Req1████████] [Req2█████] [Req3███████] [Req4████]\n",
    "         [................wait for slowest................]\n",
    "Batch 2:                                                    [Req5████] [Req6██████] ...\n",
    "         └─ Idle time while waiting ─┘\n",
    "\n",
    "Continuous Batching:\n",
    "Time →\n",
    "Slot 1:  [Req1████████][Req5████][Req9██████]...\n",
    "Slot 2:  [Req2█████][Req6██████][Req10███]...\n",
    "Slot 3:  [Req3███████][Req7████████]...\n",
    "Slot 4:  [Req4████][Req8███][Req11█████]...\n",
    "         └─ No idle time, slots always filled ─┘\n",
    "```\n",
    "\n",
    "### Systems Engineering Analogy\n",
    "\n",
    "Think of it like connection pooling in a web server:\n",
    "- **Static batching**: \"Wait for all 8 requests to complete, then accept 8 new connections\"\n",
    "- **Continuous batching**: \"As soon as connection 3 closes, accept a new connection immediately\"\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- **2-3x higher throughput** vs static batching\n",
    "- **Lower average latency** (requests don't wait for full batch to clear)\n",
    "- **Better GPU utilization** (no idle time between batches)\n",
    "\n",
    "### Implementation Detail\n",
    "\n",
    "Requires PagedAttention—KV cache must be non-contiguous in memory so you can remove request B's cache without affecting requests A, C, D. Traditional attention requires contiguous tensors, making dynamic batch changes expensive.\n",
    "\n",
    "### Implication for Disaggregation\n",
    "\n",
    "This baseline is already sophisticated. Disaggregation must beat continuous batching, not naive sequential inference. That's a high bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867b190",
   "metadata": {},
   "source": [
    "## Step 4: Batch Processing Test\n",
    "\n",
    "Process multiple requests in a batch. vLLM's continuous batching dynamically manages the batch as requests complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of 8 requests...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 8/8 [00:00<00:00, 137.92it/s]\n",
      "Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.22it/s, est. speed input: 7.62 toks/s, output: 121.88 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Results:\n",
      "  Total time: 6.63 seconds\n",
      "  Total tokens: 800\n",
      "  Throughput: 120.6 tokens/sec\n",
      "  Avg latency per request: 829.0 ms\n",
      "  Speedup vs sequential: 8.34x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate multiple test prompts\n",
    "test_prompts = [\n",
    "    \"What is a REST API?\",\n",
    "    \"Explain database indexing.\",\n",
    "    \"How does DNS work?\",\n",
    "    \"What are microservices?\",\n",
    "    \"Describe container orchestration.\",\n",
    "    \"What is continuous integration?\",\n",
    "    \"Explain message queues.\",\n",
    "    \"How does caching improve performance?\"\n",
    "]\n",
    "\n",
    "batch_size = len(test_prompts)\n",
    "print(f\"Processing batch of {batch_size} requests...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate(test_prompts, sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "total_time = end - start\n",
    "throughput = total_tokens / total_time\n",
    "avg_latency_per_request = (total_time / batch_size) * 1000\n",
    "\n",
    "print(f\"Batch Results:\")\n",
    "print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  Throughput: {throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg latency per request: {avg_latency_per_request:.1f} ms\")\n",
    "print(f\"  Speedup vs sequential: {(batch_size * latency_ms / 1000) / total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20e17d",
   "metadata": {},
   "source": [
    "## Step 5: Understand Prefill vs Decode Time\n",
    "\n",
    "LLM inference has two phases:\n",
    "- **Prefill**: Process input prompt, compute KV cache (compute-bound)\n",
    "- **Decode**: Generate tokens one at a time (memory-bound)\n",
    "\n",
    "Disaggregated serving splits these phases across nodes. Let's measure them separately.\n",
    "\n",
    "**Implementation Detail:**\n",
    "\n",
    "Requires PagedAttention—KV cache must be non-contiguous in memory so you can remove request B's cache without affecting requests A, C, D. Traditional attention requires contiguous tensors, making dynamic batch changes expensive.\n",
    "\n",
    "**Implication for Disaggregation:**\n",
    "\n",
    "This baseline is already sophisticated. Disaggregation must beat continuous batching, not naive sequential inference. That's a high bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3724fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring Prefill vs Decode Time:\n",
      "\n",
      "Prompt Length   Prefill    Decode     Per-Token    Total     \n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 369.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.27it/s, est. speed input: 72.15 toks/s, output: 14.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it, est. speed input: 2.42 toks/s, output: 14.52 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short prompt        76.5ms   1999.7ms       69.0ms   2076.2ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 62.62it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.03it/s, est. speed input: 201.16 toks/s, output: 14.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2000.14it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it, est. speed input: 6.77 toks/s, output: 14.51 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium prompt       92.3ms   1992.0ms       68.7ms   2084.4ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 121.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.15it/s, est. speed input: 381.60 toks/s, output: 12.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1589.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it, est. speed input: 15.03 toks/s, output: 14.54 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long prompt         97.1ms   1984.8ms       68.4ms   2081.9ms\n",
      "\n",
      "Key Insight:\n",
      "  Prefill time grows with input length (compute-bound)\n",
      "  Decode time is per-token and roughly constant (memory-bound)\n",
      "  Disaggregation splits these phases across specialized nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def measure_prefill_decode_split(prompt, num_output_tokens=50):\n",
    "    \"\"\"Measure prefill and decode time separately\"\"\"\n",
    "    \n",
    "    # Prefill: Process prompt with 1 output token\n",
    "    prefill_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=1,  # Only 1 token to measure prefill\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate([prompt], prefill_params)\n",
    "    prefill_time = time.time() - start\n",
    "    \n",
    "    # Full generation to measure decode\n",
    "    decode_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=num_output_tokens,\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate([prompt], decode_params)\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    # Approximate decode time\n",
    "    # (total_time - prefill_time) / (num_tokens - 1)\n",
    "    actual_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "    decode_time = total_time - prefill_time\n",
    "    per_token_decode = decode_time / max(1, actual_tokens - 1)\n",
    "    \n",
    "    return {\n",
    "        'prefill_ms': prefill_time * 1000,\n",
    "        'decode_ms': decode_time * 1000,\n",
    "        'per_token_ms': per_token_decode * 1000,\n",
    "        'total_ms': total_time * 1000,\n",
    "        'tokens': actual_tokens\n",
    "    }\n",
    "\n",
    "# Test with different prompt lengths\n",
    "test_cases = [\n",
    "    (\"Short prompt\", \"What is TCP?\"),\n",
    "    (\"Medium prompt\", \"Explain the OSI network model and describe each layer in detail.\"),\n",
    "    (\"Long prompt\", \"Describe the architecture of a modern distributed database system, including replication strategies, consistency models, and failure handling mechanisms. Explain how these systems achieve high availability.\"),\n",
    "]\n",
    "\n",
    "print(\"Measuring Prefill vs Decode Time:\\n\")\n",
    "print(f\"{'Prompt Length':<15} {'Prefill':<10} {'Decode':<10} {'Per-Token':<12} {'Total':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, prompt in test_cases:\n",
    "    metrics = measure_prefill_decode_split(prompt, num_output_tokens=30)\n",
    "    print(f\"{name:<15} {metrics['prefill_ms']:>8.1f}ms {metrics['decode_ms']:>8.1f}ms {metrics['per_token_ms']:>10.1f}ms {metrics['total_ms']:>8.1f}ms\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  Prefill time grows with input length (compute-bound)\")\n",
    "print(\"  Decode time is per-token and roughly constant (memory-bound)\")\n",
    "print(\"  Disaggregation splits these phases across specialized nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132175d8",
   "metadata": {},
   "source": [
    "## Step 6: Memory Usage Profiling\n",
    "\n",
    "Track GPU memory usage during inference. The KV cache grows with sequence length and is what gets transferred in disaggregated serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75f91a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse memory from nvidia-smi\n",
      "nvidia-smi output:\n",
      "Wed Feb  4 23:24:32 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GB10                    On  |   0000000F:01:00.0 Off |                  N/A |\n",
      "| N/A   42C    P0             11W /  N/A  | Not Supported          |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+-----\n",
      "Running inference with memory tracking...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1212.93it/s]\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.79s/it, est. speed input: 0.58 toks/s, output: 14.51 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not parse memory after inference\n",
      "Memory Usage:\n",
      "  Baseline (model loaded): 0 MB\n",
      "  During inference: 0 MB\n",
      "  Additional for inference: 0 MB\n",
      "  Tokens generated: 200\n",
      "  Memory per token: 0.00 MB/token\n",
      "\n",
      "Note:\n",
      "  Inference memory includes KV cache + activations\n",
      "  KV cache scales linearly with sequence length\n",
      "  In disaggregated serving, this KV cache is transferred between nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    import subprocess\n",
    "    import re\n",
    "    \n",
    "    # Get baseline GPU memory via nvidia-smi\n",
    "    baseline_mb = 0\n",
    "    try:\n",
    "        # Parse full nvidia-smi output - most reliable method\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        output = result.stdout\n",
    "        \n",
    "        # Look for memory usage line like \"16234MiB / 81559MiB\"\n",
    "        # This appears in the main GPU status table (not the processes table)\n",
    "        memory_pattern = r'(\\d+)MiB\\s*/\\s*(\\d+)MiB'\n",
    "        matches = re.findall(memory_pattern, output)\n",
    "        \n",
    "        if matches:\n",
    "            # First match is usually GPU 0\n",
    "            baseline_mb = int(matches[0][0])\n",
    "            total_mb = int(matches[0][1])\n",
    "            print(f\"Baseline GPU Memory: {baseline_mb} MB / {total_mb} MB total\\n\")\n",
    "        else:\n",
    "            print(\"Could not parse memory from nvidia-smi\")\n",
    "            print(f\"nvidia-smi output:\\n{output[:1000]}\")\n",
    "            baseline_mb = 0\n",
    "    except Exception as e:\n",
    "        print(f\"Could not query nvidia-smi: {e}\")\n",
    "        print(\"Continuing without memory tracking...\")\n",
    "        baseline_mb = 0\n",
    "    \n",
    "    print(\"Running inference with memory tracking...\\n\")\n",
    "    \n",
    "    # Generate with longer sequence\n",
    "    long_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=200,\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    prompt = \"Explain distributed systems in detail.\"\n",
    "    outputs = llm.generate([prompt], long_params)\n",
    "    \n",
    "    # Check GPU memory after inference\n",
    "    peak_mb = baseline_mb\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        output = result.stdout\n",
    "        \n",
    "        memory_pattern = r'(\\d+)MiB\\s*/\\s*(\\d+)MiB'\n",
    "        matches = re.findall(memory_pattern, output)\n",
    "        \n",
    "        if matches:\n",
    "            peak_mb = int(matches[0][0])\n",
    "        else:\n",
    "            print(\"Could not parse memory after inference\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not query nvidia-smi after inference: {e}\")\n",
    "    \n",
    "    inference_mb = peak_mb - baseline_mb\n",
    "    \n",
    "    tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "    memory_per_token = inference_mb / tokens_generated if tokens_generated > 0 and inference_mb > 0 else 0\n",
    "    \n",
    "    print(f\"Memory Usage:\")\n",
    "    print(f\"  Baseline (model loaded): {baseline_mb:.0f} MB\")\n",
    "    print(f\"  During inference: {peak_mb:.0f} MB\")\n",
    "    print(f\"  Additional for inference: {inference_mb:.0f} MB\")\n",
    "    print(f\"  Tokens generated: {tokens_generated}\")\n",
    "    print(f\"  Memory per token: {memory_per_token:.2f} MB/token\")\n",
    "    \n",
    "    print(\"\\nNote:\")\n",
    "    print(\"  Inference memory includes KV cache + activations\")\n",
    "    print(\"  KV cache scales linearly with sequence length\")\n",
    "    print(\"  In disaggregated serving, this KV cache is transferred between nodes\")\n",
    "else:\n",
    "    print(\"CUDA not available - skipping memory profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e673e",
   "metadata": {},
   "source": [
    "## Step 7: Baseline Performance Summary\n",
    "\n",
    "Collect all baseline metrics for comparison with disaggregated serving later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c78a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Single Request:\n",
      "  Latency: 6916.3 ms\n",
      "  Throughput: 14.5 tokens/sec\n",
      "\n",
      "Batch Processing (8 requests):\n",
      "  Throughput: 120.6 tokens/sec\n",
      "  Avg Latency: 829.0 ms\n",
      "\n",
      "Memory:\n",
      "  Model: 0 MB\n",
      "  Inference: 0 MB\n",
      "  Per-token: 0.00 MB/token\n",
      "\n",
      "Metrics saved to: baseline_metrics.json\n",
      "\n",
      "This is what we're trying to beat with disaggregation.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"config\": {\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"gpu_memory_utilization\": 0.9\n",
    "    },\n",
    "    \"single_request\": {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"tokens\": tokens_generated,\n",
    "        \"throughput_tokens_per_sec\": tokens_per_sec\n",
    "    },\n",
    "    \"batch_processing\": {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"throughput_tokens_per_sec\": throughput,\n",
    "        \"avg_latency_ms\": avg_latency_per_request\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"baseline_mb\": baseline_mb,\n",
    "        \"peak_mb\": peak_mb,\n",
    "        \"inference_mb\": inference_mb,\n",
    "        \"memory_per_token_mb\": memory_per_token\n",
    "    } if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "metrics_file = Path(\"baseline_metrics.json\")\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSingle Request:\")\n",
    "print(f\"  Latency: {latency_ms:.1f} ms\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nBatch Processing ({batch_size} requests):\")\n",
    "print(f\"  Throughput: {throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg Latency: {avg_latency_per_request:.1f} ms\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nMemory:\")\n",
    "    print(f\"  Model: {baseline_mb:.0f} MB\")\n",
    "    print(f\"  Inference: {inference_mb:.0f} MB\")\n",
    "    print(f\"  Per-token: {memory_per_token:.2f} MB/token\")\n",
    "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
    "print(\"\\nThis is what we're trying to beat with disaggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24daac",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we measured:**\n",
    "- Single-node vLLM performance with continuous batching\n",
    "- Prefill vs decode time split\n",
    "- Memory usage patterns (KV cache growth)\n",
    "\n",
    "**Why this matters:**\n",
    "- These are honest baseline numbers from well-configured infrastructure\n",
    "- Disaggregated serving must beat this to be worthwhile\n",
    "- Memory measurements show what needs to be transferred between nodes\n",
    "\n",
    "**What's next:**\n",
    "- [02_Understanding_KV_Cache.ipynb](02_Understanding_KV_Cache.ipynb) - Deep dive into what the KV cache actually contains and why transferring it is expensive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
