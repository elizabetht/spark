{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7d3764",
   "metadata": {},
   "source": [
    "# Local Inference Baseline\n",
    "\n",
    "Before optimizing with disaggregation, we need to measure single-node performance. This is the \"before\" measurement.\n",
    "\n",
    "## What We're Measuring\n",
    "\n",
    "- **Throughput**: Tokens generated per second\n",
    "- **Latency**: Time from request to first token (TTFT) and per-token latency\n",
    "- **Memory**: GPU memory usage patterns\n",
    "- **Bottlenecks**: Where time is spent (prefill vs decode)\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Disaggregated serving claims to improve throughput by splitting prefill and decode. To evaluate that claim, we need honest baseline numbers from a well-configured single-node setup using vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab4d7",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a734dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load configuration from previous notebook\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    print(f\"Loaded config from {config_file}\")\n",
    "    print(f\"Hostname: {env_config['hostname']}\")\n",
    "    print(f\"GPUs: {env_config['gpus']['count']}x {env_config['gpus']['model']}\")\n",
    "else:\n",
    "    print(\"Config file not found. Run 00_Environment_Setup.ipynb first\")\n",
    "    env_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7828b",
   "metadata": {},
   "source": [
    "## Step 2: Initialize vLLM Engine\n",
    "\n",
    "vLLM is a high-performance inference engine with continuous batching and PagedAttention. This is our baseline—not naive sequential inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd53fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "import time\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "print(\"This may take 30-60 seconds on first run...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize vLLM with default settings\n",
    "# tensor_parallel_size=1 means single GPU (baseline)\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.9,  # Use 90% of GPU memory\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"✓ Model loaded in {load_time:.2f} seconds\")\n",
    "\n",
    "# Check GPU memory usage after model load\n",
    "if torch.cuda.is_available():\n",
    "    allocated_mb = torch.cuda.memory_allocated() / 1e6\n",
    "    reserved_mb = torch.cuda.memory_reserved() / 1e6\n",
    "    print(f\"GPU Memory: {allocated_mb:.0f} MB allocated, {reserved_mb:.0f} MB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49720282",
   "metadata": {},
   "source": [
    "## Step 3: Single Request Latency Test\n",
    "\n",
    "Measure time from request submission to response completion for a single prompt. This shows best-case latency with no batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b075b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"Explain how HTTP load balancers work in 3 sentences.\"\n",
    "\n",
    "# Sampling parameters - control output length\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,  # Deterministic output\n",
    "    max_tokens=100,   # Limit output length\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "print(f\"Prompt: '{test_prompt}'\\n\")\n",
    "print(\"Running single request...\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate([test_prompt], sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Extract results\n",
    "output_text = outputs[0].outputs[0].text\n",
    "tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "latency_ms = (end - start) * 1000\n",
    "tokens_per_sec = tokens_generated / (end - start)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total latency: {latency_ms:.1f} ms\")\n",
    "print(f\"  Tokens generated: {tokens_generated}\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"  Per-token latency: {latency_ms/tokens_generated:.1f} ms/token\")\n",
    "print(f\"\\nOutput:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867b190",
   "metadata": {},
   "source": [
    "## Step 4: Batch Processing Test\n",
    "\n",
    "Process multiple requests in a batch. vLLM uses continuous batching to improve throughput. This is more realistic for production workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple test prompts\n",
    "test_prompts = [\n",
    "    \"What is a REST API?\",\n",
    "    \"Explain database indexing.\",\n",
    "    \"How does DNS work?\",\n",
    "    \"What are microservices?\",\n",
    "    \"Describe container orchestration.\",\n",
    "    \"What is continuous integration?\",\n",
    "    \"Explain message queues.\",\n",
    "    \"How does caching improve performance?\"\n",
    "]\n",
    "\n",
    "batch_size = len(test_prompts)\n",
    "print(f\"Processing batch of {batch_size} requests...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate(test_prompts, sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "total_time = end - start\n",
    "throughput = total_tokens / total_time\n",
    "avg_latency_per_request = (total_time / batch_size) * 1000\n",
    "\n",
    "print(f\"Batch Results:\")\n",
    "print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  Throughput: {throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg latency per request: {avg_latency_per_request:.1f} ms\")\n",
    "print(f\"  Speedup vs sequential: {(batch_size * latency_ms / 1000) / total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20e17d",
   "metadata": {},
   "source": [
    "## Step 5: Understand Prefill vs Decode Time\n",
    "\n",
    "LLM inference has two phases:\n",
    "- **Prefill**: Process input prompt, compute KV cache (compute-bound)\n",
    "- **Decode**: Generate tokens one at a time (memory-bound)\n",
    "\n",
    "Disaggregated serving splits these phases across nodes. Let's measure them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3724fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def measure_prefill_decode_split(prompt, num_output_tokens=50):\n",
    "    \"\"\"Measure prefill and decode time separately\"\"\"\n",
    "    \n",
    "    # Prefill: Process prompt with 1 output token\n",
    "    prefill_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=1,  # Only 1 token to measure prefill\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate([prompt], prefill_params)\n",
    "    prefill_time = time.time() - start\n",
    "    \n",
    "    # Full generation to measure decode\n",
    "    decode_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=num_output_tokens,\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate([prompt], decode_params)\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    # Approximate decode time\n",
    "    # (total_time - prefill_time) / (num_tokens - 1)\n",
    "    actual_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "    decode_time = total_time - prefill_time\n",
    "    per_token_decode = decode_time / max(1, actual_tokens - 1)\n",
    "    \n",
    "    return {\n",
    "        'prefill_ms': prefill_time * 1000,\n",
    "        'decode_ms': decode_time * 1000,\n",
    "        'per_token_ms': per_token_decode * 1000,\n",
    "        'total_ms': total_time * 1000,\n",
    "        'tokens': actual_tokens\n",
    "    }\n",
    "\n",
    "# Test with different prompt lengths\n",
    "test_cases = [\n",
    "    (\"Short prompt\", \"What is TCP?\"),\n",
    "    (\"Medium prompt\", \"Explain the OSI network model and describe each layer in detail.\"),\n",
    "    (\"Long prompt\", \"Describe the architecture of a modern distributed database system, including replication strategies, consistency models, and failure handling mechanisms. Explain how these systems achieve high availability.\"),\n",
    "]\n",
    "\n",
    "print(\"Measuring Prefill vs Decode Time:\\n\")\n",
    "print(f\"{'Prompt Length':<15} {'Prefill':<10} {'Decode':<10} {'Per-Token':<12} {'Total':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, prompt in test_cases:\n",
    "    metrics = measure_prefill_decode_split(prompt, num_output_tokens=30)\n",
    "    print(f\"{name:<15} {metrics['prefill_ms']:>8.1f}ms {metrics['decode_ms']:>8.1f}ms {metrics['per_token_ms']:>10.1f}ms {metrics['total_ms']:>8.1f}ms\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  Prefill time grows with input length (compute-bound)\")\n",
    "print(\"  Decode time is per-token and roughly constant (memory-bound)\")\n",
    "print(\"  Disaggregation splits these phases across specialized nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132175d8",
   "metadata": {},
   "source": [
    "## Step 6: Memory Usage Profiling\n",
    "\n",
    "Track GPU memory usage during inference. The KV cache grows with sequence length and is what gets transferred in disaggregated serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f91a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Reset memory stats\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Get baseline memory\n",
    "    baseline_mb = torch.cuda.memory_allocated() / 1e6\n",
    "    \n",
    "    print(\"Running inference with memory tracking...\\n\")\n",
    "    \n",
    "    # Generate with longer sequence\n",
    "    long_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=200,\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    prompt = \"Explain distributed systems in detail.\"\n",
    "    outputs = llm.generate([prompt], long_params)\n",
    "    \n",
    "    # Check peak memory\n",
    "    peak_mb = torch.cuda.max_memory_allocated() / 1e6\n",
    "    current_mb = torch.cuda.memory_allocated() / 1e6\n",
    "    inference_mb = peak_mb - baseline_mb\n",
    "    \n",
    "    tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "    memory_per_token = inference_mb / tokens_generated\n",
    "    \n",
    "    print(f\"Memory Usage:\")\n",
    "    print(f\"  Baseline (model weights): {baseline_mb:.0f} MB\")\n",
    "    print(f\"  Peak during inference: {peak_mb:.0f} MB\")\n",
    "    print(f\"  Additional for inference: {inference_mb:.0f} MB\")\n",
    "    print(f\"  Tokens generated: {tokens_generated}\")\n",
    "    print(f\"  Memory per token: {memory_per_token:.2f} MB/token\")\n",
    "    \n",
    "    print(\"\\nNote:\")\n",
    "    print(\"  Inference memory includes KV cache + activations\")\n",
    "    print(\"  KV cache scales linearly with sequence length\")\n",
    "    print(\"  In disaggregated serving, this KV cache is transferred between nodes\")\n",
    "else:\n",
    "    print(\"CUDA not available - skipping memory profiling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e673e",
   "metadata": {},
   "source": [
    "## Step 7: Baseline Performance Summary\n",
    "\n",
    "Collect all baseline metrics for comparison with disaggregated serving later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c78a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"config\": {\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"gpu_memory_utilization\": 0.9\n",
    "    },\n",
    "    \"single_request\": {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"tokens\": tokens_generated,\n",
    "        \"throughput_tokens_per_sec\": tokens_per_sec\n",
    "    },\n",
    "    \"batch_processing\": {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"throughput_tokens_per_sec\": throughput,\n",
    "        \"avg_latency_ms\": avg_latency_per_request\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"baseline_mb\": baseline_mb,\n",
    "        \"peak_mb\": peak_mb,\n",
    "        \"inference_mb\": inference_mb,\n",
    "        \"memory_per_token_mb\": memory_per_token\n",
    "    } if torch.cuda.is_available() else None\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "metrics_file = Path(\"baseline_metrics.json\")\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSingle Request:\")\n",
    "print(f\"  Latency: {latency_ms:.1f} ms\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nBatch Processing ({batch_size} requests):\")\n",
    "print(f\"  Throughput: {throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg Latency: {avg_latency_per_request:.1f} ms\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nMemory:\")\n",
    "    print(f\"  Model: {baseline_mb:.0f} MB\")\n",
    "    print(f\"  Inference: {inference_mb:.0f} MB\")\n",
    "    print(f\"  Per-token: {memory_per_token:.2f} MB/token\")\n",
    "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
    "print(\"\\nThis is what we're trying to beat with disaggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24daac",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we measured:**\n",
    "- Single-node vLLM performance with continuous batching\n",
    "- Prefill vs decode time split\n",
    "- Memory usage patterns (KV cache growth)\n",
    "\n",
    "**Why this matters:**\n",
    "- These are honest baseline numbers from well-configured infrastructure\n",
    "- Disaggregated serving must beat this to be worthwhile\n",
    "- Memory measurements show what needs to be transferred between nodes\n",
    "\n",
    "**What's next:**\n",
    "- [02_Understanding_KV_Cache.ipynb](02_Understanding_KV_Cache.ipynb) - Deep dive into what the KV cache actually contains and why transferring it is expensive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
