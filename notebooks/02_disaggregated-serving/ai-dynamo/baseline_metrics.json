{
  "timestamp": "2026-02-04T23:26:15.052721",
  "model": "meta-llama/Llama-3.1-8B-Instruct",
  "config": {
    "tensor_parallel_size": 1,
    "gpu_memory_utilization": 0.9
  },
  "single_request": {
    "latency_ms": 6916.283130645752,
    "tokens": 200,
    "throughput_tokens_per_sec": 14.458633070833136
  },
  "batch_processing": {
    "batch_size": 8,
    "total_tokens": 800,
    "throughput_tokens_per_sec": 120.63406235817034,
    "avg_latency_ms": 828.9532661437988
  },
  "memory": {
    "baseline_mb": 0,
    "peak_mb": 0,
    "inference_mb": 0,
    "memory_per_token_mb": 0
  }
}