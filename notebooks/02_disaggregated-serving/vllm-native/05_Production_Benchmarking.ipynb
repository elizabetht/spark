{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea6bbe42",
   "metadata": {},
   "source": [
    "# Production Benchmarking with guidellm\n",
    "\n",
    "The manual benchmarks in Notebooks 01, 03, and 04 fire 8 concurrent requests and measure end-to-end latency. That approach is useful for learning, but it lacks the rigor needed for production evaluation: no TTFT/TPOT breakdown, no percentile distributions, no load sweeps, and no warmup/cooldown periods.\n",
    "\n",
    "This notebook runs [guidellm](https://github.com/vllm-project/guidellm) against all three serving configurations on the same hardware. guidellm provides standardized metrics (TTFT, TPOT, ITL, P50/P95/P99), automatic rate sweeps, and saturation detection.\n",
    "\n",
    "guidellm runs on the controller (CPU-only node), keeping it off the GPU nodes entirely. This eliminates measurement interference: the load generator and the inference servers never compete for the same CPU, memory, or network interfaces.\n",
    "\n",
    "## What We're Comparing\n",
    "\n",
    "| Configuration | Setup | Endpoint |\n",
    "|---------------|-------|----------|\n",
    "| Single node | 1 vLLM instance on spark-01 | spark-01:8100 |\n",
    "| Replicated | 2 independent instances + round-robin proxy | controller:8192 |\n",
    "| Disaggregated | Prefill on spark-01, decode on spark-02, NIXL/RDMA | controller:8192 |\n",
    "\n",
    "Same model (Llama-3.1-8B-Instruct), same memory budget (0.3 `gpu-memory-utilization`), same prompt/output lengths.\n",
    "\n",
    "## Prerequisites\n",
    "- Notebooks 00, 01, 03, and 04 completed (environment verified, all configurations tested)\n",
    "- vLLM cu130 build installed on both GPU nodes\n",
    "- Model cached on both GPU nodes\n",
    "- Passwordless SSH to controller, spark-01, and spark-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d59520",
   "metadata": {},
   "source": [
    "## Step 1: Install guidellm and Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebca75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_TOKEN: loaded from /home/nvidia/src/github.com/elizabetht/spark/.env (hf_WvokP...)\n",
      "Loaded config from environment_config.json\n",
      "Model path: /home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "Checking guidellm on controller...\n",
      "  guidellm CLI: available on controller\n",
      "Copying tokenizer to controller (/tmp/tokenizer-meta-llama--Llama-3.1-8B-Instruct)...\n",
      "  tokenizer.json: copied\n",
      "  tokenizer_config.json: copied\n",
      "  special_tokens_map.json: copied\n",
      "\n",
      "Benchmark parameters:\n",
      "  Input tokens:  256\n",
      "  Output tokens: 128\n",
      "  Duration:      60s per profile\n",
      "  Load generator: controller (192.168.1.75)\n",
      "  Results dir:   benchmark_results (local, copied from controller after each phase)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from pathlib import Path\n",
    "\n",
    "# Load HuggingFace token from .env file.\n",
    "# guidellm's synthetic data generator needs HF authentication to access\n",
    "# gated models (e.g., Llama). The token is read from .env and passed to\n",
    "# the controller via SSH environment variables.\n",
    "#\n",
    "# Create the file if it does not exist:\n",
    "#   echo 'HF_TOKEN=hf_...' > /home/nvidia/src/github.com/elizabetht/spark/.env\n",
    "env_file = Path(\"/home/nvidia/src/github.com/elizabetht/spark/.env\")\n",
    "HF_TOKEN = None\n",
    "if env_file.exists():\n",
    "    for line in env_file.read_text().splitlines():\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"HF_TOKEN=\"):\n",
    "            HF_TOKEN = line.split(\"=\", 1)[1].strip().strip('\"').strip(\"'\")\n",
    "            print(f\"HF_TOKEN: loaded from {env_file} ({HF_TOKEN[:8]}...)\")\n",
    "            break\n",
    "if not HF_TOKEN:\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "    if HF_TOKEN:\n",
    "        print(f\"HF_TOKEN: loaded from environment ({HF_TOKEN[:8]}...)\")\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"HF_TOKEN not found. guidellm needs it to generate synthetic prompts \"\n",
    "            f\"for gated models.\\n\"\n",
    "            f\"Create {env_file} with:\\n\"\n",
    "            f\"  echo 'HF_TOKEN=hf_your_token_here' > {env_file}\"\n",
    "        )\n",
    "\n",
    "# Load environment config\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    print(f\"Loaded config from {config_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Run 00_Environment_Setup.ipynb first\")\n",
    "\n",
    "# Node configuration\n",
    "NODE1_HOST = env_config['network']['node1_ip']   # spark-01: 192.168.100.10 (IB)\n",
    "NODE2_HOST = env_config['network']['node2_ip']   # spark-02: 192.168.100.11 (IB)\n",
    "NODE1_LAN_HOST = \"192.168.1.76\"                  # spark-01 LAN\n",
    "NODE2_LAN_HOST = \"192.168.1.77\"                  # spark-02 LAN\n",
    "CONTROLLER_HOST = \"192.168.1.75\"                 # controller LAN\n",
    "MODEL_NAME = env_config['model']['name']\n",
    "NODE1_PORT = 8100\n",
    "NODE2_PORT = 8200\n",
    "PROXY_PORT = 8192\n",
    "NIXL_PORT = 5600\n",
    "VENV_PATH = os.path.expanduser(\"~/src/github.com/elizabetht/spark/.venv\")\n",
    "\n",
    "# Find model snapshot path\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "model_slug = MODEL_NAME.replace(\"/\", \"--\")\n",
    "model_cache = list(cache_dir.glob(f\"models--{model_slug}*\"))\n",
    "if model_cache:\n",
    "    snapshots_dir = model_cache[0] / \"snapshots\"\n",
    "    snapshot_dirs = list(snapshots_dir.iterdir()) if snapshots_dir.exists() else []\n",
    "    MODEL_PATH = str(snapshot_dirs[0]) if snapshot_dirs else str(model_cache[0])\n",
    "    print(f\"Model path: {MODEL_PATH}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model {MODEL_NAME} not found in cache\")\n",
    "\n",
    "# Benchmark parameters (consistent across all configurations)\n",
    "INPUT_LEN = 256\n",
    "OUTPUT_LEN = 128\n",
    "BENCH_DURATION = 60       # seconds per benchmark profile\n",
    "RESULTS_DIR = Path(\"benchmark_results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Remote results directory on controller\n",
    "REMOTE_RESULTS_DIR = \"/tmp/benchmark_results\"\n",
    "\n",
    "# Install guidellm on the controller (CPU-only node, no GPU contention)\n",
    "print(\"Checking guidellm on controller...\")\n",
    "check = subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     f'{VENV_PATH}/bin/guidellm --help'],\n",
    "    capture_output=True, text=True, timeout=15\n",
    ")\n",
    "if check.returncode == 0:\n",
    "    print(\"  guidellm CLI: available on controller\")\n",
    "else:\n",
    "    print(\"  guidellm not found on controller, installing...\")\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "         f'{VENV_PATH}/bin/pip install \"guidellm[recommended]\"'],\n",
    "        capture_output=True, text=True, timeout=600\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"  guidellm installed on controller\")\n",
    "    else:\n",
    "        print(f\"  Installation failed: {result.stderr.strip()[-500:]}\")\n",
    "        raise RuntimeError(\"Failed to install guidellm on controller\")\n",
    "\n",
    "# Create remote results directory\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     f'mkdir -p {REMOTE_RESULTS_DIR}'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "\n",
    "# Copy tokenizer to controller so guidellm can generate synthetic prompts\n",
    "# without needing HuggingFace authentication for the gated Llama model.\n",
    "# Only the tokenizer files are needed (~10 MB), not the full model weights.\n",
    "CONTROLLER_TOKENIZER_PATH = f\"/tmp/tokenizer-{model_slug}\"\n",
    "\n",
    "tokenizer_files = [\"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"]\n",
    "\n",
    "print(f\"Copying tokenizer to controller ({CONTROLLER_TOKENIZER_PATH})...\")\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     f'mkdir -p {CONTROLLER_TOKENIZER_PATH}'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "\n",
    "for tf in tokenizer_files:\n",
    "    local_tf = Path(MODEL_PATH) / tf\n",
    "    if local_tf.exists():\n",
    "        scp = subprocess.run(\n",
    "            ['scp', str(local_tf),\n",
    "             f'nvidia@{CONTROLLER_HOST}:{CONTROLLER_TOKENIZER_PATH}/{tf}'],\n",
    "            capture_output=True, text=True, timeout=15\n",
    "        )\n",
    "        if scp.returncode == 0:\n",
    "            print(f\"  {tf}: copied\")\n",
    "        else:\n",
    "            print(f\"  {tf}: SCP failed ({scp.stderr.strip()})\")\n",
    "    else:\n",
    "        print(f\"  {tf}: not found locally (skipping)\")\n",
    "\n",
    "print(f\"\\nBenchmark parameters:\")\n",
    "print(f\"  Input tokens:  {INPUT_LEN}\")\n",
    "print(f\"  Output tokens: {OUTPUT_LEN}\")\n",
    "print(f\"  Duration:      {BENCH_DURATION}s per profile\")\n",
    "print(f\"  Load generator: controller ({CONTROLLER_HOST})\")\n",
    "print(f\"  Results dir:   {RESULTS_DIR} (local, copied from controller after each phase)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3c022",
   "metadata": {},
   "source": [
    "## Step 2: Helper Functions\n",
    "\n",
    "Shared utilities for starting/stopping vLLM instances, proxies, and running guidellm. The load generator (guidellm) runs on the controller, a CPU-only node with no GPU workload. This avoids contention with the vLLM instances on the GPU nodes and provides a consistent measurement point across all three configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25539851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "def wait_for_server(host, port, label, timeout=300, interval=10):\n",
    "    \"\"\"Poll a vLLM server's health endpoint until it responds.\"\"\"\n",
    "    url = f\"http://{host}:{port}/health\"\n",
    "    start = time.time()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            req = urllib.request.Request(url, method='GET')\n",
    "            with urllib.request.urlopen(req, timeout=5) as resp:\n",
    "                if resp.status == 200:\n",
    "                    elapsed = time.time() - start\n",
    "                    print(f\"  {label}: ready ({elapsed:.0f}s)\")\n",
    "                    return True\n",
    "        except (urllib.error.URLError, ConnectionRefusedError, OSError):\n",
    "            pass\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  {label}: waiting... ({elapsed:.0f}s / {timeout}s)\")\n",
    "        time.sleep(interval)\n",
    "    print(f\"  {label}: TIMEOUT after {timeout}s\")\n",
    "    return False\n",
    "\n",
    "\n",
    "def stop_all_services():\n",
    "    \"\"\"Kill all vLLM and proxy processes on all nodes.\"\"\"\n",
    "    print(\"Stopping all services...\")\n",
    "    # Local vLLM (spark-01)\n",
    "    subprocess.run(['pkill', '-f', 'vllm serve'], capture_output=True, timeout=5)\n",
    "    # Remote vLLM (spark-02)\n",
    "    subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{NODE2_HOST}',\n",
    "         'pkill -f \"vllm serve\" 2>/dev/null; true'],\n",
    "        capture_output=True, timeout=10\n",
    "    )\n",
    "    # Proxies on controller\n",
    "    subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "         'pkill -f disagg_proxy.py 2>/dev/null; pkill -f replicated_proxy.py 2>/dev/null; true'],\n",
    "        capture_output=True, timeout=10\n",
    "    )\n",
    "    time.sleep(3)\n",
    "    # Force-kill survivors\n",
    "    subprocess.run(['pkill', '-9', '-f', 'vllm serve'], capture_output=True, timeout=5)\n",
    "    subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{NODE2_HOST}',\n",
    "         'pkill -9 -f \"vllm serve\" 2>/dev/null; true'],\n",
    "        capture_output=True, timeout=10\n",
    "    )\n",
    "    time.sleep(2)\n",
    "    print(\"  All services stopped\")\n",
    "\n",
    "\n",
    "def start_vllm_local(port, kv_transfer_config=None, log_file=\"/tmp/vllm_bench.log\"):\n",
    "    \"\"\"Start a vLLM instance on the local node (spark-01).\"\"\"\n",
    "    cmd = (\n",
    "        f\". {VENV_PATH}/bin/activate && \"\n",
    "        f\"CUDA_HOME=/usr/local/cuda-13.0 \"\n",
    "        f\"HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \"\n",
    "    )\n",
    "    if kv_transfer_config:\n",
    "        cmd += f\"VLLM_NIXL_SIDE_CHANNEL_HOST={NODE1_HOST} VLLM_NIXL_SIDE_CHANNEL_PORT={NIXL_PORT} \"\n",
    "    cmd += (\n",
    "        f\"vllm serve {MODEL_PATH} \"\n",
    "        f\"--port {port} \"\n",
    "        f\"--gpu-memory-utilization 0.3 \"\n",
    "        f\"--tensor-parallel-size 1 \"\n",
    "    )\n",
    "    if kv_transfer_config:\n",
    "        cmd += f\"--kv-transfer-config '{kv_transfer_config}' \"\n",
    "    cmd += f\"> {log_file} 2>&1\"\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        cmd, shell=True,\n",
    "        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    "        preexec_fn=os.setsid\n",
    "    )\n",
    "    return proc\n",
    "\n",
    "\n",
    "def start_vllm_remote(host, port, kv_transfer_config=None, log_file=\"/tmp/vllm_bench.log\"):\n",
    "    \"\"\"Start a vLLM instance on a remote node via SSH.\"\"\"\n",
    "    cmd = (\n",
    "        f\". {VENV_PATH}/bin/activate && \"\n",
    "        f\"CUDA_HOME=/usr/local/cuda-13.0 \"\n",
    "        f\"HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \"\n",
    "    )\n",
    "    if kv_transfer_config:\n",
    "        cmd += f\"VLLM_NIXL_SIDE_CHANNEL_HOST={NODE2_HOST} VLLM_NIXL_SIDE_CHANNEL_PORT={NIXL_PORT} \"\n",
    "    cmd += (\n",
    "        f\"vllm serve {MODEL_PATH} \"\n",
    "        f\"--port {port} \"\n",
    "        f\"--gpu-memory-utilization 0.3 \"\n",
    "        f\"--tensor-parallel-size 1 \"\n",
    "    )\n",
    "    if kv_transfer_config:\n",
    "        cmd += f\"--kv-transfer-config '{kv_transfer_config}' \"\n",
    "    cmd += f\"> {log_file} 2>&1\"\n",
    "\n",
    "    proc = subprocess.Popen(\n",
    "        ['ssh', f'nvidia@{host}', cmd],\n",
    "        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    "        preexec_fn=os.setsid\n",
    "    )\n",
    "    return proc\n",
    "\n",
    "\n",
    "def run_guidellm(target_url, label, profile=\"sweep\", rate=None, extra_args=None):\n",
    "    \"\"\"\n",
    "    Run guidellm benchmark on the controller node via SSH.\n",
    "\n",
    "    guidellm runs on the controller (CPU-only) to avoid contending with\n",
    "    vLLM on the GPU nodes. Results are written to the controller's\n",
    "    /tmp/benchmark_results/<label>/ directory, then copied back to the\n",
    "    local RESULTS_DIR via SCP.\n",
    "\n",
    "    Args:\n",
    "        target_url: Full URL (e.g., http://192.168.1.76:8100)\n",
    "        label: Name for results files (e.g., 'single_node')\n",
    "        profile: 'sweep', 'synchronous', 'concurrent', 'throughput'\n",
    "        rate: Rate parameter (concurrent connections or RPS depending on profile)\n",
    "        extra_args: Additional CLI arguments as a list\n",
    "    \"\"\"\n",
    "    remote_output = f\"{REMOTE_RESULTS_DIR}/{label}\"\n",
    "    local_output = RESULTS_DIR / label\n",
    "    local_output.mkdir(exist_ok=True)\n",
    "\n",
    "    # Build the guidellm command to run on the controller.\n",
    "    # HF_TOKEN is required: guidellm's synthetic data generator downloads\n",
    "    # the model config from HuggingFace to calibrate prompt token counts.\n",
    "    guidellm_cmd = (\n",
    "        f\". {VENV_PATH}/bin/activate && \"\n",
    "        f\"export HF_TOKEN={HF_TOKEN} && \"\n",
    "        f\"mkdir -p {remote_output} && \"\n",
    "        f\"guidellm benchmark \"\n",
    "        f\"--target {target_url} \"\n",
    "        f\"--request-type text_completions \"\n",
    "        f\"--data 'prompt_tokens={INPUT_LEN},output_tokens={OUTPUT_LEN}' \"\n",
    "        f\"--processor {CONTROLLER_TOKENIZER_PATH} \"\n",
    "        f\"--profile {profile} \"\n",
    "        f\"--max-seconds {BENCH_DURATION} \"\n",
    "        f\"--output-path {remote_output}\"\n",
    "    )\n",
    "    if rate is not None:\n",
    "        guidellm_cmd += f\" --rate {rate}\"\n",
    "    if extra_args:\n",
    "        guidellm_cmd += \" \" + \" \".join(extra_args)\n",
    "\n",
    "    print(f\"Running guidellm [{label}] on controller ({CONTROLLER_HOST})\")\n",
    "    print(f\"  Profile: {profile}\")\n",
    "    print(f\"  Target:  {target_url}\")\n",
    "    print(f\"  Remote output: {remote_output}/\")\n",
    "    print(f\"  Duration: ~{BENCH_DURATION}s per rate point\")\n",
    "    print()\n",
    "\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}', guidellm_cmd],\n",
    "        capture_output=False, text=True,\n",
    "        timeout=BENCH_DURATION * 20  # generous timeout for sweep (multiple rate points)\n",
    "    )\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"\\nBenchmark [{label}] completed on controller.\")\n",
    "        # Copy results back to local machine\n",
    "        print(f\"Copying results from controller to {local_output}/...\")\n",
    "        scp_result = subprocess.run(\n",
    "            ['scp', '-r', f'nvidia@{CONTROLLER_HOST}:{remote_output}/',\n",
    "             str(local_output.parent) + '/'],\n",
    "            capture_output=True, text=True, timeout=30\n",
    "        )\n",
    "        if scp_result.returncode == 0:\n",
    "            print(f\"  Results saved to {local_output}/\")\n",
    "        else:\n",
    "            print(f\"  SCP failed: {scp_result.stderr.strip()}\")\n",
    "            print(f\"  Results are still on controller at {remote_output}/\")\n",
    "    else:\n",
    "        print(f\"\\nBenchmark [{label}] failed (exit code {result.returncode})\")\n",
    "\n",
    "    return result.returncode\n",
    "\n",
    "print(\"Helper functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e530c9",
   "metadata": {},
   "source": [
    "## Phase 1: Single Node Benchmark\n",
    "\n",
    "Start one vLLM instance on spark-01 and run guidellm directly against it. No proxy, no second node. This is the baseline that both replicated and disaggregated must beat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f09070c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all services...\n",
      "  All services stopped\n",
      "\n",
      "Starting single-node vLLM on spark-01...\n",
      "  PID: 987244\n",
      "  Log: tail -f /tmp/vllm_single.log\n",
      "\n",
      "Waiting for vLLM to load model...\n",
      "  Single node (spark-01): waiting... (0s / 300s)\n",
      "  Single node (spark-01): waiting... (10s / 300s)\n",
      "  Single node (spark-01): waiting... (20s / 300s)\n",
      "  Single node (spark-01): waiting... (30s / 300s)\n",
      "  Single node (spark-01): waiting... (40s / 300s)\n",
      "  Single node (spark-01): waiting... (50s / 300s)\n",
      "  Single node (spark-01): waiting... (60s / 300s)\n",
      "  Single node (spark-01): waiting... (70s / 300s)\n",
      "  Single node (spark-01): waiting... (80s / 300s)\n",
      "  Single node (spark-01): waiting... (90s / 300s)\n",
      "  Single node (spark-01): waiting... (100s / 300s)\n",
      "  Single node (spark-01): waiting... (110s / 300s)\n",
      "  Single node (spark-01): ready (120s)\n"
     ]
    }
   ],
   "source": [
    "# Ensure clean state\n",
    "stop_all_services()\n",
    "\n",
    "# Start single vLLM instance on spark-01\n",
    "print(\"\\nStarting single-node vLLM on spark-01...\")\n",
    "single_proc = start_vllm_local(NODE1_PORT, log_file=\"/tmp/vllm_single.log\")\n",
    "print(f\"  PID: {single_proc.pid}\")\n",
    "print(f\"  Log: tail -f /tmp/vllm_single.log\")\n",
    "\n",
    "# Wait for ready\n",
    "print(\"\\nWaiting for vLLM to load model...\")\n",
    "ready = wait_for_server(NODE1_HOST, NODE1_PORT, \"Single node (spark-01)\")\n",
    "if not ready:\n",
    "    raise RuntimeError(\"vLLM failed to start. Check /tmp/vllm_single.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "772a1929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running guidellm [single_node] on controller (192.168.1.75)\n",
      "  Profile: sweep\n",
      "  Target:  http://192.168.1.76:8100\n",
      "  Remote output: /tmp/benchmark_results/single_node/\n",
      "  Duration: ~60s per rate point\n",
      "\n",
      "✔ OpenAIHTTPBackend backend validated with model \n",
      "/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/sn\n",
      "apshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "  {'target': 'http://192.168.1.76:8100', 'model': None, 'timeout': 60.0,        \n",
      "  'http2': True, 'follow_redirects': True, 'verify': False, 'openai_paths':     \n",
      "  {'health': 'health', 'models': 'v1/models', 'text_completions':               \n",
      "  'v1/completions', 'chat_completions': 'v1/chat/completions',                  \n",
      "  'audio_transcriptions': 'v1/audio/transcriptions', 'audio_translations':      \n",
      "  'v1/audio/translations'}, 'validate_backend': {'method': 'GET', 'url':        \n",
      "  'http://192.168.1.76:8100/health'}}                                           \n",
      "✔ Processor resolved\n",
      "  Using processor '/tmp/tokenizer-meta-llama--Llama-3.1-8B-Instruct'            \n",
      "✔ Request loader initialized with inf unique requests\n",
      "  {'data': \"['prompt_tokens=256,output_tokens=128']\", 'data_args': '[]',        \n",
      "  'data_samples': -1, 'preprocessors': ['GenerativeColumnMapper',               \n",
      "  'GenerativeTextCompletionsRequestFormatter'], 'collator':                     \n",
      "  'GenerativeRequestCollator', 'sampler': 'None', 'num_workers': 1,             \n",
      "  'random_seed': 42}                                                            \n",
      "✔ Resolved transient phase configurations\n",
      "  Warmup: percent=None value=None mode='prefer_duration'                        \n",
      "  Cooldown: percent=None value=None mode='prefer_duration'                      \n",
      "  Rampup (Throughput/Concurrent): 0.0                                           \n",
      "✔ SweepProfile profile resolved\n",
      "  {'str': \"type_='sweep' completed_strategies=[] constraints={'max_seconds':    \n",
      "  60.0} rampup_duration=0.0 sweep_size=10 strategy_type='constant'              \n",
      "  max_concurrency=None random_seed=42 synchronous_rate=-1.0 throughput_rate=-1.0\n",
      "  async_rates=[] measured_rates=[] strategy_types=['synchronous', 'throughput', \n",
      "  'constant', 'constant', 'constant', 'constant', 'constant', 'constant',       \n",
      "  'constant', 'constant']\", 'type': 'SweepProfile', 'class': 'SweepProfile',    \n",
      "  'module': 'guidellm.benchmark.profiles', 'attributes': {'type_': 'sweep',     \n",
      "  'completed_strategies': [], 'constraints': {'max_seconds': 60.0},             \n",
      "  'rampup_duration': 0.0, 'sweep_size': 10, 'strategy_type': 'constant',        \n",
      "  'max_concurrency': 'None', 'random_seed': 42, 'synchronous_rate': -1.0,       \n",
      "  'throughput_rate': -1.0, 'async_rates': [], 'measured_rates': []}}            \n",
      "✔ Output formats resolved\n",
      "  {'json': \"output_path=PosixPath('/tmp/benchmark_results/single_node')\", 'csv':\n",
      "  \"output_path=PosixPath('/tmp/benchmark_results/single_node')\", 'html':        \n",
      "  \"output_path=PosixPath('/tmp/benchmark_results/single_node')\"}                \n",
      "✔ Setup complete, starting benchmarks...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "╭─ Benchmarks ─────────────────────────────────────────────────────────────────╮\n",
      "│ [0… synch… (… Req:    0.1 req/s,    9.06s Lat,     1.0 Conc,       7 Comp, … │\n",
      "│               Tok:   14.2 gen/s,   42.6 tot/s, 203.4ms TTFT,   69.7ms ITL, … │\n",
      "│ [0… throu… (… Req:    3.1 req/s,   50.04s Lat,   157.6 Conc,     189 Comp, … │\n",
      "│               Tok:  406.0 gen/s, 1217.9 tot/s, 7908.2ms TTFT,  331.7ms ITL,… │\n",
      "│ [0… const… (… Req:    0.4 req/s,    8.75s Lat,     3.6 Conc,      25 Comp, … │\n",
      "│               Tok:   54.4 gen/s,  163.3 tot/s, 223.1ms TTFT,   67.1ms ITL, … │\n",
      "│ [0… const… (… Req:    0.7 req/s,    8.98s Lat,     6.6 Conc,      44 Comp, … │\n",
      "│               Tok:   95.2 gen/s,  285.5 tot/s, 168.6ms TTFT,   69.3ms ITL, … │\n",
      "│ [0… const… (… Req:    1.0 req/s,    9.29s Lat,     9.8 Conc,      63 Comp, … │\n",
      "│               Tok:  135.1 gen/s,  405.4 tot/s, 185.3ms TTFT,   71.7ms ITL, … │\n",
      "│ [0… const… (… Req:    1.3 req/s,    9.61s Lat,    13.0 Conc,      81 Comp, … │\n",
      "│               Tok:  174.6 gen/s,  523.8 tot/s, 161.8ms TTFT,   74.4ms ITL, … │\n",
      "│ [0… const… (… Req:    1.6 req/s,   11.46s Lat,    18.3 Conc,      96 Comp, … │\n",
      "│               Tok:  206.1 gen/s,  618.2 tot/s, 179.9ms TTFT,   88.8ms ITL, … │\n",
      "│ [0… const… (… Req:    1.9 req/s,   12.01s Lat,    22.6 Conc,     113 Comp, … │\n",
      "│               Tok:  242.3 gen/s,  727.0 tot/s, 187.1ms TTFT,   93.1ms ITL, … │\n",
      "│ [0… const… (… Req:    2.1 req/s,   12.46s Lat,    27.0 Conc,     130 Comp, … │\n",
      "│               Tok:  277.8 gen/s,  833.3 tot/s, 183.8ms TTFT,   96.7ms ITL, … │\n",
      "│ [0… const… (… Req:    1.9 req/s,   13.41s Lat,    25.7 Conc,     115 Comp, … │\n",
      "│               Tok:  246.5 gen/s,  739.4 tot/s, 214.8ms TTFT,  103.7ms ITL, … │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "Generating... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ (10/10) [ 0:10:28 < 0:00:00 ]\n",
      "\n",
      "ℹ Run Summary Info\n",
      "|=============|==========|==========|======|======|======|=========|==========|=====|=========|=========|=====|\n",
      "| Benchmark   | Timings                              ||||| Input Tokens           ||| Output Tokens         |||\n",
      "| Strategy    | Start    | End      | Dur  | Warm | Cool | Comp    | Inc      | Err | Comp    | Inc     | Err |\n",
      "|             |          |          | Sec  | Sec  | Sec  | Tot     | Tot      | Tot | Tot     | Tot     | Tot |\n",
      "|-------------|----------|----------|------|------|------|---------|----------|-----|---------|---------|-----|\n",
      "| synchronous | 05:02:54 | 05:03:54 | 60.0 | 0.0  | 0.0  | 1792.0  | 0.0      | 0.0 | 896.0   | 0.0     | 0.0 |\n",
      "| throughput  | 05:03:59 | 05:04:59 | 60.0 | 0.0  | 0.0  | 48384.0 | 130304.0 | 0.0 | 24192.0 | 10264.0 | 0.0 |\n",
      "| constant    | 05:05:03 | 05:06:03 | 60.0 | 0.0  | 0.0  | 6400.0  | 1024.0   | 0.0 | 3200.0  | 310.0   | 0.0 |\n",
      "| constant    | 05:06:07 | 05:07:07 | 60.0 | 0.0  | 0.0  | 11264.0 | 2048.0   | 0.0 | 5632.0  | 469.0   | 0.0 |\n",
      "| constant    | 05:07:10 | 05:08:10 | 60.0 | 0.0  | 0.0  | 16128.0 | 2816.0   | 0.0 | 8064.0  | 733.0   | 0.0 |\n",
      "| constant    | 05:08:12 | 05:09:12 | 60.0 | 0.0  | 0.0  | 20736.0 | 4096.0   | 0.0 | 10368.0 | 988.0   | 0.0 |\n",
      "| constant    | 05:09:15 | 05:10:15 | 60.0 | 0.0  | 0.0  | 24576.0 | 5888.0   | 0.0 | 12288.0 | 1513.0  | 0.0 |\n",
      "| constant    | 05:10:17 | 05:11:17 | 60.0 | 0.0  | 0.0  | 28928.0 | 7424.0   | 0.0 | 14464.0 | 1790.0  | 0.0 |\n",
      "| constant    | 05:11:19 | 05:12:19 | 60.0 | 0.0  | 0.0  | 33280.0 | 8704.0   | 0.0 | 16640.0 | 2117.0  | 0.0 |\n",
      "| constant    | 05:12:21 | 05:13:21 | 60.0 | 0.0  | 0.0  | 29440.0 | 18432.0  | 0.0 | 14720.0 | 2353.0  | 0.0 |\n",
      "|=============|==========|==========|======|======|======|=========|==========|=====|=========|=========|=====|\n",
      "\n",
      "\n",
      "ℹ Text Metrics Statistics (Completed Requests)\n",
      "|=============|=======|=======|=======|========|=======|=======|=======|========|========|========|========|=========|\n",
      "| Benchmark   | Input Tokens                |||| Input Words                 |||| Input Characters                ||||\n",
      "| Strategy    | Per Request  || Per Second    || Per Request  || Per Second    || Per Request    || Per Second      ||\n",
      "|             | Mdn   | p95   | Mdn   | Mean   | Mdn   | p95   | Mdn   | Mean   | Mdn    | p95    | Mdn    | Mean    |\n",
      "|-------------|-------|-------|-------|--------|-------|-------|-------|--------|--------|--------|--------|---------|\n",
      "| synchronous | 256.0 | 256.0 | 28.3  | 33.0   | 213.0 | 215.0 | 23.7  | 27.5   | 1436.0 | 1493.0 | 160.8  | 185.5   |\n",
      "| throughput  | 256.0 | 256.0 | 644.5 | 2440.8 | 213.0 | 216.0 | 536.3 | 2032.4 | 1425.0 | 1471.0 | 3557.3 | 13561.1 |\n",
      "| constant    | 256.0 | 256.0 | 121.1 | 127.0  | 214.0 | 217.0 | 101.8 | 106.0  | 1424.0 | 1471.0 | 672.7  | 707.8   |\n",
      "| constant    | 256.0 | 256.0 | 216.6 | 223.0  | 213.0 | 216.0 | 180.8 | 185.4  | 1424.0 | 1468.0 | 1205.9 | 1237.4  |\n",
      "| constant    | 256.0 | 256.0 | 309.7 | 317.9  | 214.0 | 216.0 | 259.2 | 265.0  | 1432.0 | 1482.0 | 1745.9 | 1778.1  |\n",
      "| constant    | 256.0 | 256.0 | 404.1 | 412.7  | 214.0 | 217.0 | 338.8 | 344.5  | 1422.0 | 1473.0 | 2255.6 | 2297.4  |\n",
      "| constant    | 256.0 | 256.0 | 512.9 | 490.7  | 214.0 | 217.0 | 424.3 | 409.2  | 1424.0 | 1473.0 | 2814.1 | 2730.4  |\n",
      "| constant    | 256.0 | 256.0 | 600.2 | 582.4  | 213.0 | 216.0 | 498.7 | 485.1  | 1424.0 | 1482.0 | 3319.6 | 3242.3  |\n",
      "| constant    | 256.0 | 256.0 | 658.0 | 672.7  | 214.0 | 216.0 | 550.1 | 560.5  | 1418.0 | 1476.0 | 3675.1 | 3729.6  |\n",
      "| constant    | 256.0 | 256.0 | 719.5 | 603.4  | 213.0 | 217.0 | 599.0 | 503.0  | 1426.0 | 1481.0 | 3986.4 | 3360.9  |\n",
      "|=============|=======|=======|=======|========|=======|=======|=======|========|========|========|========|=========|\n",
      "| Benchmark   | Output Tokens               |||| Output Words                |||| Output Characters               ||||\n",
      "| Strategy    | Per Request  || Per Second    || Per Request  || Per Second    || Per Request    || Per Second      ||\n",
      "|             | Mdn   | p95   | Mdn   | Mean   | Mdn   | p95   | Mdn   | Mean   | Mdn    | p95    | Mdn    | Mean    |\n",
      "|-------------|-------|-------|-------|--------|-------|-------|-------|--------|--------|--------|--------|---------|\n",
      "| synchronous | 128.0 | 128.0 | 14.1  | 16.5   | 103.0 | 106.0 | 11.3  | 13.0   | 632.0  | 691.0  | 69.8   | 81.5    |\n",
      "| throughput  | 128.0 | 128.0 | 322.3 | 1220.4 | 98.0  | 105.0 | 242.5 | 922.7  | 598.0  | 676.0  | 1440.5 | 5618.0  |\n",
      "| constant    | 128.0 | 128.0 | 60.6  | 63.5   | 99.0  | 107.0 | 46.7  | 48.5   | 609.0  | 659.0  | 287.6  | 299.2   |\n",
      "| constant    | 128.0 | 128.0 | 108.3 | 111.5  | 97.0  | 108.0 | 82.8  | 84.3   | 606.0  | 655.0  | 511.2  | 518.5   |\n",
      "| constant    | 128.0 | 128.0 | 154.9 | 159.0  | 98.0  | 109.0 | 119.7 | 121.4  | 611.0  | 677.0  | 746.9  | 747.7   |\n",
      "| constant    | 128.0 | 128.0 | 202.0 | 206.3  | 99.0  | 105.0 | 156.1 | 156.1  | 608.0  | 663.0  | 960.9  | 950.0   |\n",
      "| constant    | 128.0 | 128.0 | 256.5 | 245.4  | 99.0  | 106.0 | 188.9 | 185.3  | 591.0  | 651.0  | 1123.4 | 1104.1  |\n",
      "| constant    | 128.0 | 128.0 | 300.1 | 291.2  | 98.0  | 108.0 | 227.3 | 221.3  | 608.0  | 676.0  | 1386.4 | 1357.9  |\n",
      "| constant    | 128.0 | 128.0 | 329.0 | 336.4  | 98.0  | 109.0 | 255.6 | 256.3  | 602.0  | 670.0  | 1564.8 | 1543.0  |\n",
      "| constant    | 128.0 | 128.0 | 359.8 | 301.7  | 99.0  | 105.0 | 273.4 | 229.7  | 600.0  | 683.0  | 1643.9 | 1399.8  |\n",
      "|=============|=======|=======|=======|========|=======|=======|=======|========|========|========|========|=========|\n",
      "\n",
      "\n",
      "ℹ Request Token Statistics (Completed Requests)\n",
      "|=============|=======|=======|=======|=======|=======|=======|=======|=======|=========|========|\n",
      "| Benchmark   | Input Tok    || Output Tok   || Total Tok    || Stream Iter  || Output Tok      ||\n",
      "| Strategy    | Per Req      || Per Req      || Per Req      || Per Req      || Per Stream Iter ||\n",
      "|             | Mdn   | p95   | Mdn   | p95   | Mdn   | p95   | Mdn   | p95   | Mdn     | p95    |\n",
      "|-------------|-------|-------|-------|-------|-------|-------|-------|-------|---------|--------|\n",
      "| synchronous | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| throughput  | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "|=============|=======|=======|=======|=======|=======|=======|=======|=======|=========|========|\n",
      "\n",
      "\n",
      "ℹ Request Latency Statistics (Completed Requests)\n",
      "|=============|=========|========|========|=========|=======|=======|=======|=======|\n",
      "| Benchmark   | Request Latency || TTFT            || ITL          || TPOT         ||\n",
      "| Strategy    | Sec             || ms              || ms           || ms           ||\n",
      "|             | Mdn     | p95    | Mdn    | p95     | Mdn   | p95   | Mdn   | p95   |\n",
      "|-------------|---------|--------|--------|---------|-------|-------|-------|-------|\n",
      "| synchronous | 9.1     | 9.1    | 144.3  | 351.9   | 69.5  | 70.9  | 71.1  | 71.2  |\n",
      "| throughput  | 49.8    | 59.3   | 7529.9 | 15376.4 | 332.9 | 345.7 | 389.1 | 463.1 |\n",
      "| constant    | 8.7     | 9.1    | 169.0  | 489.2   | 67.1  | 67.7  | 67.9  | 71.0  |\n",
      "| constant    | 9.0     | 9.0    | 162.3  | 352.8   | 69.4  | 69.5  | 70.1  | 70.4  |\n",
      "| constant    | 9.3     | 9.3    | 166.2  | 306.5   | 71.9  | 72.0  | 72.6  | 72.9  |\n",
      "| constant    | 9.6     | 9.7    | 154.6  | 187.8   | 74.6  | 74.7  | 75.2  | 75.5  |\n",
      "| constant    | 11.6    | 11.7   | 171.7  | 235.3   | 90.2  | 90.3  | 90.8  | 91.2  |\n",
      "| constant    | 12.2    | 12.2   | 172.8  | 219.2   | 94.6  | 94.7  | 95.1  | 95.5  |\n",
      "| constant    | 12.6    | 12.7   | 177.4  | 223.4   | 98.1  | 98.4  | 98.7  | 99.2  |\n",
      "| constant    | 13.2    | 17.2   | 198.2  | 351.4   | 102.2 | 134.5 | 103.0 | 134.5 |\n",
      "|=============|=========|========|========|=========|=======|=======|=======|=======|\n",
      "\n",
      "\n",
      "ℹ Server Throughput Statistics (All Requests)\n",
      "|=============|=====|======|=======|=======|=======|========|========|=======|=======|========|\n",
      "| Benchmark   | Requests                |||| Input Tokens  || Output Tokens || Total Tokens  ||\n",
      "| Strategy    | Per Sec   || Concurrency  || Per Sec       || Per Sec       || Per Sec       ||\n",
      "|             | Mdn | Mean | Mdn   | Mean  | Mdn   | Mean   | Mdn    | Mean  | Mdn   | Mean   |\n",
      "|-------------|-----|------|-------|-------|-------|--------|--------|-------|-------|--------|\n",
      "| synchronous | 0.1 | 0.1  | 1.0   | 1.0   | 28.3  | 33.1   | 14.4   | 14.2  | 14.4  | 42.6   |\n",
      "| throughput  | 0.0 | 3.1  | 512.0 | 511.2 | 642.8 | 2997.9 | 160.4  | 578.1 | 169.2 | 3575.9 |\n",
      "| constant    | 0.5 | 0.4  | 4.0   | 3.9   | 121.2 | 126.4  | 31.0   | 57.9  | 31.0  | 180.3  |\n",
      "| constant    | 0.8 | 0.7  | 8.0   | 7.1   | 216.7 | 223.2  | 34.9   | 102.3 | 34.9  | 325.4  |\n",
      "| constant    | 1.2 | 1.0  | 11.0  | 10.6  | 309.9 | 318.2  | 36.3   | 146.3 | 36.9  | 461.2  |\n",
      "| constant    | 1.6 | 1.3  | 15.0  | 14.2  | 404.6 | 415.5  | 30.9   | 190.0 | 30.9  | 605.5  |\n",
      "| constant    | 1.9 | 1.6  | 23.0  | 20.6  | 513.9 | 509.2  | 33.0   | 229.7 | 36.8  | 736.7  |\n",
      "| constant    | 2.3 | 1.9  | 28.0  | 25.5  | 602.1 | 607.5  | 25.2   | 271.6 | 25.4  | 879.2  |\n",
      "| constant    | 2.6 | 2.1  | 34.0  | 30.5  | 677.0 | 700.8  | 30.2   | 313.1 | 30.3  | 1013.9 |\n",
      "| constant    | 2.6 | 1.9  | 41.0  | 39.5  | 790.8 | 800.3  | 161.1  | 285.4 | 163.1 | 1085.8 |\n",
      "|=============|=====|======|=======|=======|=======|========|========|=======|=======|========|\n",
      "\n",
      "\n",
      "\n",
      "✔ Benchmarking complete, generated 10 benchmark(s)\n",
      "…   json    : /tmp/benchmark_results/single_node/benchmarks.json\n",
      "…   csv     : /tmp/benchmark_results/single_node/benchmarks.csv\n",
      "…   html    : /tmp/benchmark_results/single_node/benchmarks.html\n",
      "\n",
      "Benchmark [single_node] completed on controller.\n",
      "Copying results from controller to benchmark_results/single_node/...\n",
      "  Results saved to benchmark_results/single_node/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run guidellm sweep against single node\n",
    "# guidellm runs on the controller and targets spark-01's LAN IP\n",
    "run_guidellm(\n",
    "    target_url=f\"http://{NODE1_LAN_HOST}:{NODE1_PORT}\",\n",
    "    label=\"single_node\",\n",
    "    profile=\"sweep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ccbacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop single node before starting replicated setup\n",
    "stop_all_services()\n",
    "print(\"Phase 1 complete. Single-node benchmark results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5f56c",
   "metadata": {},
   "source": [
    "## Phase 2: Replicated Benchmark\n",
    "\n",
    "Start two independent vLLM instances (one per node) behind the round-robin proxy on the controller. This is the same configuration as Notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0cfe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting replicated setup...\n",
      "  Instance A (spark-01): PID 988599\n",
      "  Instance B (spark-02): PID 988601\n",
      "\n",
      "Waiting for both instances...\n",
      "  Instance A (spark-01): ready (0s)\n",
      "  Instance B (spark-02): ready (0s)\n",
      "  Proxy (controller): ready (0s)\n",
      "\n",
      "Replicated setup ready.\n"
     ]
    }
   ],
   "source": [
    "# Start Instance A on spark-01\n",
    "print(\"Starting replicated setup...\")\n",
    "rep_proc_a = start_vllm_local(NODE1_PORT, log_file=\"/tmp/vllm_rep_a.log\")\n",
    "print(f\"  Instance A (spark-01): PID {rep_proc_a.pid}\")\n",
    "\n",
    "# Start Instance B on spark-02\n",
    "rep_proc_b = start_vllm_remote(NODE2_HOST, NODE2_PORT, log_file=\"/tmp/vllm_rep_b.log\")\n",
    "print(f\"  Instance B (spark-02): PID {rep_proc_b.pid}\")\n",
    "\n",
    "# Wait for both\n",
    "print(\"\\nWaiting for both instances...\")\n",
    "ready_a = wait_for_server(NODE1_HOST, NODE1_PORT, \"Instance A (spark-01)\")\n",
    "ready_b = wait_for_server(NODE2_HOST, NODE2_PORT, \"Instance B (spark-02)\")\n",
    "\n",
    "if not (ready_a and ready_b):\n",
    "    print(\"Check logs:\")\n",
    "    print(f\"  Instance A: tail -50 /tmp/vllm_rep_a.log\")\n",
    "    print(f\"  Instance B: ssh nvidia@{NODE2_HOST} 'tail -50 /tmp/vllm_rep_b.log'\")\n",
    "    raise RuntimeError(\"One or both instances failed to start\")\n",
    "\n",
    "# Deploy and start round-robin proxy on controller\n",
    "# Always regenerate to ensure the /v1/models route is present\n",
    "# (guidellm calls /v1/models to discover the model before benchmarking)\n",
    "proxy_script = Path(\"/tmp/replicated_proxy.py\")\n",
    "script_content = f'''#!/usr/bin/env python3\n",
    "import itertools, logging\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse, StreamingResponse, Response\n",
    "import httpx\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [proxy] %(message)s\")\n",
    "logger = logging.getLogger(\"replicated_proxy\")\n",
    "\n",
    "BACKENDS = [\n",
    "    \"http://{NODE1_LAN_HOST}:{NODE1_PORT}\",\n",
    "    \"http://{NODE2_LAN_HOST}:{NODE2_PORT}\",\n",
    "]\n",
    "backend_cycle = itertools.cycle(BACKENDS)\n",
    "TIMEOUT = httpx.Timeout(timeout=120.0)\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {{\"status\": \"ok\"}}\n",
    "\n",
    "@app.get(\"/v1/models\")\n",
    "async def models():\n",
    "    backend = next(backend_cycle)\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        resp = await client.get(f\"{{backend}}/v1/models\", timeout=TIMEOUT)\n",
    "        return JSONResponse(content=resp.json())\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def handle_request(request: Request):\n",
    "    body = await request.json()\n",
    "    backend = next(backend_cycle)\n",
    "    path = request.url.path\n",
    "    is_streaming = body.get(\"stream\", False)\n",
    "\n",
    "    if is_streaming:\n",
    "        # Stream SSE chunks directly from the backend to the client.\n",
    "        # guidellm sends stream=true by default, so responses arrive as\n",
    "        # Server-Sent Events that cannot be parsed as a single JSON blob.\n",
    "        async def stream_response():\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                async with client.stream(\"POST\", f\"{{backend}}{{path}}\", json=body, timeout=TIMEOUT) as resp:\n",
    "                    async for chunk in resp.aiter_bytes():\n",
    "                        yield chunk\n",
    "        return StreamingResponse(stream_response(), media_type=\"text/event-stream\")\n",
    "    else:\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            resp = await client.post(f\"{{backend}}{{path}}\", json=body, timeout=TIMEOUT)\n",
    "            return Response(content=resp.content, status_code=resp.status_code,\n",
    "                            media_type=resp.headers.get(\"content-type\", \"application/json\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(f\"Replicated proxy on 0.0.0.0:{PROXY_PORT}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port={PROXY_PORT}, log_level=\"info\")\n",
    "'''\n",
    "proxy_script.write_text(script_content)\n",
    "\n",
    "# Kill any existing proxy, copy the updated script, and start fresh\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     'pkill -f replicated_proxy.py 2>/dev/null; sleep 1; pkill -9 -f replicated_proxy.py 2>/dev/null; true'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "time.sleep(1)\n",
    "subprocess.run(\n",
    "    ['scp', str(proxy_script), f'nvidia@{CONTROLLER_HOST}:/tmp/replicated_proxy.py'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "proxy_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"nohup python /tmp/replicated_proxy.py > /tmp/replicated_proxy.log 2>&1 < /dev/null &\"\n",
    ")\n",
    "subprocess.Popen(\n",
    "    ['ssh', f'nvidia@{CONTROLLER_HOST}', proxy_cmd],\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL\n",
    ")\n",
    "time.sleep(3)\n",
    "proxy_ready = wait_for_server(CONTROLLER_HOST, PROXY_PORT, \"Proxy (controller)\", timeout=30, interval=2)\n",
    "if not proxy_ready:\n",
    "    raise RuntimeError(\"Proxy failed to start. Check /tmp/replicated_proxy.log on controller\")\n",
    "\n",
    "print(\"\\nReplicated setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87758c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running guidellm [replicated] on controller (192.168.1.75)\n",
      "  Profile: sweep\n",
      "  Target:  http://192.168.1.75:8192\n",
      "  Remote output: /tmp/benchmark_results/replicated/\n",
      "  Duration: ~60s per rate point\n",
      "\n",
      "✔ OpenAIHTTPBackend backend validated with model \n",
      "/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/sn\n",
      "apshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "  {'target': 'http://192.168.1.75:8192', 'model': None, 'timeout': 60.0,        \n",
      "  'http2': True, 'follow_redirects': True, 'verify': False, 'openai_paths':     \n",
      "  {'health': 'health', 'models': 'v1/models', 'text_completions':               \n",
      "  'v1/completions', 'chat_completions': 'v1/chat/completions',                  \n",
      "  'audio_transcriptions': 'v1/audio/transcriptions', 'audio_translations':      \n",
      "  'v1/audio/translations'}, 'validate_backend': {'method': 'GET', 'url':        \n",
      "  'http://192.168.1.75:8192/health'}}                                           \n",
      "✔ Processor resolved\n",
      "  Using processor '/tmp/tokenizer-meta-llama--Llama-3.1-8B-Instruct'            \n",
      "✔ Request loader initialized with inf unique requests\n",
      "  {'data': \"['prompt_tokens=256,output_tokens=128']\", 'data_args': '[]',        \n",
      "  'data_samples': -1, 'preprocessors': ['GenerativeColumnMapper',               \n",
      "  'GenerativeTextCompletionsRequestFormatter'], 'collator':                     \n",
      "  'GenerativeRequestCollator', 'sampler': 'None', 'num_workers': 1,             \n",
      "  'random_seed': 42}                                                            \n",
      "✔ Resolved transient phase configurations\n",
      "  Warmup: percent=None value=None mode='prefer_duration'                        \n",
      "  Cooldown: percent=None value=None mode='prefer_duration'                      \n",
      "  Rampup (Throughput/Concurrent): 0.0                                           \n",
      "✔ SweepProfile profile resolved\n",
      "  {'str': \"type_='sweep' completed_strategies=[] constraints={'max_seconds':    \n",
      "  60.0} rampup_duration=0.0 sweep_size=10 strategy_type='constant'              \n",
      "  max_concurrency=None random_seed=42 synchronous_rate=-1.0 throughput_rate=-1.0\n",
      "  async_rates=[] measured_rates=[] strategy_types=['synchronous', 'throughput', \n",
      "  'constant', 'constant', 'constant', 'constant', 'constant', 'constant',       \n",
      "  'constant', 'constant']\", 'type': 'SweepProfile', 'class': 'SweepProfile',    \n",
      "  'module': 'guidellm.benchmark.profiles', 'attributes': {'type_': 'sweep',     \n",
      "  'completed_strategies': [], 'constraints': {'max_seconds': 60.0},             \n",
      "  'rampup_duration': 0.0, 'sweep_size': 10, 'strategy_type': 'constant',        \n",
      "  'max_concurrency': 'None', 'random_seed': 42, 'synchronous_rate': -1.0,       \n",
      "  'throughput_rate': -1.0, 'async_rates': [], 'measured_rates': []}}            \n",
      "✔ Output formats resolved\n",
      "  {'json': \"output_path=PosixPath('/tmp/benchmark_results/replicated')\", 'csv': \n",
      "  \"output_path=PosixPath('/tmp/benchmark_results/replicated')\", 'html':         \n",
      "  \"output_path=PosixPath('/tmp/benchmark_results/replicated')\"}                 \n",
      "✔ Setup complete, starting benchmarks...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "╭─ Benchmarks ─────────────────────────────────────────────────────────────────╮\n",
      "│ [0… synch… (… Req:    0.1 req/s,    9.41s Lat,     1.0 Conc,       7 Comp, … │\n",
      "│               Tok:   13.8 gen/s,   41.3 tot/s, 648.6ms TTFT,   69.0ms ITL, … │\n",
      "│ [0… throu… (… Req:    0.0 req/s,    0.00s Lat,     0.0 Conc,       0 Comp, … │\n",
      "│               Tok:    0.0 gen/s,    0.0 tot/s,   0.0ms TTFT,    0.0ms ITL, … │\n",
      "│ [0… const… (… Req:    0.1 req/s,   10.81s Lat,     1.0 Conc,       6 Comp, … │\n",
      "│               Tok:   13.3 gen/s,   39.8 tot/s, 1947.0ms TTFT,   69.8ms ITL,… │\n",
      "│ [0… const… (… Req:    0.1 req/s,    9.53s Lat,     0.7 Conc,       5 Comp, … │\n",
      "│               Tok:   10.3 gen/s,   30.8 tot/s, 588.5ms TTFT,   70.4ms ITL, … │\n",
      "│ [0… const… (… Req:    0.1 req/s,    9.58s Lat,     0.6 Conc,       4 Comp, … │\n",
      "│               Tok:    9.0 gen/s,   26.9 tot/s, 780.7ms TTFT,   69.3ms ITL, … │\n",
      "│ [0… const… (… Req:    0.1 req/s,    9.53s Lat,     0.5 Conc,       3 Comp, … │\n",
      "│               Tok:    7.9 gen/s,   23.6 tot/s, 686.2ms TTFT,   69.6ms ITL, … │\n",
      "│ [0… const… (… Req:    0.0 req/s,    9.54s Lat,     0.4 Conc,       3 Comp, … │\n",
      "│               Tok:    6.2 gen/s,   18.5 tot/s, 726.6ms TTFT,   69.4ms ITL, … │\n",
      "│ [0… const… (… Req:    0.0 req/s,    9.62s Lat,     0.3 Conc,       2 Comp, … │\n",
      "│               Tok:    5.2 gen/s,   15.7 tot/s, 858.5ms TTFT,   68.9ms ITL, … │\n",
      "│ [0… const… (… Req:    0.0 req/s,    9.53s Lat,     0.2 Conc,       1 Comp, … │\n",
      "│               Tok:   14.4 gen/s,   43.1 tot/s, 612.6ms TTFT,   70.2ms ITL, … │\n",
      "│ [-… const… (…                                                                │\n",
      "│                                                                              │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "Generating... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━     (9/10) [ 0:13:49 < -:--:-- ]\n",
      "\n",
      "ℹ Run Summary Info\n",
      "|=============|==========|==========|======|======|======|========|==========|===========|=======|=========|=====|\n",
      "| Benchmark   | Timings                              ||||| Input Tokens                ||| Output Tokens       |||\n",
      "| Strategy    | Start    | End      | Dur  | Warm | Cool | Comp   | Inc      | Err       | Comp  | Inc     | Err |\n",
      "|             |          |          | Sec  | Sec  | Sec  | Tot    | Tot      | Tot       | Tot   | Tot     | Tot |\n",
      "|-------------|----------|----------|------|------|------|--------|----------|-----------|-------|---------|-----|\n",
      "| synchronous | 06:25:20 | 06:26:20 | 60.0 | 0.0  | 0.0  | 1792.0 | 0.0      | 0.0       | 896.0 | 0.0     | 0.0 |\n",
      "| throughput  | 06:26:28 | 06:27:28 | 60.3 | 0.0  | 0.0  | 0.0    | 129280.0 | 8705024.0 | 0.0   | 22783.0 | 0.0 |\n",
      "| constant    | 06:27:45 | 06:28:45 | 60.0 | 0.0  | 0.0  | 1536.0 | 0.0      | 0.0       | 768.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:29:06 | 06:30:06 | 60.0 | 0.0  | 0.0  | 1280.0 | 0.0      | 0.0       | 640.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:30:27 | 06:31:27 | 60.0 | 0.0  | 0.0  | 1024.0 | 0.0      | 0.0       | 512.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:31:53 | 06:32:53 | 60.0 | 0.0  | 0.0  | 768.0  | 0.0      | 0.0       | 384.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:33:21 | 06:34:21 | 60.0 | 0.0  | 0.0  | 768.0  | 0.0      | 0.0       | 384.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:35:06 | 06:36:06 | 60.0 | 0.0  | 0.0  | 512.0  | 0.0      | 0.0       | 256.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:37:48 | 06:38:48 | 60.0 | 0.0  | 0.0  | 256.0  | 0.0      | 0.0       | 128.0 | 0.0     | 0.0 |\n",
      "|=============|==========|==========|======|======|======|========|==========|===========|=======|=========|=====|\n",
      "\n",
      "\n",
      "ℹ Text Metrics Statistics (Completed Requests)\n",
      "|=============|=======|=======|======|======|=======|=======|======|======|========|========|=======|=======|\n",
      "| Benchmark   | Input Tokens             |||| Input Words              |||| Input Characters             ||||\n",
      "| Strategy    | Per Request  || Per Second || Per Request  || Per Second || Per Request    || Per Second   ||\n",
      "|             | Mdn   | p95   | Mdn  | Mean | Mdn   | p95   | Mdn  | Mean | Mdn    | p95    | Mdn   | Mean  |\n",
      "|-------------|-------|-------|------|------|-------|-------|------|------|--------|--------|-------|-------|\n",
      "| synchronous | 256.0 | 256.0 | 27.3 | 31.7 | 213.0 | 215.0 | 22.7 | 26.4 | 1436.0 | 1493.0 | 154.9 | 178.2 |\n",
      "| throughput  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0    | 0.0    | 0.0   | 0.0   |\n",
      "| constant    | 256.0 | 256.0 | 22.2 | 31.4 | 214.0 | 217.0 | 18.8 | 26.3 | 1439.0 | 1481.0 | 128.5 | 175.9 |\n",
      "| constant    | 256.0 | 256.0 | 19.0 | 23.9 | 212.0 | 214.0 | 15.7 | 19.8 | 1435.0 | 1460.0 | 106.2 | 134.0 |\n",
      "| constant    | 256.0 | 256.0 | 16.0 | 21.1 | 211.0 | 216.0 | 13.4 | 17.5 | 1395.0 | 1461.0 | 86.5  | 117.2 |\n",
      "| constant    | 256.0 | 256.0 | 25.0 | 19.2 | 213.0 | 215.0 | 21.0 | 16.0 | 1438.0 | 1460.0 | 140.4 | 107.8 |\n",
      "| constant    | 256.0 | 256.0 | 9.6  | 14.4 | 214.0 | 214.0 | 8.0  | 12.0 | 1417.0 | 1493.0 | 53.0  | 80.8  |\n",
      "| constant    | 256.0 | 256.0 | 12.7 | 12.7 | 211.0 | 214.0 | 10.5 | 10.6 | 1394.0 | 1464.0 | 69.3  | 71.0  |\n",
      "| constant    | 256.0 | 256.0 | 0.0  | 0.0  | 215.0 | 215.0 | 0.0  | 0.0  | 1393.0 | 1393.0 | 0.0   | 0.0   |\n",
      "|=============|=======|=======|======|======|=======|=======|======|======|========|========|=======|=======|\n",
      "| Benchmark   | Output Tokens            |||| Output Words             |||| Output Characters            ||||\n",
      "| Strategy    | Per Request  || Per Second || Per Request  || Per Second || Per Request    || Per Second   ||\n",
      "|             | Mdn   | p95   | Mdn  | Mean | Mdn   | p95   | Mdn  | Mean | Mdn    | p95    | Mdn   | Mean  |\n",
      "|-------------|-------|-------|------|------|-------|-------|------|------|--------|--------|-------|-------|\n",
      "| synchronous | 128.0 | 128.0 | 13.7 | 15.9 | 99.0  | 107.0 | 10.6 | 12.2 | 608.0  | 718.0  | 63.8  | 75.1  |\n",
      "| throughput  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0    | 0.0    | 0.0   | 0.0   |\n",
      "| constant    | 128.0 | 128.0 | 11.1 | 15.7 | 101.0 | 103.0 | 8.8  | 12.2 | 547.0  | 660.0  | 57.3  | 72.1  |\n",
      "| constant    | 128.0 | 128.0 | 9.5  | 12.0 | 106.0 | 112.0 | 8.1  | 9.9  | 648.0  | 693.0  | 48.2  | 60.0  |\n",
      "| constant    | 128.0 | 128.0 | 8.0  | 10.5 | 94.0  | 107.0 | 6.6  | 8.0  | 559.0  | 605.0  | 35.8  | 47.0  |\n",
      "| constant    | 128.0 | 128.0 | 12.5 | 9.6  | 101.0 | 109.0 | 10.1 | 7.7  | 592.0  | 628.0  | 60.9  | 44.1  |\n",
      "| constant    | 128.0 | 128.0 | 4.8  | 7.2  | 96.0  | 101.0 | 3.2  | 5.3  | 635.0  | 656.0  | 20.6  | 34.5  |\n",
      "| constant    | 128.0 | 128.0 | 6.4  | 6.4  | 91.0  | 100.0 | 4.5  | 4.7  | 548.0  | 601.0  | 27.2  | 28.6  |\n",
      "| constant    | 128.0 | 128.0 | 0.0  | 0.0  | 100.0 | 100.0 | 0.0  | 0.0  | 600.0  | 600.0  | 0.0   | 0.0   |\n",
      "|=============|=======|=======|======|======|=======|=======|======|======|========|========|=======|=======|\n",
      "\n",
      "\n",
      "ℹ Request Token Statistics (Completed Requests)\n",
      "|=============|=======|=======|=======|=======|=======|=======|=======|=======|=========|========|\n",
      "| Benchmark   | Input Tok    || Output Tok   || Total Tok    || Stream Iter  || Output Tok      ||\n",
      "| Strategy    | Per Req      || Per Req      || Per Req      || Per Req      || Per Stream Iter ||\n",
      "|             | Mdn   | p95   | Mdn   | p95   | Mdn   | p95   | Mdn   | p95   | Mdn     | p95    |\n",
      "|-------------|-------|-------|-------|-------|-------|-------|-------|-------|---------|--------|\n",
      "| synchronous | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| throughput  | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0     | 0.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "|=============|=======|=======|=======|=======|=======|=======|=======|=======|=========|========|\n",
      "\n",
      "\n",
      "ℹ Request Latency Statistics (Completed Requests)\n",
      "|=============|========|=========|=======|========|======|======|======|=======|\n",
      "| Benchmark   | Request Latency || TTFT          || ITL        || TPOT        ||\n",
      "| Strategy    | Sec             || ms            || ms         || ms          ||\n",
      "|             | Mdn    | p95     | Mdn   | p95    | Mdn  | p95  | Mdn  | p95   |\n",
      "|-------------|--------|---------|-------|--------|------|------|------|-------|\n",
      "| synchronous | 9.4    | 9.9     | 607.7 | 920.5  | 67.8 | 71.0 | 73.2 | 77.6  |\n",
      "| throughput  | 0.0    | 0.0     | 0.0   | 0.0    | 0.0  | 0.0  | 0.0  | 0.0   |\n",
      "| constant    | 9.4    | 17.8    | 527.0 | 8697.5 | 70.1 | 71.7 | 73.3 | 139.1 |\n",
      "| constant    | 9.5    | 9.8     | 542.6 | 882.2  | 70.2 | 72.5 | 74.5 | 76.5  |\n",
      "| constant    | 9.5    | 9.8     | 761.0 | 860.0  | 68.5 | 70.6 | 73.9 | 76.8  |\n",
      "| constant    | 9.7    | 9.7     | 769.3 | 827.1  | 70.2 | 70.2 | 75.6 | 76.1  |\n",
      "| constant    | 9.7    | 9.8     | 779.0 | 888.3  | 70.2 | 70.2 | 75.8 | 76.5  |\n",
      "| constant    | 9.5    | 9.7     | 790.2 | 926.8  | 67.8 | 70.1 | 74.5 | 75.8  |\n",
      "| constant    | 9.5    | 9.5     | 612.6 | 612.6  | 70.2 | 70.2 | 74.4 | 74.4  |\n",
      "|=============|========|=========|=======|========|======|======|======|=======|\n",
      "\n",
      "\n",
      "ℹ Server Throughput Statistics (All Requests)\n",
      "|=============|=======|=======|=======|=======|==========|==========|=======|========|==========|==========|\n",
      "| Benchmark   | Requests                   |||| Input Tokens       || Output Tokens || Total Tokens       ||\n",
      "| Strategy    | Per Sec      || Concurrency  || Per Sec            || Per Sec       || Per Sec            ||\n",
      "|             | Mdn   | Mean  | Mdn   | Mean  | Mdn      | Mean     | Mdn   | Mean   | Mdn      | Mean     |\n",
      "|-------------|-------|-------|-------|-------|----------|----------|-------|--------|----------|----------|\n",
      "| synchronous | 0.1   | 0.1   | 1.0   | 1.0   | 27.3     | 31.7     | 14.8  | 13.8   | 14.8     | 41.3     |\n",
      "| throughput  | 542.5 | 563.5 | 507.0 | 507.4 | 140248.4 | 147702.3 | 0.0   | 380.9  | 128678.1 | 148083.2 |\n",
      "| constant    | 0.1   | 0.1   | 1.0   | 1.0   | 22.5     | 31.3     | 14.2  | 13.3   | 14.3     | 39.8     |\n",
      "| constant    | 0.1   | 0.1   | 1.0   | 0.7   | 18.9     | 24.1     | 14.2  | 10.3   | 14.3     | 30.8     |\n",
      "| constant    | 0.1   | 0.1   | 1.0   | 0.6   | 16.0     | 21.2     | 14.2  | 9.0    | 14.3     | 26.9     |\n",
      "| constant    | 0.1   | 0.1   | 0.0   | 0.5   | 25.3     | 19.2     | 14.3  | 7.9    | 14.3     | 23.6     |\n",
      "| constant    | 0.0   | 0.0   | 0.0   | 0.4   | 18.9     | 14.4     | 0.1   | 6.2    | 14.3     | 18.5     |\n",
      "| constant    | 0.0   | 0.0   | 0.0   | 0.3   | 12.8     | 12.8     | 0.1   | 5.2    | 14.3     | 15.7     |\n",
      "| constant    | 0.0   | 0.0   | 0.0   | 0.2   | 0.0      | 0.0      | 14.3  | 14.4   | 14.3     | 43.1     |\n",
      "|=============|=======|=======|=======|=======|==========|==========|=======|========|==========|==========|\n",
      "\n",
      "\n",
      "\n",
      "✔ Benchmarking complete, generated 9 benchmark(s)\n",
      "…   json    : /tmp/benchmark_results/replicated/benchmarks.json\n",
      "…   csv     : /tmp/benchmark_results/replicated/benchmarks.csv\n",
      "…   html    : /tmp/benchmark_results/replicated/benchmarks.html\n",
      "\n",
      "Benchmark [replicated] completed on controller.\n",
      "Copying results from controller to benchmark_results/replicated/...\n",
      "  Results saved to benchmark_results/replicated/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run guidellm sweep against replicated proxy\n",
    "run_guidellm(\n",
    "    target_url=f\"http://{CONTROLLER_HOST}:{PROXY_PORT}\",\n",
    "    label=\"replicated\",\n",
    "    profile=\"sweep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8cc91c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all services...\n",
      "  All services stopped\n",
      "Phase 2 complete. Replicated benchmark results saved.\n"
     ]
    }
   ],
   "source": [
    "# Stop replicated setup before starting disaggregated\n",
    "stop_all_services()\n",
    "print(\"Phase 2 complete. Replicated benchmark results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bcf3e5",
   "metadata": {},
   "source": [
    "## Phase 3: Disaggregated Benchmark\n",
    "\n",
    "Start prefill on spark-01 and decode on spark-02 with NixlConnector, behind the disaggregated proxy on the controller. This is the same configuration as Notebook 04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d8ffe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting disaggregated setup...\n",
      "  Prefill (spark-01): PID 990114\n",
      "  Decode (spark-02):  PID 990116\n",
      "\n",
      "Waiting for instances to load model and initialize NIXL...\n",
      "  Prefill (spark-01): waiting... (0s / 300s)\n",
      "  Prefill (spark-01): waiting... (10s / 300s)\n",
      "  Prefill (spark-01): waiting... (20s / 300s)\n",
      "  Prefill (spark-01): waiting... (30s / 300s)\n",
      "  Prefill (spark-01): waiting... (40s / 300s)\n",
      "  Prefill (spark-01): waiting... (50s / 300s)\n",
      "  Prefill (spark-01): waiting... (60s / 300s)\n",
      "  Prefill (spark-01): waiting... (70s / 300s)\n",
      "  Prefill (spark-01): waiting... (80s / 300s)\n",
      "  Prefill (spark-01): waiting... (90s / 300s)\n",
      "  Prefill (spark-01): waiting... (100s / 300s)\n",
      "  Prefill (spark-01): waiting... (110s / 300s)\n",
      "  Prefill (spark-01): waiting... (120s / 300s)\n",
      "  Prefill (spark-01): ready (130s)\n",
      "  Decode (spark-02): ready (0s)\n",
      "  Proxy (controller): ready (1s)\n",
      "\n",
      "Disaggregated setup ready.\n"
     ]
    }
   ],
   "source": [
    "# KV transfer configuration for NixlConnector\n",
    "kv_config_prefill = json.dumps({\n",
    "    \"kv_connector\": \"NixlConnector\",\n",
    "    \"kv_role\": \"kv_both\",\n",
    "    \"kv_ip\": NODE1_HOST,\n",
    "    \"kv_port\": NIXL_PORT\n",
    "})\n",
    "kv_config_decode = json.dumps({\n",
    "    \"kv_connector\": \"NixlConnector\",\n",
    "    \"kv_role\": \"kv_both\",\n",
    "    \"kv_ip\": NODE2_HOST,\n",
    "    \"kv_port\": NIXL_PORT\n",
    "})\n",
    "\n",
    "# Start prefill on spark-01\n",
    "print(\"Starting disaggregated setup...\")\n",
    "prefill_proc = start_vllm_local(\n",
    "    NODE1_PORT,\n",
    "    kv_transfer_config=kv_config_prefill,\n",
    "    log_file=\"/tmp/vllm_prefill.log\"\n",
    ")\n",
    "print(f\"  Prefill (spark-01): PID {prefill_proc.pid}\")\n",
    "\n",
    "# Start decode on spark-02\n",
    "decode_proc = start_vllm_remote(\n",
    "    NODE2_HOST, NODE2_PORT,\n",
    "    kv_transfer_config=kv_config_decode,\n",
    "    log_file=\"/tmp/vllm_decode.log\"\n",
    ")\n",
    "print(f\"  Decode (spark-02):  PID {decode_proc.pid}\")\n",
    "\n",
    "# Wait for both instances\n",
    "print(\"\\nWaiting for instances to load model and initialize NIXL...\")\n",
    "prefill_ready = wait_for_server(NODE1_HOST, NODE1_PORT, \"Prefill (spark-01)\")\n",
    "decode_ready = wait_for_server(NODE2_HOST, NODE2_PORT, \"Decode (spark-02)\")\n",
    "\n",
    "if not (prefill_ready and decode_ready):\n",
    "    print(\"Check logs:\")\n",
    "    print(f\"  Prefill: tail -50 /tmp/vllm_prefill.log\")\n",
    "    print(f\"  Decode:  ssh nvidia@{NODE2_HOST} 'tail -50 /tmp/vllm_decode.log'\")\n",
    "    raise RuntimeError(\"One or both instances failed to start\")\n",
    "\n",
    "# Deploy and start disaggregated proxy on controller.\n",
    "# Always regenerate to ensure /v1/models route and streaming support are present\n",
    "# (guidellm calls /v1/models for model discovery and sends stream=true requests).\n",
    "disagg_proxy = Path(\"/tmp/disagg_proxy.py\")\n",
    "disagg_content = f'''#!/usr/bin/env python3\n",
    "import uuid, logging, itertools\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse, StreamingResponse, Response\n",
    "import httpx\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [proxy] %(message)s\")\n",
    "logger = logging.getLogger(\"disagg_proxy\")\n",
    "\n",
    "PREFILL_URL = \"http://{NODE1_LAN_HOST}:{NODE1_PORT}\"\n",
    "DECODE_URL = \"http://{NODE2_LAN_HOST}:{NODE2_PORT}\"\n",
    "BACKENDS = [PREFILL_URL, DECODE_URL]\n",
    "backend_cycle = itertools.cycle(BACKENDS)\n",
    "TIMEOUT = httpx.Timeout(timeout=120.0)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {{\"status\": \"ok\"}}\n",
    "\n",
    "@app.get(\"/v1/models\")\n",
    "async def models():\n",
    "    backend = next(backend_cycle)\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        resp = await client.get(f\"{{backend}}/v1/models\", timeout=TIMEOUT)\n",
    "        return JSONResponse(content=resp.json())\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def handle_request(request: Request):\n",
    "    body = await request.json()\n",
    "    path = request.url.path\n",
    "    request_id = str(uuid.uuid4())\n",
    "    is_streaming = body.get(\"stream\", False)\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Step 1: Prefill (prompt processing + KV cache generation)\n",
    "        prefill_body = dict(body)\n",
    "        prefill_body[\"max_tokens\"] = 1\n",
    "        prefill_body[\"stream\"] = False\n",
    "        prefill_body[\"kv_transfer_params\"] = {{\n",
    "            \"do_remote_decode\": True,\n",
    "            \"do_remote_prefill\": False,\n",
    "            \"remote_engine_id\": None,\n",
    "            \"remote_block_ids\": None,\n",
    "            \"remote_host\": None,\n",
    "            \"remote_port\": None,\n",
    "        }}\n",
    "        headers = {{\"X-Request-Id\": request_id}}\n",
    "        resp = await client.post(f\"{{PREFILL_URL}}{{path}}\", json=prefill_body,\n",
    "                                 headers=headers, timeout=TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "        prefill_resp = resp.json()\n",
    "\n",
    "        # Step 2: Extract kv_transfer_params\n",
    "        kv_params = prefill_resp.get(\"kv_transfer_params\")\n",
    "        if not kv_params:\n",
    "            logger.error(f\"No kv_transfer_params in prefill response for {{request_id}}\")\n",
    "            return JSONResponse(status_code=502, content={{\n",
    "                \"error\": \"Prefill did not return kv_transfer_params. \"\n",
    "                         \"Verify NixlConnector is configured on both instances.\"}})\n",
    "\n",
    "        # Step 3: Decode (KV cache pull via NIXL + token generation)\n",
    "        decode_body = dict(body)\n",
    "        decode_body[\"kv_transfer_params\"] = kv_params\n",
    "\n",
    "        if is_streaming:\n",
    "            async def stream_response():\n",
    "                async with httpx.AsyncClient() as dc:\n",
    "                    async with dc.stream(\"POST\", f\"{{DECODE_URL}}{{path}}\",\n",
    "                                         json=decode_body, headers=headers,\n",
    "                                         timeout=TIMEOUT) as dresp:\n",
    "                        async for chunk in dresp.aiter_bytes():\n",
    "                            yield chunk\n",
    "            return StreamingResponse(stream_response(), media_type=\"text/event-stream\")\n",
    "        else:\n",
    "            dresp = await client.post(f\"{{DECODE_URL}}{{path}}\", json=decode_body,\n",
    "                                      headers=headers, timeout=TIMEOUT)\n",
    "            dresp.raise_for_status()\n",
    "            return Response(content=dresp.content, status_code=dresp.status_code,\n",
    "                            media_type=dresp.headers.get(\"content-type\", \"application/json\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(f\"Proxy listening on 0.0.0.0:{PROXY_PORT}\")\n",
    "    logger.info(f\"  Prefill: {{PREFILL_URL}}\")\n",
    "    logger.info(f\"  Decode:  {{DECODE_URL}}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port={PROXY_PORT}, log_level=\"info\")\n",
    "'''\n",
    "disagg_proxy.write_text(disagg_content)\n",
    "\n",
    "# Kill any existing proxy, copy, and start fresh\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     'pkill -f disagg_proxy.py 2>/dev/null; sleep 1; pkill -9 -f disagg_proxy.py 2>/dev/null; true'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "time.sleep(1)\n",
    "subprocess.run(\n",
    "    ['scp', str(disagg_proxy), f'nvidia@{CONTROLLER_HOST}:/tmp/disagg_proxy.py'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "proxy_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"nohup python /tmp/disagg_proxy.py > /tmp/disagg_proxy.log 2>&1 < /dev/null &\"\n",
    ")\n",
    "subprocess.Popen(\n",
    "    ['ssh', f'nvidia@{CONTROLLER_HOST}', proxy_cmd],\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL\n",
    ")\n",
    "time.sleep(3)\n",
    "proxy_ready = wait_for_server(CONTROLLER_HOST, PROXY_PORT, \"Proxy (controller)\", timeout=30, interval=2)\n",
    "if not proxy_ready:\n",
    "    raise RuntimeError(\"Proxy failed to start. Check /tmp/disagg_proxy.log on controller\")\n",
    "\n",
    "print(\"\\nDisaggregated setup ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e47992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running guidellm [disaggregated] on controller (192.168.1.75)\n",
      "  Profile: sweep\n",
      "  Target:  http://192.168.1.75:8192\n",
      "  Remote output: /tmp/benchmark_results/disaggregated/\n",
      "  Duration: ~60s per rate point\n",
      "\n",
      "✔ OpenAIHTTPBackend backend validated with model \n",
      "/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/sn\n",
      "apshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "  {'target': 'http://192.168.1.75:8192', 'model': None, 'timeout': 60.0,        \n",
      "  'http2': True, 'follow_redirects': True, 'verify': False, 'openai_paths':     \n",
      "  {'health': 'health', 'models': 'v1/models', 'text_completions':               \n",
      "  'v1/completions', 'chat_completions': 'v1/chat/completions',                  \n",
      "  'audio_transcriptions': 'v1/audio/transcriptions', 'audio_translations':      \n",
      "  'v1/audio/translations'}, 'validate_backend': {'method': 'GET', 'url':        \n",
      "  'http://192.168.1.75:8192/health'}}                                           \n",
      "✔ Processor resolved\n",
      "  Using processor '/tmp/tokenizer-meta-llama--Llama-3.1-8B-Instruct'            \n",
      "✔ Request loader initialized with inf unique requests\n",
      "  {'data': \"['prompt_tokens=256,output_tokens=128']\", 'data_args': '[]',        \n",
      "  'data_samples': -1, 'preprocessors': ['GenerativeColumnMapper',               \n",
      "  'GenerativeTextCompletionsRequestFormatter'], 'collator':                     \n",
      "  'GenerativeRequestCollator', 'sampler': 'None', 'num_workers': 1,             \n",
      "  'random_seed': 42}                                                            \n",
      "✔ Resolved transient phase configurations\n",
      "  Warmup: percent=None value=None mode='prefer_duration'                        \n",
      "  Cooldown: percent=None value=None mode='prefer_duration'                      \n",
      "  Rampup (Throughput/Concurrent): 0.0                                           \n",
      "✔ SweepProfile profile resolved\n",
      "  {'str': \"type_='sweep' completed_strategies=[] constraints={'max_seconds':    \n",
      "  60.0} rampup_duration=0.0 sweep_size=10 strategy_type='constant'              \n",
      "  max_concurrency=None random_seed=42 synchronous_rate=-1.0 throughput_rate=-1.0\n",
      "  async_rates=[] measured_rates=[] strategy_types=['synchronous', 'throughput', \n",
      "  'constant', 'constant', 'constant', 'constant', 'constant', 'constant',       \n",
      "  'constant', 'constant']\", 'type': 'SweepProfile', 'class': 'SweepProfile',    \n",
      "  'module': 'guidellm.benchmark.profiles', 'attributes': {'type_': 'sweep',     \n",
      "  'completed_strategies': [], 'constraints': {'max_seconds': 60.0},             \n",
      "  'rampup_duration': 0.0, 'sweep_size': 10, 'strategy_type': 'constant',        \n",
      "  'max_concurrency': 'None', 'random_seed': 42, 'synchronous_rate': -1.0,       \n",
      "  'throughput_rate': -1.0, 'async_rates': [], 'measured_rates': []}}            \n",
      "✔ Output formats resolved\n",
      "  {'json': \"output_path=PosixPath('/tmp/benchmark_results/disaggregated')\",     \n",
      "  'csv': \"output_path=PosixPath('/tmp/benchmark_results/disaggregated')\",       \n",
      "  'html': \"output_path=PosixPath('/tmp/benchmark_results/disaggregated')\"}      \n",
      "✔ Setup complete, starting benchmarks...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "╭─ Benchmarks ─────────────────────────────────────────────────────────────────╮\n",
      "│ [0… synch… (… Req:    0.1 req/s,    9.47s Lat,     1.0 Conc,       7 Comp, … │\n",
      "│               Tok:   13.6 gen/s,   40.9 tot/s, 568.2ms TTFT,   70.1ms ITL, … │\n",
      "│ [0… throu… (… Req:    0.0 req/s,    0.00s Lat,     0.0 Conc,       0 Comp, … │\n",
      "│               Tok:    0.0 gen/s,    0.0 tot/s,   0.0ms TTFT,    0.0ms ITL, … │\n",
      "│ [0… const… (… Req:    0.1 req/s,   10.57s Lat,     1.0 Conc,       6 Comp, … │\n",
      "│               Tok:   13.0 gen/s,   39.1 tot/s, 1745.1ms TTFT,   69.5ms ITL,… │\n",
      "│ [0… const… (… Req:    0.1 req/s,    9.62s Lat,     0.8 Conc,       5 Comp, … │\n",
      "│               Tok:   10.3 gen/s,   30.9 tot/s, 651.7ms TTFT,   70.6ms ITL, … │\n",
      "│ [0… const… (… Req:    0.1 req/s,    9.65s Lat,     0.6 Conc,       4 Comp, … │\n",
      "│               Tok:    8.9 gen/s,   26.6 tot/s, 717.6ms TTFT,   70.4ms ITL, … │\n",
      "│ [0… const… (… Req:    0.1 req/s,    9.65s Lat,     0.5 Conc,       4 Comp, … │\n",
      "│               Tok:    7.5 gen/s,   22.4 tot/s, 804.0ms TTFT,   69.6ms ITL, … │\n",
      "│ [0… const… (… Req:    0.0 req/s,    9.66s Lat,     0.4 Conc,       3 Comp, … │\n",
      "│               Tok:    6.2 gen/s,   18.5 tot/s, 686.2ms TTFT,   70.7ms ITL, … │\n",
      "│ [0… const… (… Req:    0.0 req/s,    9.54s Lat,     0.3 Conc,       2 Comp, … │\n",
      "│               Tok:    5.2 gen/s,   15.7 tot/s, 605.1ms TTFT,   70.4ms ITL, … │\n",
      "│ [0… const… (… Req:    0.0 req/s,    9.64s Lat,     0.2 Conc,       1 Comp, … │\n",
      "│               Tok:   14.0 gen/s,   42.1 tot/s, 520.3ms TTFT,   71.8ms ITL, … │\n",
      "│ [-… const… (…                                                                │\n",
      "│                                                                              │\n",
      "╰──────────────────────────────────────────────────────────────────────────────╯\n",
      "Generating... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━     (9/10) [ 0:13:59 < -:--:-- ]\n",
      "\n",
      "ℹ Run Summary Info\n",
      "|=============|==========|==========|======|======|======|========|==========|===========|=======|=========|=====|\n",
      "| Benchmark   | Timings                              ||||| Input Tokens                ||| Output Tokens       |||\n",
      "| Strategy    | Start    | End      | Dur  | Warm | Cool | Comp   | Inc      | Err       | Comp  | Inc     | Err |\n",
      "|             |          |          | Sec  | Sec  | Sec  | Tot    | Tot      | Tot       | Tot   | Tot     | Tot |\n",
      "|-------------|----------|----------|------|------|------|--------|----------|-----------|-------|---------|-----|\n",
      "| synchronous | 06:49:50 | 06:50:50 | 60.0 | 0.0  | 0.0  | 1792.0 | 0.0      | 0.0       | 896.0 | 0.0     | 0.0 |\n",
      "| throughput  | 06:50:58 | 06:51:58 | 60.3 | 0.0  | 0.0  | 0.0    | 129280.0 | 9512192.0 | 0.0   | 29672.0 | 0.0 |\n",
      "| constant    | 06:52:15 | 06:53:15 | 60.0 | 0.0  | 0.0  | 1536.0 | 0.0      | 0.0       | 768.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:53:37 | 06:54:37 | 60.0 | 0.0  | 0.0  | 1280.0 | 0.0      | 0.0       | 640.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:54:57 | 06:55:57 | 60.0 | 0.0  | 0.0  | 1024.0 | 0.0      | 0.0       | 512.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:56:23 | 06:57:23 | 60.0 | 0.0  | 0.0  | 1024.0 | 0.0      | 0.0       | 512.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:58:01 | 06:59:01 | 60.0 | 0.0  | 0.0  | 768.0  | 0.0      | 0.0       | 384.0 | 0.0     | 0.0 |\n",
      "| constant    | 06:59:46 | 07:00:46 | 60.0 | 0.0  | 0.0  | 512.0  | 0.0      | 0.0       | 256.0 | 0.0     | 0.0 |\n",
      "| constant    | 07:02:28 | 07:03:28 | 60.0 | 0.0  | 0.0  | 256.0  | 0.0      | 0.0       | 128.0 | 0.0     | 0.0 |\n",
      "|=============|==========|==========|======|======|======|========|==========|===========|=======|=========|=====|\n",
      "\n",
      "\n",
      "ℹ Text Metrics Statistics (Completed Requests)\n",
      "|=============|=======|=======|======|======|=======|=======|======|======|========|========|=======|=======|\n",
      "| Benchmark   | Input Tokens             |||| Input Words              |||| Input Characters             ||||\n",
      "| Strategy    | Per Request  || Per Second || Per Request  || Per Second || Per Request    || Per Second   ||\n",
      "|             | Mdn   | p95   | Mdn  | Mean | Mdn   | p95   | Mdn  | Mean | Mdn    | p95    | Mdn   | Mean  |\n",
      "|-------------|-------|-------|------|------|-------|-------|------|------|--------|--------|-------|-------|\n",
      "| synchronous | 256.0 | 256.0 | 27.3 | 31.5 | 213.0 | 215.0 | 22.9 | 26.3 | 1436.0 | 1493.0 | 153.3 | 177.3 |\n",
      "| throughput  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0    | 0.0    | 0.0   | 0.0   |\n",
      "| constant    | 256.0 | 256.0 | 22.5 | 30.9 | 214.0 | 217.0 | 18.8 | 25.8 | 1439.0 | 1481.0 | 130.3 | 172.6 |\n",
      "| constant    | 256.0 | 256.0 | 19.2 | 24.1 | 211.0 | 214.0 | 16.0 | 19.9 | 1435.0 | 1460.0 | 109.3 | 134.3 |\n",
      "| constant    | 256.0 | 256.0 | 16.0 | 20.8 | 211.0 | 216.0 | 13.3 | 17.3 | 1395.0 | 1461.0 | 86.3  | 115.7 |\n",
      "| constant    | 256.0 | 256.0 | 12.8 | 17.2 | 213.0 | 216.0 | 10.6 | 14.4 | 1438.0 | 1476.0 | 73.1  | 97.0  |\n",
      "| constant    | 256.0 | 256.0 | 9.6  | 14.5 | 214.0 | 214.0 | 8.1  | 12.0 | 1417.0 | 1493.0 | 53.3  | 81.3  |\n",
      "| constant    | 256.0 | 256.0 | 12.8 | 12.8 | 211.0 | 214.0 | 10.7 | 10.6 | 1394.0 | 1464.0 | 73.3  | 71.6  |\n",
      "| constant    | 256.0 | 256.0 | 0.0  | 0.0  | 215.0 | 215.0 | 0.0  | 0.0  | 1393.0 | 1393.0 | 0.0   | 0.0   |\n",
      "|=============|=======|=======|======|======|=======|=======|======|======|========|========|=======|=======|\n",
      "| Benchmark   | Output Tokens            |||| Output Words             |||| Output Characters            ||||\n",
      "| Strategy    | Per Request  || Per Second || Per Request  || Per Second || Per Request    || Per Second   ||\n",
      "|             | Mdn   | p95   | Mdn  | Mean | Mdn   | p95   | Mdn  | Mean | Mdn    | p95    | Mdn   | Mean  |\n",
      "|-------------|-------|-------|------|------|-------|-------|------|------|--------|--------|-------|-------|\n",
      "| synchronous | 128.0 | 128.0 | 13.7 | 15.8 | 100.0 | 104.0 | 10.7 | 12.3 | 648.0  | 731.0  | 69.0  | 80.7  |\n",
      "| throughput  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0   | 0.0   | 0.0  | 0.0  | 0.0    | 0.0    | 0.0   | 0.0   |\n",
      "| constant    | 128.0 | 128.0 | 11.3 | 15.4 | 99.0  | 110.0 | 8.8  | 12.0 | 599.0  | 632.0  | 55.6  | 69.4  |\n",
      "| constant    | 128.0 | 128.0 | 9.6  | 12.0 | 103.0 | 111.0 | 7.7  | 9.4  | 623.0  | 642.0  | 48.1  | 56.5  |\n",
      "| constant    | 128.0 | 128.0 | 8.0  | 10.4 | 100.0 | 103.0 | 6.1  | 7.9  | 602.0  | 640.0  | 38.3  | 48.8  |\n",
      "| constant    | 128.0 | 128.0 | 6.4  | 8.6  | 88.0  | 99.0  | 4.6  | 6.1  | 549.0  | 658.0  | 28.9  | 38.6  |\n",
      "| constant    | 128.0 | 128.0 | 4.8  | 7.2  | 101.0 | 101.0 | 3.8  | 5.6  | 634.0  | 645.0  | 23.9  | 35.1  |\n",
      "| constant    | 128.0 | 128.0 | 6.4  | 6.4  | 103.0 | 103.0 | 5.2  | 5.2  | 575.0  | 591.0  | 28.8  | 29.2  |\n",
      "| constant    | 128.0 | 128.0 | 0.0  | 0.0  | 97.0  | 97.0  | 0.0  | 0.0  | 625.0  | 625.0  | 0.0   | 0.0   |\n",
      "|=============|=======|=======|======|======|=======|=======|======|======|========|========|=======|=======|\n",
      "\n",
      "\n",
      "ℹ Request Token Statistics (Completed Requests)\n",
      "|=============|=======|=======|=======|=======|=======|=======|=======|=======|=========|========|\n",
      "| Benchmark   | Input Tok    || Output Tok   || Total Tok    || Stream Iter  || Output Tok      ||\n",
      "| Strategy    | Per Req      || Per Req      || Per Req      || Per Req      || Per Stream Iter ||\n",
      "|             | Mdn   | p95   | Mdn   | p95   | Mdn   | p95   | Mdn   | p95   | Mdn     | p95    |\n",
      "|-------------|-------|-------|-------|-------|-------|-------|-------|-------|---------|--------|\n",
      "| synchronous | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| throughput  | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0   | 0.0     | 0.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "| constant    | 256.0 | 256.0 | 128.0 | 128.0 | 384.0 | 384.0 | 260.0 | 260.0 | 1.0     | 1.0    |\n",
      "|=============|=======|=======|=======|=======|=======|=======|=======|=======|=========|========|\n",
      "\n",
      "\n",
      "ℹ Request Latency Statistics (Completed Requests)\n",
      "|=============|========|=========|=======|========|======|======|======|=======|\n",
      "| Benchmark   | Request Latency || TTFT          || ITL        || TPOT        ||\n",
      "| Strategy    | Sec             || ms            || ms         || ms          ||\n",
      "|             | Mdn    | p95     | Mdn   | p95    | Mdn  | p95  | Mdn  | p95   |\n",
      "|-------------|--------|---------|-------|--------|------|------|------|-------|\n",
      "| synchronous | 9.5    | 10.1    | 594.5 | 927.6  | 70.2 | 72.6 | 74.0 | 79.2  |\n",
      "| throughput  | 0.0    | 0.0     | 0.0   | 0.0    | 0.0  | 0.0  | 0.0  | 0.0   |\n",
      "| constant    | 9.4    | 16.6    | 558.4 | 7476.0 | 69.2 | 71.6 | 73.1 | 129.4 |\n",
      "| constant    | 9.5    | 10.1    | 770.4 | 905.4  | 70.2 | 72.6 | 74.6 | 79.1  |\n",
      "| constant    | 9.5    | 10.1    | 724.9 | 925.5  | 67.8 | 73.4 | 74.1 | 79.2  |\n",
      "| constant    | 9.4    | 10.0    | 773.7 | 864.3  | 67.8 | 72.6 | 73.7 | 77.9  |\n",
      "| constant    | 9.7    | 9.9     | 717.6 | 783.4  | 70.1 | 72.5 | 75.7 | 77.6  |\n",
      "| constant    | 9.5    | 9.6     | 582.7 | 627.6  | 70.2 | 70.5 | 74.2 | 74.9  |\n",
      "| constant    | 9.6    | 9.6     | 520.3 | 520.3  | 71.8 | 71.8 | 75.3 | 75.3  |\n",
      "|=============|========|=========|=======|========|======|======|======|=======|\n",
      "\n",
      "\n",
      "ℹ Server Throughput Statistics (All Requests)\n",
      "|=============|=======|=======|=======|=======|==========|==========|=======|========|==========|==========|\n",
      "| Benchmark   | Requests                   |||| Input Tokens       || Output Tokens || Total Tokens       ||\n",
      "| Strategy    | Per Sec      || Concurrency  || Per Sec            || Per Sec       || Per Sec            ||\n",
      "|             | Mdn   | Mean  | Mdn   | Mean  | Mdn      | Mean     | Mdn   | Mean   | Mdn      | Mean     |\n",
      "|-------------|-------|-------|-------|-------|----------|----------|-------|--------|----------|----------|\n",
      "| synchronous | 0.1   | 0.1   | 1.0   | 1.0   | 26.9     | 31.4     | 14.3  | 13.6   | 14.3     | 40.9     |\n",
      "| throughput  | 588.3 | 615.8 | 507.0 | 507.1 | 152368.6 | 161306.6 | 0.0   | 496.4  | 135951.1 | 161803.1 |\n",
      "| constant    | 0.1   | 0.1   | 1.0   | 1.0   | 22.7     | 30.6     | 14.3  | 13.0   | 14.4     | 39.1     |\n",
      "| constant    | 0.1   | 0.1   | 1.0   | 0.8   | 19.2     | 24.2     | 13.8  | 10.3   | 14.2     | 30.9     |\n",
      "| constant    | 0.1   | 0.1   | 1.0   | 0.6   | 16.1     | 21.1     | 13.6  | 8.9    | 13.8     | 26.6     |\n",
      "| constant    | 0.0   | 0.1   | 0.0   | 0.5   | 12.8     | 17.1     | 13.8  | 7.5    | 14.2     | 22.4     |\n",
      "| constant    | 0.0   | 0.0   | 0.0   | 0.4   | 9.6      | 14.4     | 0.1   | 6.2    | 14.3     | 18.5     |\n",
      "| constant    | 0.0   | 0.0   | 0.0   | 0.3   | 12.8     | 12.8     | 0.1   | 5.2    | 14.2     | 15.7     |\n",
      "| constant    | 0.0   | 0.0   | 0.0   | 0.2   | 0.0      | 0.0      | 13.9  | 14.0   | 13.9     | 42.1     |\n",
      "|=============|=======|=======|=======|=======|==========|==========|=======|========|==========|==========|\n",
      "\n",
      "\n",
      "\n",
      "✔ Benchmarking complete, generated 9 benchmark(s)\n",
      "…   json    : /tmp/benchmark_results/disaggregated/benchmarks.json\n",
      "…   csv     : /tmp/benchmark_results/disaggregated/benchmarks.csv\n",
      "…   html    : /tmp/benchmark_results/disaggregated/benchmarks.html\n",
      "\n",
      "Benchmark [disaggregated] completed on controller.\n",
      "Copying results from controller to benchmark_results/disaggregated/...\n",
      "  Results saved to benchmark_results/disaggregated/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run guidellm sweep against disaggregated proxy\n",
    "run_guidellm(\n",
    "    target_url=f\"http://{CONTROLLER_HOST}:{PROXY_PORT}\",\n",
    "    label=\"disaggregated\",\n",
    "    profile=\"sweep\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a0c498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all services...\n",
      "  All services stopped\n",
      "Phase 3 complete. Disaggregated benchmark results saved.\n"
     ]
    }
   ],
   "source": [
    "# Stop all services\n",
    "stop_all_services()\n",
    "print(\"Phase 3 complete. Disaggregated benchmark results saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acda462",
   "metadata": {},
   "source": [
    "## Step 3: Compare Results\n",
    "\n",
    "Load the benchmark results from all three phases and produce a side-by-side comparison. guidellm saves results as `benchmarks.json` in each output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71dcc4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benchmark results...\n",
      "\n",
      "  single_node: loaded from benchmark_results/single_node/benchmarks.json\n",
      "  replicated: loaded from benchmark_results/replicated/benchmarks.json\n",
      "  disaggregated: loaded from benchmark_results/disaggregated/benchmarks.json\n",
      "\n",
      "================================================================================\n",
      "PEAK THROUGHPUT COMPARISON\n",
      "================================================================================\n",
      "Configuration        Output tok/s    TTFT mean     TTFT P95    TPOT mean  Latency P95\n",
      "--------------------------------------------------------------------------------\n",
      "Single node             406.0 (baseline)     7908.2ms    15376.4ms      390.9ms    59280.4ms\n",
      "Replicated               14.4 (0.04x)      612.6ms      612.6ms       74.4ms     9525.0ms\n",
      "Disaggregated            14.0 (0.03x)      520.3ms      520.3ms       75.3ms     9644.3ms\n",
      "\n",
      "Note: Peak throughput is the highest output tok/s observed across all\n",
      "rate points in the sweep. Other metrics are from that same rate point.\n",
      "\n",
      "Full results in: /home/nvidia/src/github.com/elizabetht/spark/notebooks/02_disaggregated-serving/vllm-native/benchmark_results/\n",
      "  Each subdirectory contains benchmarks.json (raw data) and benchmarks.html (charts)\n"
     ]
    }
   ],
   "source": [
    "def load_guidellm_results(label):\n",
    "    \"\"\"Load guidellm benchmark results from the output directory.\"\"\"\n",
    "    results_path = RESULTS_DIR / label / \"benchmarks.json\"\n",
    "    if not results_path.exists():\n",
    "        # Try alternate filename\n",
    "        alt_paths = list((RESULTS_DIR / label).glob(\"*.json\"))\n",
    "        if alt_paths:\n",
    "            results_path = alt_paths[0]\n",
    "        else:\n",
    "            print(f\"  {label}: no results found in {RESULTS_DIR / label}\")\n",
    "            return None\n",
    "    with open(results_path) as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"  {label}: loaded from {results_path}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_summary(data, label):\n",
    "    \"\"\"Extract key metrics from guidellm v0.5.3 results.\n",
    "\n",
    "    guidellm nests metrics under: metrics -> <metric_name> -> successful -> {mean, percentiles}\n",
    "    Time-related keys use _ms suffixes (already in milliseconds), except\n",
    "    request_latency which is in seconds.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    benchmarks = data.get('benchmarks', [data]) if isinstance(data, dict) else data\n",
    "\n",
    "    summaries = []\n",
    "    for bench in benchmarks:\n",
    "        if not isinstance(bench, dict):\n",
    "            continue\n",
    "        metrics = bench.get('metrics', {})\n",
    "\n",
    "        # Helper to extract a stat from the nested structure\n",
    "        def get_stat(metric_key, stat='mean', category='successful'):\n",
    "            bucket = metrics.get(metric_key, {}).get(category, {})\n",
    "            if stat == 'mean':\n",
    "                return bucket.get('mean', 0) or 0\n",
    "            percentiles = bucket.get('percentiles', {})\n",
    "            return percentiles.get(stat, 0) or 0\n",
    "\n",
    "        # Output tokens/sec (raw number, not ms)\n",
    "        otps_mean = get_stat('output_tokens_per_second', 'mean')\n",
    "\n",
    "        # TTFT: already in ms (key is time_to_first_token_ms)\n",
    "        ttft_mean = get_stat('time_to_first_token_ms', 'mean')\n",
    "        ttft_p95 = get_stat('time_to_first_token_ms', 'p95')\n",
    "\n",
    "        # TPOT: already in ms (key is time_per_output_token_ms)\n",
    "        tpot_mean = get_stat('time_per_output_token_ms', 'mean')\n",
    "        tpot_p95 = get_stat('time_per_output_token_ms', 'p95')\n",
    "\n",
    "        # ITL: already in ms (key is inter_token_latency_ms)\n",
    "        itl_mean = get_stat('inter_token_latency_ms', 'mean')\n",
    "\n",
    "        # Request latency: in seconds, convert to ms\n",
    "        latency_mean_s = get_stat('request_latency', 'mean')\n",
    "        latency_p95_s = get_stat('request_latency', 'p95')\n",
    "\n",
    "        # Request rate\n",
    "        rps_mean = get_stat('requests_per_second', 'mean')\n",
    "\n",
    "        summary = {\n",
    "            'request_rate_mean': rps_mean,\n",
    "            'output_tps_mean': otps_mean,\n",
    "            'ttft_mean_ms': ttft_mean,\n",
    "            'ttft_p95_ms': ttft_p95,\n",
    "            'tpot_mean_ms': tpot_mean,\n",
    "            'tpot_p95_ms': tpot_p95,\n",
    "            'itl_mean_ms': itl_mean,\n",
    "            'latency_mean_ms': latency_mean_s * 1000,\n",
    "            'latency_p95_ms': latency_p95_s * 1000,\n",
    "        }\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return summaries\n",
    "\n",
    "\n",
    "print(\"Loading benchmark results...\\n\")\n",
    "results = {}\n",
    "for label in ['single_node', 'replicated', 'disaggregated']:\n",
    "    data = load_guidellm_results(label)\n",
    "    if data:\n",
    "        results[label] = extract_summary(data, label)\n",
    "\n",
    "# Display peak throughput comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PEAK THROUGHPUT COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Configuration':<20} {'Output tok/s':>12} {'TTFT mean':>12} {'TTFT P95':>12} {'TPOT mean':>12} {'Latency P95':>12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "baseline_tps = None\n",
    "for label, display_name in [('single_node', 'Single node'), ('replicated', 'Replicated'), ('disaggregated', 'Disaggregated')]:\n",
    "    if label not in results or not results[label]:\n",
    "        print(f\"{display_name:<20} {'(no data)':>12}\")\n",
    "        continue\n",
    "\n",
    "    # Find the run with highest throughput (peak of the sweep)\n",
    "    peak = max(results[label], key=lambda x: x['output_tps_mean'])\n",
    "\n",
    "    tps = peak['output_tps_mean']\n",
    "    if baseline_tps is None:\n",
    "        baseline_tps = tps\n",
    "        ratio_str = \"(baseline)\"\n",
    "    else:\n",
    "        ratio = tps / baseline_tps if baseline_tps > 0 else 0\n",
    "        ratio_str = f\"({ratio:.2f}x)\"\n",
    "\n",
    "    print(\n",
    "        f\"{display_name:<20} \"\n",
    "        f\"{tps:>8.1f} {ratio_str:>3} \"\n",
    "        f\"{peak['ttft_mean_ms']:>10.1f}ms \"\n",
    "        f\"{peak['ttft_p95_ms']:>10.1f}ms \"\n",
    "        f\"{peak['tpot_mean_ms']:>10.1f}ms \"\n",
    "        f\"{peak['latency_p95_ms']:>10.1f}ms\"\n",
    "    )\n",
    "\n",
    "print(\"\\nNote: Peak throughput is the highest output tok/s observed across all\")\n",
    "print(\"rate points in the sweep. Other metrics are from that same rate point.\")\n",
    "print(f\"\\nFull results in: {RESULTS_DIR.resolve()}/\")\n",
    "print(\"  Each subdirectory contains benchmarks.json (raw data) and benchmarks.html (charts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6261cb6",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What guidellm adds over manual benchmarks\n",
    "\n",
    "| Metric | Manual (Notebooks 01/03/04) | guidellm (this notebook) |\n",
    "|--------|---------------------------|-------------------------|\n",
    "| Latency breakdown | End-to-end only | TTFT, TPOT, ITL separately |\n",
    "| Statistical rigor | Single data point | P50, P95, P99 distributions |\n",
    "| Load patterns | 8 requests, all at once | Sweep across rates, warmup/cooldown |\n",
    "| Saturation detection | None | Automatic |\n",
    "| Output formats | Print statements | JSON, CSV, HTML charts |\n",
    "\n",
    "### How to read the results\n",
    "\n",
    "- **TTFT (Time to First Token)**: How long until the first token arrives. Disaggregated TTFT includes the NIXL transfer hop, so it will be higher than single-node.\n",
    "- **TPOT (Time Per Output Token)**: Average time between consecutive tokens during decode. This should be similar across configurations since decode is memory-bandwidth-bound on the same GPU.\n",
    "- **ITL (Inter-Token Latency)**: Similar to TPOT but measured as the gap between each token pair. More sensitive to jitter.\n",
    "- **Output tok/s under sweep**: The sweep increases request rate until saturation. Peak throughput is where the system delivers maximum tokens/sec before latency degrades.\n",
    "\n",
    "### Where the HTML reports live\n",
    "\n",
    "Each configuration's `benchmarks.html` file contains interactive charts showing latency vs throughput curves. These are suitable for screenshots in LinkedIn articles or technical documentation.\n",
    "\n",
    "```\n",
    "benchmark_results/\n",
    "  single_node/benchmarks.json, benchmarks.html\n",
    "  replicated/benchmarks.json, benchmarks.html\n",
    "  disaggregated/benchmarks.json, benchmarks.html\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
