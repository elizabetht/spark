{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19dc4012",
   "metadata": {},
   "source": [
    "# Understanding KV Cache\n",
    "\n",
    "The KV cache is what makes disaggregated serving possible, and expensive. Before splitting inference across nodes, we need to understand what this cache contains, how large it is, and what it costs to transfer.\n",
    "\n",
    "## What is KV Cache?\n",
    "\n",
    "In transformer models, each new token attends to all previous tokens. Without caching, we would recompute attention keys and values for every token generated. The KV cache stores these intermediate results.\n",
    "\n",
    "**Systems analogy**: KV cache is session state in a web server. Prefill populates it (like initial login), decode reads from it (like subsequent API calls), and transfer moves it between servers (like session replication).\n",
    "\n",
    "## Why This Matters for Disaggregation\n",
    "\n",
    "When prefill runs on spark-01 and decode runs on spark-02:\n",
    "1. spark-01 processes the prompt and generates the KV cache\n",
    "2. The KV cache transfers from spark-01 to spark-02\n",
    "3. spark-02 uses the cache to generate tokens\n",
    "\n",
    "Transfer speed determines whether disaggregation is worth the overhead. This notebook calculates the numbers using model architecture constants. No model loading required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dbd05a",
   "metadata": {},
   "source": [
    "## Step 1: KV Cache Dimensions from Model Architecture\n",
    "\n",
    "Llama-3.1-8B-Instruct uses Grouped Query Attention (GQA). The relevant architecture constants:\n",
    "\n",
    "| Parameter | Value | Description |\n",
    "|-----------|-------|-------------|\n",
    "| `num_hidden_layers` | 32 | Number of transformer layers |\n",
    "| `num_key_value_heads` | 8 | KV heads per layer (GQA, not full 32) |\n",
    "| `head_dim` | 128 | Dimension per attention head |\n",
    "| `dtype` | float16 | 2 bytes per element |\n",
    "\n",
    "Each layer stores one key tensor and one value tensor. Each tensor has shape `[batch_size, num_kv_heads, sequence_length, head_dim]`.\n",
    "\n",
    "These are fixed architecture constants. We do not need to load the model to compute cache size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45a7e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV Cache Per-Token Size (Llama-3.1-8B-Instruct)\n",
      "==================================================\n",
      "Per layer:  4,096 bytes (4.0 KB)\n",
      "All layers: 131,072 bytes (0.1250 MB)\n",
      "\n",
      "Breakdown:\n",
      "  2 tensors (K+V) x 8 heads x 128 dim x 2 bytes = 4,096 bytes/layer\n",
      "  4,096 bytes/layer x 32 layers = 131,072 bytes/token\n"
     ]
    }
   ],
   "source": [
    "# Llama-3.1-8B-Instruct architecture constants\n",
    "NUM_LAYERS = 32\n",
    "NUM_KV_HEADS = 8       # GQA: 8 KV heads, not 32 attention heads\n",
    "HEAD_DIM = 128\n",
    "BYTES_PER_ELEMENT = 2   # float16\n",
    "\n",
    "# Per-token KV cache size\n",
    "# Each layer: 2 tensors (key + value) x num_kv_heads x head_dim x bytes_per_element\n",
    "bytes_per_token_per_layer = 2 * NUM_KV_HEADS * HEAD_DIM * BYTES_PER_ELEMENT\n",
    "bytes_per_token = bytes_per_token_per_layer * NUM_LAYERS\n",
    "mb_per_token = bytes_per_token / (1024 * 1024)\n",
    "\n",
    "print(\"KV Cache Per-Token Size (Llama-3.1-8B-Instruct)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Per layer:  {bytes_per_token_per_layer:,} bytes ({bytes_per_token_per_layer / 1024:.1f} KB)\")\n",
    "print(f\"All layers: {bytes_per_token:,} bytes ({mb_per_token:.4f} MB)\")\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"  2 tensors (K+V) x {NUM_KV_HEADS} heads x {HEAD_DIM} dim x {BYTES_PER_ELEMENT} bytes = {bytes_per_token_per_layer:,} bytes/layer\")\n",
    "print(f\"  {bytes_per_token_per_layer:,} bytes/layer x {NUM_LAYERS} layers = {bytes_per_token:,} bytes/token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c8f61b",
   "metadata": {},
   "source": [
    "## Step 2: Cache Size at Different Sequence Lengths\n",
    "\n",
    "The KV cache scales linearly with sequence length. A 100-token prompt produces a 100-token cache. A 4,096-token conversation produces a 4,096-token cache. This linear scaling is what makes transfer cost predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f49ac16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length    KV Cache Size   Notes\n",
      "------------------------------------------------------------\n",
      "32                       4.00 MB    Short prompt\n",
      "128                     16.00 MB    \n",
      "512                     64.00 MB    Typical single-turn\n",
      "1024                   128.00 MB    \n",
      "2048                   256.00 MB    Multi-turn conversation\n",
      "4096                   512.00 MB    Long context\n",
      "\n",
      "Scaling: 0.1250 MB per token, linear growth\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [32, 128, 512, 1024, 2048, 4096]\n",
    "\n",
    "print(f\"{'Sequence Length':<18} {'KV Cache Size':<15} {'Notes'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for seq_len in sequence_lengths:\n",
    "    cache_bytes = bytes_per_token * seq_len\n",
    "    cache_mb = cache_bytes / (1024 * 1024)\n",
    "    \n",
    "    note = \"\"\n",
    "    if seq_len == 32:\n",
    "        note = \"Short prompt\"\n",
    "    elif seq_len == 512:\n",
    "        note = \"Typical single-turn\"\n",
    "    elif seq_len == 2048:\n",
    "        note = \"Multi-turn conversation\"\n",
    "    elif seq_len == 4096:\n",
    "        note = \"Long context\"\n",
    "    \n",
    "    print(f\"{seq_len:<18} {cache_mb:>10.2f} MB    {note}\")\n",
    "\n",
    "print(f\"\\nScaling: {mb_per_token:.4f} MB per token, linear growth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acc703e",
   "metadata": {},
   "source": [
    "## Step 3: Transfer Cost, TCP vs RDMA\n",
    "\n",
    "Disaggregated serving moves the KV cache from the prefill node to the decode node. The transport matters.\n",
    "\n",
    "**TCP path (6 copies):** GPU memory → CPU memory → kernel buffer → NIC → wire → NIC → kernel buffer → CPU memory → GPU memory\n",
    "\n",
    "**RDMA path (2 copies):** GPU memory → NIC (GPUDirect) → wire → NIC → GPU memory (GPUDirect)\n",
    "\n",
    "RDMA with GPUDirect bypasses the CPU and kernel entirely. The NIC reads directly from GPU memory on one side and writes directly to GPU memory on the other.\n",
    "\n",
    "We use conservative estimates: 1 GB/s effective for TCP (IPoIB overhead on 100 Gbps link), 10 GB/s effective for RDMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ab783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline from Notebook 01:\n",
      "  Single request: 6910.1 ms for 200 tokens\n",
      "  Per-token decode: 34.6 ms/token\n",
      "\n",
      "Seq Len    Cache Size   TCP Transfer    RDMA Transfer   Decode Time    TCP Overhead   RDMA Overhead\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "128           16.00 MB        15.62 ms         1.56 ms       3455.1 ms          0.5%          0.0%\n",
      "512           64.00 MB        62.50 ms         6.25 ms       3455.1 ms          1.8%          0.2%\n",
      "1024         128.00 MB       125.00 ms        12.50 ms       3455.1 ms          3.6%          0.4%\n",
      "2048         256.00 MB       250.00 ms        25.00 ms       3455.1 ms          7.2%          0.7%\n",
      "4096         512.00 MB       500.00 ms        50.00 ms       3455.1 ms         14.5%          1.4%\n",
      "\n",
      "Overhead = transfer time / decode time for 100 output tokens\n",
      "Decode time based on baseline: 34.6 ms/token x 100 tokens = 3455 ms\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load baseline metrics from Notebook 01\n",
    "baseline_file = Path(\"baseline_metrics.json\")\n",
    "if baseline_file.exists():\n",
    "    with open(baseline_file) as f:\n",
    "        baseline = json.load(f)\n",
    "    single_latency_ms = baseline['single_request']['latency_ms']\n",
    "    single_tokens = baseline['single_request']['tokens']\n",
    "    per_token_ms = single_latency_ms / single_tokens\n",
    "    print(f\"Baseline from Notebook 01:\")\n",
    "    print(f\"  Single request: {single_latency_ms:.1f} ms for {single_tokens} tokens\")\n",
    "    print(f\"  Per-token decode: {per_token_ms:.1f} ms/token\")\n",
    "else:\n",
    "    per_token_ms = 69.2  # Fallback from typical baseline_metrics.json\n",
    "    print(f\"baseline_metrics.json not found, using estimate: {per_token_ms:.1f} ms/token\")\n",
    "\n",
    "# Transfer rates (conservative practical estimates)\n",
    "TCP_GBYTES_PER_SEC = 1.0    # ~8 Gbps effective (IPoIB overhead on 100G link)\n",
    "RDMA_GBYTES_PER_SEC = 10.0  # ~80 Gbps effective (RDMA over same link)\n",
    "\n",
    "print(f\"\\n{'Seq Len':<10} {'Cache Size':<12} {'TCP Transfer':<15} {'RDMA Transfer':<15} {'Decode Time':<14} {'TCP Overhead':<14} {'RDMA Overhead'}\")\n",
    "print(\"-\" * 105)\n",
    "\n",
    "for seq_len in [128, 512, 1024, 2048, 4096]:\n",
    "    cache_bytes = bytes_per_token * seq_len\n",
    "    cache_mb = cache_bytes / (1024 * 1024)\n",
    "    cache_gb = cache_bytes / (1024**3)\n",
    "    \n",
    "    tcp_ms = (cache_gb / TCP_GBYTES_PER_SEC) * 1000\n",
    "    rdma_ms = (cache_gb / RDMA_GBYTES_PER_SEC) * 1000\n",
    "    \n",
    "    # Decode time for 100 output tokens at baseline rate\n",
    "    decode_ms = per_token_ms * 100\n",
    "    \n",
    "    tcp_overhead_pct = (tcp_ms / decode_ms) * 100\n",
    "    rdma_overhead_pct = (rdma_ms / decode_ms) * 100\n",
    "    \n",
    "    print(f\"{seq_len:<10} {cache_mb:>8.2f} MB   {tcp_ms:>10.2f} ms   {rdma_ms:>10.2f} ms   {decode_ms:>10.1f} ms   {tcp_overhead_pct:>10.1f}%   {rdma_overhead_pct:>10.1f}%\")\n",
    "\n",
    "print(f\"\\nOverhead = transfer time / decode time for 100 output tokens\")\n",
    "print(f\"Decode time based on baseline: {per_token_ms:.1f} ms/token x 100 tokens = {per_token_ms * 100:.0f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7903290d",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**KV Cache Structure:**\n",
    "- Llama-3.1-8B stores 2 tensors (key + value) per layer, 32 layers total\n",
    "- With GQA, only 8 KV heads (not 32), so cache is smaller than full attention\n",
    "- Per-token size: ~0.125 MB (131,072 bytes)\n",
    "\n",
    "**Transfer Cost:**\n",
    "- Linear with sequence length: double the tokens, double the transfer\n",
    "- TCP adds significant overhead at longer sequences (kernel copies, serialization)\n",
    "- RDMA keeps overhead minimal by bypassing the CPU entirely\n",
    "\n",
    "**Why This Matters for Notebook 03:**\n",
    "- vLLM's `NixlConnector` uses RDMA (NIXL) for KV cache transfer between nodes\n",
    "- The transfer overhead we calculated here is what NixlConnector minimizes\n",
    "- At typical sequence lengths (512-2048 tokens), RDMA overhead stays under 1%\n",
    "\n",
    "**What's Next:**\n",
    "- [03_Disaggregated_Serving.ipynb](03_Disaggregated_Serving.ipynb): Run prefill on spark-01, decode on spark-02, and measure the actual overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
