{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53dcb80",
   "metadata": {},
   "source": [
    "# Disaggregated Serving with vLLM and NIXL\n",
    "\n",
    "Split LLM inference across two DGX Spark nodes: prefill on spark-01, decode on spark-02. KV cache transfers between nodes via NIXL (GPU-to-GPU RDMA).\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      |\n",
    "      v\n",
    "  [Proxy Server] (controller:8192)    ← CPU node, no GPU needed\n",
    "      |\n",
    "      +---> [Prefill Instance] (spark-01:8100)\n",
    "      |         |\n",
    "      |         | KV Cache via NIXL/RDMA\n",
    "      |         v\n",
    "      +---> [Decode Instance]  (spark-02:8200)\n",
    "                  |\n",
    "                  v\n",
    "          Response (tokens)\n",
    "```\n",
    "\n",
    "The proxy runs on a dedicated CPU node (`controller`) so the GPU nodes are fully available for inference.\n",
    "\n",
    "## Prerequisites\n",
    "- Notebooks 00, 01, and 03 completed (environment verified, baseline and replicated metrics measured)\n",
    "- Notebook 02 reviewed (KV cache size and transfer cost understood)\n",
    "- vLLM **cu130 build** installed on both GPU nodes (Step 2 will verify and install if needed)\n",
    "- Model cached on both GPU nodes\n",
    "- Passwordless SSH to controller, spark-01, and spark-02\n",
    "\n",
    "> **CUDA 13 requirement:** DGX Spark ships with CUDA 13 only. The default PyPI `vllm` package links against CUDA 12 (`libcudart.so.12`), which does not exist on these systems. You need the `cu130` build from `wheels.vllm.ai`, along with a matching PyTorch cu130 build. Step 2 below checks for this automatically and installs the correct versions if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fcb7ac",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration and Verify Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab29c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from environment_config.json\n",
      "Loaded baseline from baseline_metrics.json\n",
      "  Single request: 6874.1 ms, 14.5 tok/s\n",
      "  Batch (8 req):  122.5 tok/s\n",
      "Loaded replicated metrics from replicated_metrics.json\n",
      "  Single request: 7303.8 ms, 13.7 tok/s\n",
      "  Batch (8 req):  26.5 tok/s\n",
      "\n",
      "Configuration:\n",
      "  Prefill node:   192.168.100.10:8100 (RDMA: 192.168.100.10, LAN: 192.168.1.76)\n",
      "  Decode node:    192.168.100.11:8200 (RDMA: 192.168.100.11, LAN: 192.168.1.77)\n",
      "  Proxy/Router:   192.168.1.75:8192 (controller, routes via LAN)\n",
      "  NIXL port:      5600\n",
      "  Model:          meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment config from Notebook 00\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    print(f\"Loaded config from {config_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Run 00_Environment_Setup.ipynb first\")\n",
    "\n",
    "# Load baseline metrics from Notebook 01\n",
    "baseline_file = Path(\"baseline_metrics.json\")\n",
    "if baseline_file.exists():\n",
    "    with open(baseline_file) as f:\n",
    "        baseline = json.load(f)\n",
    "    print(f\"Loaded baseline from {baseline_file}\")\n",
    "    print(f\"  Single request: {baseline['single_request']['latency_ms']:.1f} ms, \"\n",
    "          f\"{baseline['single_request']['throughput_tokens_per_sec']:.1f} tok/s\")\n",
    "    print(f\"  Batch (8 req):  {baseline['batch_processing']['throughput_tokens_per_sec']:.1f} tok/s\")\n",
    "else:\n",
    "    print(\"WARNING: baseline_metrics.json not found. Run 01_Local_Inference_Baseline.ipynb first.\")\n",
    "    baseline = None\n",
    "\n",
    "# Load replicated metrics from Notebook 03\n",
    "replicated_file = Path(\"replicated_metrics.json\")\n",
    "if replicated_file.exists():\n",
    "    with open(replicated_file) as f:\n",
    "        replicated = json.load(f)\n",
    "    print(f\"Loaded replicated metrics from {replicated_file}\")\n",
    "    print(f\"  Single request: {replicated['single_request']['latency_ms']:.1f} ms, \"\n",
    "          f\"{replicated['single_request']['throughput_tokens_per_sec']:.1f} tok/s\")\n",
    "    print(f\"  Batch (8 req):  {replicated['batch_processing']['throughput_tokens_per_sec']:.1f} tok/s\")\n",
    "else:\n",
    "    print(\"WARNING: replicated_metrics.json not found. Run 03_Replicated_Serving.ipynb first.\")\n",
    "    replicated = None\n",
    "\n",
    "# Configuration\n",
    "# InfiniBand IPs (192.168.100.x): direct link between GPU nodes, used for\n",
    "# NIXL/RDMA transfers and vLLM side-channel. Not routable from controller.\n",
    "PREFILL_HOST = env_config['network']['node1_ip']  # spark-01: 192.168.100.10\n",
    "DECODE_HOST = env_config['network']['node2_ip']    # spark-02: 192.168.100.11\n",
    "\n",
    "# LAN IPs (192.168.1.x): shared subnet between all three nodes.\n",
    "# The proxy on the controller uses these to reach vLLM's HTTP API.\n",
    "PREFILL_LAN_HOST = \"192.168.1.76\"                  # spark-01 LAN\n",
    "DECODE_LAN_HOST = \"192.168.1.77\"                   # spark-02 LAN\n",
    "CONTROLLER_HOST = \"192.168.1.75\"                   # controller: CPU-only node\n",
    "MODEL_NAME = env_config['model']['name']\n",
    "PREFILL_PORT = 8100\n",
    "DECODE_PORT = 8200\n",
    "PROXY_PORT = 8192\n",
    "NIXL_PORT = 5600\n",
    "\n",
    "# Virtual environment paths\n",
    "VENV_PATH = os.path.expanduser(\"~/src/github.com/elizabetht/spark/.venv\")          # GPU nodes\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Prefill node:   {PREFILL_HOST}:{PREFILL_PORT} (RDMA: {PREFILL_HOST}, LAN: {PREFILL_LAN_HOST})\")\n",
    "print(f\"  Decode node:    {DECODE_HOST}:{DECODE_PORT} (RDMA: {DECODE_HOST}, LAN: {DECODE_LAN_HOST})\")\n",
    "print(f\"  Proxy/Router:   {CONTROLLER_HOST}:{PROXY_PORT} (controller, routes via LAN)\")\n",
    "print(f\"  NIXL port:      {NIXL_PORT}\")\n",
    "print(f\"  Model:          {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d54d2d",
   "metadata": {},
   "source": [
    "## Step 2: Verify SSH Connectivity\n",
    "\n",
    "We need passwordless SSH to spark-02 (decode worker) and to the controller (proxy). The prefill worker runs locally on spark-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91585254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking SSH connectivity...\n",
      "\n",
      "  SSH to spark-02 (192.168.100.11): OK (hostname: spark-02)\n",
      "  SSH to controller (192.168.1.75): OK (hostname: controller)\n",
      "\n",
      "Model on spark-02: FOUND\n",
      "\\nvLLM CUDA 13 build check:\n",
      "  spark-01: Version: 0.13.0+cu130 (CUDA 13 build)\n",
      "  spark-02: Version: 0.13.0+cu130 (CUDA 13 build)\n",
      "\n",
      "Controller proxy dependencies:\n",
      "  fastapi: v0.128.2\n",
      "  httpx: v0.28.1\n",
      "  uvicorn: v0.40.0\n",
      "  All dependencies available\n"
     ]
    }
   ],
   "source": [
    "def check_ssh(host, label):\n",
    "    \"\"\"Verify passwordless SSH connectivity.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', '-o', 'BatchMode=yes',\n",
    "         f'nvidia@{host}', 'hostname'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  SSH to {label} ({host}): OK (hostname: {result.stdout.strip()})\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"  SSH to {label} ({host}): FAILED\")\n",
    "        print(f\"    Error: {result.stderr.strip()}\")\n",
    "        print(f\"    Fix: ssh-copy-id nvidia@{host}\")\n",
    "        return False\n",
    "\n",
    "# Check SSH to all remote nodes\n",
    "print(\"Checking SSH connectivity...\\n\")\n",
    "check_ssh(DECODE_HOST, \"spark-02\")\n",
    "check_ssh(CONTROLLER_HOST, \"controller\")\n",
    "\n",
    "# Verify model is cached on spark-02\n",
    "result = subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{DECODE_HOST}',\n",
    "     'ls -d ~/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct 2>/dev/null && echo FOUND || echo MISSING'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "model_status = result.stdout.strip().split('\\n')[-1]\n",
    "print(f\"\\nModel on spark-02: {model_status}\")\n",
    "\n",
    "# Verify vLLM is installed with CUDA 13 support on both GPU nodes.\n",
    "# The default PyPI build links against CUDA 12 (libcudart.so.12), which is not\n",
    "# available on DGX Spark (CUDA 13 only). The cu130 build is required.\n",
    "#\n",
    "# Install commands (run on each GPU node):\n",
    "#   pip install vllm==0.13.0 --extra-index-url https://wheels.vllm.ai/0.13.0/cu130 --extra-index-url https://download.pytorch.org/whl/cu130\n",
    "#   pip install torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1 --index-url https://download.pytorch.org/whl/cu130\n",
    "VLLM_INSTALL_CMD = (\n",
    "    f\"{VENV_PATH}/bin/pip install vllm==0.13.0 \"\n",
    "    f\"--extra-index-url https://wheels.vllm.ai/0.13.0/cu130 \"\n",
    "    f\"--extra-index-url https://download.pytorch.org/whl/cu130\"\n",
    ")\n",
    "TORCH_INSTALL_CMD = (\n",
    "    f\"{VENV_PATH}/bin/pip install torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1 \"\n",
    "    f\"--index-url https://download.pytorch.org/whl/cu130\"\n",
    ")\n",
    "\n",
    "print(\"\\\\nvLLM CUDA 13 build check:\")\n",
    "for host, label in [(PREFILL_HOST, \"spark-01\"), (DECODE_HOST, \"spark-02\")]:\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "         f'{VENV_PATH}/bin/pip show vllm 2>/dev/null | grep Version'],\n",
    "        capture_output=True, text=True, timeout=15\n",
    "    )\n",
    "    version_line = result.stdout.strip()\n",
    "    if \"cu130\" in version_line:\n",
    "        print(f\"  {label}: {version_line} (CUDA 13 build)\")\n",
    "    elif version_line:\n",
    "        print(f\"  {label}: {version_line} (wrong build, needs cu130)\")\n",
    "        print(f\"    Installing vLLM cu130 on {label}... (this takes several minutes)\")\n",
    "        vllm_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}', VLLM_INSTALL_CMD],\n",
    "            capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "        if vllm_result.returncode == 0:\n",
    "            print(f\"    vLLM cu130 installed on {label}\")\n",
    "        else:\n",
    "            print(f\"    vLLM install failed on {label}: {vllm_result.stderr.strip()[-200:]}\")\n",
    "\n",
    "        print(f\"    Installing torch cu130 on {label}...\")\n",
    "        torch_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}', TORCH_INSTALL_CMD],\n",
    "            capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "        if torch_result.returncode == 0:\n",
    "            print(f\"    torch cu130 installed on {label}\")\n",
    "        else:\n",
    "            print(f\"    torch install failed on {label}: {torch_result.stderr.strip()[-200:]}\")\n",
    "    else:\n",
    "        print(f\"  {label}: vLLM NOT FOUND\")\n",
    "        print(f\"    Installing vLLM cu130 on {label}... (this takes several minutes)\")\n",
    "        vllm_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}', VLLM_INSTALL_CMD],\n",
    "            capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "        if vllm_result.returncode == 0:\n",
    "            print(f\"    vLLM cu130 installed on {label}\")\n",
    "        else:\n",
    "            print(f\"    vLLM install failed on {label}: {vllm_result.stderr.strip()[-200:]}\")\n",
    "\n",
    "        print(f\"    Installing torch cu130 on {label}...\")\n",
    "        torch_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}', TORCH_INSTALL_CMD],\n",
    "            capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "        if torch_result.returncode == 0:\n",
    "            print(f\"    torch cu130 installed on {label}\")\n",
    "        else:\n",
    "            print(f\"    torch install failed on {label}: {torch_result.stderr.strip()[-200:]}\")\n",
    "\n",
    "# Verify controller venv has proxy dependencies (fastapi, httpx, uvicorn).\n",
    "# If any are missing, install them:\n",
    "#   ssh nvidia@192.168.1.75\n",
    "#   source ~/src/github.com/elizabetht/spark/.venv/bin/activate\n",
    "#   pip install fastapi httpx uvicorn\n",
    "print(f\"\\nController proxy dependencies:\")\n",
    "missing_pkgs = []\n",
    "for pkg in [\"fastapi\", \"httpx\", \"uvicorn\"]:\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "         f'{VENV_PATH}/bin/python -c \"import {pkg}; print({pkg}.__version__)\"'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  {pkg}: v{result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(f\"  {pkg}: MISSING\")\n",
    "        missing_pkgs.append(pkg)\n",
    "\n",
    "if missing_pkgs:\n",
    "    pkgs_str = \" \".join(missing_pkgs)\n",
    "    print(f\"\\nInstalling missing packages on controller: {pkgs_str}\")\n",
    "    install = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "         f'{VENV_PATH}/bin/pip install {pkgs_str}'],\n",
    "        capture_output=True, text=True, timeout=120\n",
    "    )\n",
    "    if install.returncode == 0:\n",
    "        print(f\"  Installed successfully\")\n",
    "    else:\n",
    "        print(f\"  Installation failed:\")\n",
    "        print(f\"  {install.stderr.strip()}\")\n",
    "else:\n",
    "    print(\"  All dependencies available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caedb5f",
   "metadata": {},
   "source": [
    "## Step 3: Stop Existing vLLM Processes\n",
    "\n",
    "Notebook 01 runs vLLM as an in-process `LLM` instance that holds GPU memory until the kernel is restarted. Any leftover vLLM processes on either GPU node will consume memory and prevent the disaggregated instances from starting. This step kills stale processes on both nodes before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26f93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for stale vLLM processes...\n",
      "\n",
      "  spark-01 (192.168.100.10): found 2 vLLM process(es), stopping...\n",
      "  spark-01 (192.168.100.10): stopped\n",
      "  spark-02 (192.168.100.11): found 1 vLLM process(es), stopping...\n",
      "  spark-02 (192.168.100.11): stopped\n",
      "  controller (192.168.1.75): cleared any stale proxy\n",
      "\n",
      "GPU memory status:\n",
      "  spark-01: [N/A] MiB / [N/A] MiB used\n",
      "  spark-02: [N/A] MiB / [N/A] MiB used\n"
     ]
    }
   ],
   "source": [
    "def stop_stale_vllm(host, label):\n",
    "    \"\"\"Kill any running vLLM processes on a node.\"\"\"\n",
    "    # Check for running vLLM processes\n",
    "    check = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "         'pgrep -f \"vllm\" | head -5'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if check.stdout.strip():\n",
    "        pids = check.stdout.strip().split('\\n')\n",
    "        print(f\"  {label} ({host}): found {len(pids)} vLLM process(es), stopping...\")\n",
    "        subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "             'pkill -f \"vllm\" || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        # Wait briefly for processes to terminate, then force-kill survivors\n",
    "        time.sleep(2)\n",
    "        subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "             'pkill -9 -f \"vllm\" 2>/dev/null || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        print(f\"  {label} ({host}): stopped\")\n",
    "    else:\n",
    "        print(f\"  {label} ({host}): no vLLM processes running\")\n",
    "\n",
    "print(\"Checking for stale vLLM processes...\\n\")\n",
    "stop_stale_vllm(PREFILL_HOST, \"spark-01\")\n",
    "stop_stale_vllm(DECODE_HOST, \"spark-02\")\n",
    "\n",
    "# Also stop any leftover proxy on the controller\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     'pkill -f disagg_proxy.py 2>/dev/null || true'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "print(f\"  controller ({CONTROLLER_HOST}): cleared any stale proxy\")\n",
    "\n",
    "# Verify GPU memory is free on both nodes\n",
    "print(\"\\nGPU memory status:\")\n",
    "for host, label in [(PREFILL_HOST, \"spark-01\"), (DECODE_HOST, \"spark-02\")]:\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "         'nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        used, total = result.stdout.strip().split(', ')\n",
    "        print(f\"  {label}: {used} MiB / {total} MiB used\")\n",
    "    else:\n",
    "        print(f\"  {label}: unable to query GPU (nvidia-smi failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b42d3",
   "metadata": {},
   "source": [
    "## Step 4: Start the Prefill Instance (spark-01)\n",
    "\n",
    "The prefill instance processes incoming prompts and generates the KV cache. It runs on the local node (spark-01).\n",
    "\n",
    "Key configuration:\n",
    "- `kv_connector: NixlConnector`: Uses NIXL for GPU-to-GPU RDMA cache transfer\n",
    "- `kv_role: kv_both`: NixlConnector does not use this field to determine behavior. The proxy controls which instance acts as prefiller vs decoder by injecting `kv_transfer_params` into requests.\n",
    "- `VLLM_NIXL_SIDE_CHANNEL_HOST`: Must be set to the node's own IP so the NIXL side-channel binds to a reachable address. Without this, NIXL binds to localhost and cross-node handshake fails.\n",
    "- `kv_ip` / `kv_port`: NIXL side-channel endpoint for cache coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b264237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: /home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "Starting prefill instance on spark-01...\n",
      "Command: vllm serve ... --port 8100 --kv-transfer-config '{...NixlConnector...}'\n",
      "Prefill process started (PID: 965917)\n",
      "Log: tail -f /tmp/vllm_prefill.log\n"
     ]
    }
   ],
   "source": [
    "# Find model snapshot path (same logic as Notebook 01)\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "model_slug = MODEL_NAME.replace(\"/\", \"--\")\n",
    "model_cache = list(cache_dir.glob(f\"models--{model_slug}*\"))\n",
    "\n",
    "if model_cache:\n",
    "    snapshots_dir = model_cache[0] / \"snapshots\"\n",
    "    if snapshots_dir.exists():\n",
    "        snapshot_dirs = list(snapshots_dir.iterdir())\n",
    "        if snapshot_dirs:\n",
    "            MODEL_PATH = str(snapshot_dirs[0])\n",
    "        else:\n",
    "            MODEL_PATH = str(model_cache[0])\n",
    "    else:\n",
    "        MODEL_PATH = str(model_cache[0])\n",
    "    print(f\"Model path: {MODEL_PATH}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model {MODEL_NAME} not found in cache\")\n",
    "\n",
    "# KV transfer configuration for NixlConnector\n",
    "# kv_role is a placeholder for NixlConnector: the proxy determines the actual\n",
    "# prefill/decode roles by routing requests. kv_ip and kv_port configure the\n",
    "# NIXL side-channel used for cross-node cache coordination.\n",
    "KV_TRANSFER_CONFIG = json.dumps({\n",
    "    \"kv_connector\": \"NixlConnector\",\n",
    "    \"kv_role\": \"kv_both\",\n",
    "    \"kv_ip\": PREFILL_HOST,\n",
    "    \"kv_port\": NIXL_PORT\n",
    "})\n",
    "\n",
    "# Build the prefill vLLM command\n",
    "# VLLM_NIXL_SIDE_CHANNEL_HOST: required for cross-machine NIXL handshake.\n",
    "# Without it, NIXL binds to localhost and the decode node cannot connect.\n",
    "prefill_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"CUDA_HOME=/usr/local/cuda-13.0 \"\n",
    "    f\"HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \"\n",
    "    f\"VLLM_NIXL_SIDE_CHANNEL_HOST={PREFILL_HOST} \"\n",
    "    f\"VLLM_NIXL_SIDE_CHANNEL_PORT={NIXL_PORT} \"\n",
    "    f\"vllm serve {MODEL_PATH} \"\n",
    "    f\"--port {PREFILL_PORT} \"\n",
    "    f\"--gpu-memory-utilization 0.3 \"\n",
    "    f\"--kv-transfer-config '{KV_TRANSFER_CONFIG}' \"\n",
    "    f\"--tensor-parallel-size 1 \"\n",
    "    f\"> /tmp/vllm_prefill.log 2>&1\"\n",
    ")\n",
    "\n",
    "print(\"Starting prefill instance on spark-01...\")\n",
    "print(f\"Command: vllm serve ... --port {PREFILL_PORT} --kv-transfer-config '{{...NixlConnector...}}'\")\n",
    "\n",
    "# Start prefill as background process.\n",
    "# stdout/stderr are redirected to the log file in the shell command,\n",
    "# so we use DEVNULL here to avoid a broken-pipe when the cell finishes.\n",
    "prefill_proc = subprocess.Popen(\n",
    "    prefill_cmd, shell=True,\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    "    preexec_fn=os.setsid\n",
    ")\n",
    "\n",
    "print(f\"Prefill process started (PID: {prefill_proc.pid})\")\n",
    "print(f\"Log: tail -f /tmp/vllm_prefill.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c88c5e6",
   "metadata": {},
   "source": [
    "## Step 5: Start the Decode Instance (spark-02)\n",
    "\n",
    "The decode instance runs on spark-02. It receives KV cache from the prefill node via NIXL/RDMA and generates output tokens.\n",
    "\n",
    "We start it via SSH. The configuration mirrors the prefill instance: same `kv_role: kv_both`, same NixlConnector. `VLLM_NIXL_SIDE_CHANNEL_HOST` is set to spark-02's IP so the prefill node can reach it for the NIXL handshake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0617af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting decode instance on spark-02 via SSH...\n",
      "Command: ssh nvidia@192.168.100.11 'vllm serve ... --port 8200 --kv-transfer-config {...NixlConnector...}'\n",
      "Decode process started on spark-02 (local PID: 966016)\n",
      "Remote log: ssh nvidia@192.168.100.11 'tail -f /tmp/vllm_decode.log'\n"
     ]
    }
   ],
   "source": [
    "# KV transfer configuration for decode (same structure as prefill)\n",
    "# NixlConnector treats kv_role as a placeholder. The proxy determines\n",
    "# which instance acts as prefiller vs decoder via kv_transfer_params.\n",
    "KV_TRANSFER_CONFIG_DECODE = json.dumps({\n",
    "    \"kv_connector\": \"NixlConnector\",\n",
    "    \"kv_role\": \"kv_both\",\n",
    "    \"kv_ip\": DECODE_HOST,\n",
    "    \"kv_port\": NIXL_PORT\n",
    "})\n",
    "\n",
    "# Build decode command for remote execution\n",
    "# VLLM_NIXL_SIDE_CHANNEL_HOST must be the decode node's own IP so NIXL\n",
    "# binds to an address reachable from the prefill node.\n",
    "decode_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"CUDA_HOME=/usr/local/cuda-13.0 \"\n",
    "    f\"HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \"\n",
    "    f\"VLLM_NIXL_SIDE_CHANNEL_HOST={DECODE_HOST} \"\n",
    "    f\"VLLM_NIXL_SIDE_CHANNEL_PORT={NIXL_PORT} \"\n",
    "    f\"vllm serve {MODEL_PATH} \"\n",
    "    f\"--port {DECODE_PORT} \"\n",
    "    f\"--gpu-memory-utilization 0.3 \"\n",
    "    f\"--kv-transfer-config '{KV_TRANSFER_CONFIG_DECODE}' \"\n",
    "    f\"--tensor-parallel-size 1 \"\n",
    "    f\"> /tmp/vllm_decode.log 2>&1\"\n",
    ")\n",
    "\n",
    "print(\"Starting decode instance on spark-02 via SSH...\")\n",
    "print(f\"Command: ssh nvidia@{DECODE_HOST} 'vllm serve ... --port {DECODE_PORT} --kv-transfer-config {{...NixlConnector...}}'\")\n",
    "\n",
    "# Start decode on remote node.\n",
    "# Output is redirected to the log file on spark-02 in the shell command.\n",
    "decode_proc = subprocess.Popen(\n",
    "    ['ssh', f'nvidia@{DECODE_HOST}', decode_cmd],\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    "    preexec_fn=os.setsid\n",
    ")\n",
    "print(f\"Decode process started on spark-02 (local PID: {decode_proc.pid})\")\n",
    "print(f\"Remote log: ssh nvidia@{DECODE_HOST} 'tail -f /tmp/vllm_decode.log'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6632988",
   "metadata": {},
   "source": [
    "## Step 6: Wait for Both Instances to Be Ready\n",
    "\n",
    "vLLM takes time to load the model and initialize NIXL. We poll the health endpoints until both respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f30c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for vLLM instances to load model and initialize NIXL...\n",
      "(This typically takes 2-4 minutes per instance)\n",
      "\n",
      "  Prefill (spark-01): waiting... (0s / 300s)\n",
      "  Prefill (spark-01): waiting... (10s / 300s)\n",
      "  Prefill (spark-01): waiting... (20s / 300s)\n",
      "  Prefill (spark-01): ready (30s)\n",
      "  Decode (spark-02): ready (0s)\n",
      "\n",
      "Both instances ready.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "def wait_for_server(host, port, label, timeout=300, interval=10):\n",
    "    \"\"\"Poll a vLLM server's health endpoint until it responds.\"\"\"\n",
    "    url = f\"http://{host}:{port}/health\"\n",
    "    start = time.time()\n",
    "    \n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            req = urllib.request.Request(url, method='GET')\n",
    "            with urllib.request.urlopen(req, timeout=5) as resp:\n",
    "                if resp.status == 200:\n",
    "                    elapsed = time.time() - start\n",
    "                    print(f\"  {label}: ready ({elapsed:.0f}s)\")\n",
    "                    return True\n",
    "        except (urllib.error.URLError, ConnectionRefusedError, OSError):\n",
    "            pass\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  {label}: waiting... ({elapsed:.0f}s / {timeout}s)\")\n",
    "        time.sleep(interval)\n",
    "    \n",
    "    print(f\"  {label}: TIMEOUT after {timeout}s\")\n",
    "    return False\n",
    "\n",
    "print(\"Waiting for vLLM instances to load model and initialize NIXL...\")\n",
    "print(\"(This typically takes 2-4 minutes per instance)\\n\")\n",
    "\n",
    "prefill_ready = wait_for_server(PREFILL_HOST, PREFILL_PORT, \"Prefill (spark-01)\")\n",
    "decode_ready = wait_for_server(DECODE_HOST, DECODE_PORT, \"Decode (spark-02)\")\n",
    "\n",
    "if prefill_ready and decode_ready:\n",
    "    print(\"\\nBoth instances ready.\")\n",
    "else:\n",
    "    print(\"\\nOne or both instances failed to start.\")\n",
    "    print(\"Check logs:\")\n",
    "    print(f\"  Prefill: tail -50 /tmp/vllm_prefill.log\")\n",
    "    print(f\"  Decode:  ssh nvidia@{DECODE_HOST} 'tail -50 /tmp/vllm_decode.log'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bc09e0",
   "metadata": {},
   "source": [
    "## Step 7: Start the Proxy Server (Controller)\n",
    "\n",
    "The proxy runs on the controller node (CPU-only), keeping GPU nodes fully dedicated to inference. It orchestrates the prefill-then-decode pipeline using `kv_transfer_params`, the mechanism vLLM uses to coordinate KV cache transfers between disaggregated instances.\n",
    "\n",
    "### How `kv_transfer_params` works\n",
    "\n",
    "1. Client sends a request to the proxy\n",
    "2. Proxy forwards to prefill with `kv_transfer_params.do_remote_decode = true` and `max_tokens = 1`. This tells vLLM: \"process this prompt, build the KV cache, but don't decode. Another instance will handle decoding.\"\n",
    "3. vLLM returns a response with populated `kv_transfer_params` containing `remote_engine_id` and `remote_block_ids`: the cache location metadata the decode instance needs\n",
    "4. Proxy sends the original request to decode, passing the populated `kv_transfer_params`. The decode instance uses this metadata to pull the KV cache via NIXL/RDMA and generate tokens.\n",
    "\n",
    "This is the same pattern used in vLLM's reference `toy_proxy_server.py`. In production, AI Dynamo replaces this with a KV-aware router.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "The proxy uses FastAPI + httpx (async HTTP client) + uvicorn, matching vLLM's reference implementation. These are installed in a venv on the controller node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6706520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy script written to /tmp/disagg_proxy.py\n",
      "Proxy script copied to controller (192.168.1.75)\n",
      "Proxy launch command sent to controller\n",
      "Log: ssh nvidia@192.168.1.75 'tail -f /tmp/disagg_proxy.log'\n",
      "  Proxy (controller): ready (0s)\n"
     ]
    }
   ],
   "source": [
    "# Proxy script based on vLLM's toy_proxy_server.py.\n",
    "# Orchestrates the prefill -> decode pipeline using kv_transfer_params.\n",
    "proxy_script = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Disaggregated serving proxy using kv_transfer_params.\n",
    "\n",
    "Based on vLLM's toy_proxy_server.py. Routes completions requests through\n",
    "a prefill-then-decode pipeline, passing KV cache metadata between instances.\n",
    "\"\"\"\n",
    "import uuid\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import httpx\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [proxy] %(message)s\")\n",
    "logger = logging.getLogger(\"disagg_proxy\")\n",
    "\n",
    "PREFILL_URL = \"http://{PREFILL_LAN_HOST}:{PREFILL_PORT}\"\n",
    "DECODE_URL = \"http://{DECODE_LAN_HOST}:{DECODE_PORT}\"\n",
    "TIMEOUT = httpx.Timeout(timeout=120.0)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {{\"status\": \"ok\"}}\n",
    "\n",
    "async def send_prefill_request(client: httpx.AsyncClient, path: str, body: dict, request_id: str):\n",
    "    \"\"\"\n",
    "    Send request to prefill instance with kv_transfer_params injected.\n",
    "\n",
    "    Sets max_tokens=1 and do_remote_decode=True so the prefill instance\n",
    "    processes the prompt and builds the KV cache without generating tokens.\n",
    "    The response will contain populated kv_transfer_params with the cache\n",
    "    location metadata (remote_engine_id, remote_block_ids).\n",
    "\n",
    "    Note: kv_transfer_params is a top-level field on vLLM's CompletionRequest,\n",
    "    not nested in extra_body. The extra_body pattern is specific to the OpenAI\n",
    "    Python client, which merges extra_body keys into the top-level JSON body.\n",
    "    With raw HTTP (httpx), we set kv_transfer_params directly.\n",
    "    \"\"\"\n",
    "    prefill_body = dict(body)\n",
    "    prefill_body[\"max_tokens\"] = 1\n",
    "    prefill_body[\"stream\"] = False\n",
    "    prefill_body[\"kv_transfer_params\"] = {{\n",
    "        \"do_remote_decode\": True,\n",
    "        \"do_remote_prefill\": False,\n",
    "        \"remote_engine_id\": None,\n",
    "        \"remote_block_ids\": None,\n",
    "        \"remote_host\": None,\n",
    "        \"remote_port\": None,\n",
    "    }}\n",
    "\n",
    "    headers = {{\"X-Request-Id\": request_id}}\n",
    "    url = f\"{{PREFILL_URL}}{{path}}\"\n",
    "    logger.info(f\"Prefill request {{request_id}} -> {{url}}\")\n",
    "\n",
    "    resp = await client.post(url, json=prefill_body, headers=headers, timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "async def send_decode_request(client: httpx.AsyncClient, path: str, body: dict,\n",
    "                               kv_transfer_params: dict, request_id: str):\n",
    "    \"\"\"\n",
    "    Send request to decode instance with populated kv_transfer_params.\n",
    "\n",
    "    The decode instance uses the metadata (remote_engine_id, remote_block_ids)\n",
    "    to pull the KV cache from the prefill instance via NIXL/RDMA and generate\n",
    "    the full response.\n",
    "    \"\"\"\n",
    "    decode_body = dict(body)\n",
    "    decode_body[\"kv_transfer_params\"] = kv_transfer_params\n",
    "\n",
    "    headers = {{\"X-Request-Id\": request_id}}\n",
    "    url = f\"{{DECODE_URL}}{{path}}\"\n",
    "    logger.info(f\"Decode request {{request_id}} -> {{url}}\")\n",
    "\n",
    "    resp = await client.post(url, json=decode_body, headers=headers, timeout=TIMEOUT)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def handle_request(request: Request):\n",
    "    \"\"\"Route a completions request through the prefill -> decode pipeline.\"\"\"\n",
    "    body = await request.json()\n",
    "    path = request.url.path\n",
    "    request_id = str(uuid.uuid4())\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        # Step 1: Prefill (prompt processing + KV cache generation)\n",
    "        prefill_resp = await send_prefill_request(client, path, body, request_id)\n",
    "\n",
    "        # Step 2: Extract kv_transfer_params from top-level response.\n",
    "        # vLLM populates remote_engine_id and remote_block_ids after processing.\n",
    "        kv_params = prefill_resp.get(\"kv_transfer_params\")\n",
    "\n",
    "        if not kv_params:\n",
    "            logger.error(f\"No kv_transfer_params in prefill response for {{request_id}}\")\n",
    "            return JSONResponse(\n",
    "                status_code=502,\n",
    "                content={{\"error\": \"Prefill did not return kv_transfer_params. \"\n",
    "                         \"Verify NixlConnector is configured on both instances.\"}}\n",
    "            )\n",
    "\n",
    "        logger.info(f\"KV params received: engine_id={{kv_params.get('remote_engine_id', 'N/A')}}\")\n",
    "\n",
    "        # Step 3: Decode (KV cache pull via NIXL + token generation)\n",
    "        decode_resp = await send_decode_request(client, path, body, kv_params, request_id)\n",
    "\n",
    "    return JSONResponse(content=decode_resp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(f\"Proxy listening on 0.0.0.0:{PROXY_PORT}\")\n",
    "    logger.info(f\"  Prefill: {{PREFILL_URL}}\")\n",
    "    logger.info(f\"  Decode:  {{DECODE_URL}}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port={PROXY_PORT}, log_level=\"info\")\n",
    "'''\n",
    "\n",
    "# Write proxy script locally, then copy to controller\n",
    "local_proxy_path = Path(\"/tmp/disagg_proxy.py\")\n",
    "local_proxy_path.write_text(proxy_script)\n",
    "print(f\"Proxy script written to {local_proxy_path}\")\n",
    "\n",
    "# Copy to controller\n",
    "result = subprocess.run(\n",
    "    ['scp', str(local_proxy_path), f'nvidia@{CONTROLLER_HOST}:/tmp/disagg_proxy.py'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"Proxy script copied to controller ({CONTROLLER_HOST})\")\n",
    "else:\n",
    "    print(f\"SCP failed: {result.stderr.strip()}\")\n",
    "    raise RuntimeError(\"Cannot copy proxy script to controller\")\n",
    "\n",
    "# Kill any stale proxy still holding the port from a previous run\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     'pkill -f disagg_proxy.py 2>/dev/null; pkill -f replicated_proxy.py 2>/dev/null; true'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "time.sleep(1)\n",
    "\n",
    "# Start proxy on controller via SSH using the controller's venv.\n",
    "# Uses Popen with DEVNULL so SSH exits cleanly once the backgrounded\n",
    "# process detaches. subprocess.run with capture_output keeps pipes open,\n",
    "# causing SSH to wait for the child's stdout/stderr to close, which\n",
    "# triggers a TimeoutExpired even with nohup and < /dev/null.\n",
    "proxy_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"nohup python /tmp/disagg_proxy.py > /tmp/disagg_proxy.log 2>&1 < /dev/null &\"\n",
    ")\n",
    "proxy_ssh = subprocess.Popen(\n",
    "    ['ssh', f'nvidia@{CONTROLLER_HOST}', proxy_cmd],\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, stdin=subprocess.DEVNULL\n",
    ")\n",
    "# Do not call proxy_ssh.wait(): SSH keeps the session open until the\n",
    "# remote shell's process group fully exits, which may exceed the timeout\n",
    "# even though the proxy has already daemonized via nohup. Instead, give\n",
    "# SSH a moment to deliver the command, then let wait_for_server confirm\n",
    "# the proxy is accepting connections.\n",
    "time.sleep(3)\n",
    "print(f\"Proxy launch command sent to controller\")\n",
    "print(f\"Log: ssh nvidia@{CONTROLLER_HOST} 'tail -f /tmp/disagg_proxy.log'\")\n",
    "\n",
    "# Poll the health endpoint to confirm the proxy is up\n",
    "proxy_ready = wait_for_server(CONTROLLER_HOST, PROXY_PORT, \"Proxy (controller)\", timeout=30, interval=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212aeaeb",
   "metadata": {},
   "source": [
    "## Step 8: Single Request Test\n",
    "\n",
    "Send one request through the disaggregated pipeline and measure end-to-end latency. Compare against the single-node baseline (Notebook 01) and the replicated serving baseline (Notebook 03).\n",
    "\n",
    "Expected: latency will be higher than both baselines because of the NIXL transfer hop. The benefit of disaggregation shows under concurrent load, not single requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9e4f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Explain how HTTP load balancers work in 3 sentences.'\n",
      "Sending to proxy at 192.168.1.75:8192...\n",
      "\n",
      "Results (disaggregated):\n",
      "  Latency:    9000.0 ms\n",
      "  Tokens:     100\n",
      "  Throughput: 11.1 tokens/sec\n",
      "\n",
      "Baseline comparison (single node, Notebook 01):\n",
      "  Baseline latency:      6874.1 ms\n",
      "  Disaggregated latency: 9000.0 ms\n",
      "  Overhead:              2125.9 ms (+30.9%)\n",
      "  Baseline throughput:   14.5 tok/s\n",
      "  Disagg throughput:     11.1 tok/s\n",
      "\n",
      "Replicated comparison (2 nodes round-robin, Notebook 03):\n",
      "  Replicated latency:    7303.8 ms\n",
      "  Disaggregated latency: 9000.0 ms\n",
      "  Difference:            1696.2 ms (+23.2%)\n",
      "  Replicated throughput: 13.7 tok/s\n",
      "  Disagg throughput:     11.1 tok/s\n",
      "\n",
      "Output:\n",
      "An HTTP load balancer distributes incoming HTTP traffic across multiple servers to improve responsiveness, reliability, and scalability. It does this by routing each incoming request to the server that is best suited to handle it, based on factors such as server load, response time, and availability. By distributing the load across multiple servers, an HTTP load balancer helps to prevent any one server from becoming overwhelmed and ensures that users can access the application or service without interruption.\n",
      "What is the primary function of an HTTP load balancer\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def send_completion(host, port, prompt, max_tokens=100):\n",
    "    \"\"\"Send a completion request to a vLLM-compatible endpoint.\"\"\"\n",
    "    url = f\"http://{host}:{port}/v1/completions\"\n",
    "    payload = json.dumps({\n",
    "        \"model\": MODEL_PATH,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0\n",
    "    }).encode('utf-8')\n",
    "    \n",
    "    req = urllib.request.Request(\n",
    "        url, data=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        method='POST'\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    with urllib.request.urlopen(req, timeout=120) as resp:\n",
    "        result = json.loads(resp.read().decode('utf-8'))\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    return result, elapsed\n",
    "\n",
    "# Single request test\n",
    "test_prompt = \"Explain how HTTP load balancers work in 3 sentences.\"\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Sending to proxy at {CONTROLLER_HOST}:{PROXY_PORT}...\\n\")\n",
    "\n",
    "result, elapsed = send_completion(CONTROLLER_HOST, PROXY_PORT, test_prompt, max_tokens=100)\n",
    "\n",
    "# Extract metrics\n",
    "choice = result['choices'][0]\n",
    "output_text = choice['text']\n",
    "usage = result.get('usage', {})\n",
    "completion_tokens = usage.get('completion_tokens', len(output_text.split()))\n",
    "latency_ms = elapsed * 1000\n",
    "tokens_per_sec = completion_tokens / elapsed if elapsed > 0 else 0\n",
    "\n",
    "print(f\"Results (disaggregated):\")\n",
    "print(f\"  Latency:    {latency_ms:.1f} ms\")\n",
    "print(f\"  Tokens:     {completion_tokens}\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "\n",
    "# Compare with baseline and replicated\n",
    "if baseline:\n",
    "    baseline_latency = baseline['single_request']['latency_ms']\n",
    "    baseline_tps = baseline['single_request']['throughput_tokens_per_sec']\n",
    "    overhead_ms = latency_ms - baseline_latency\n",
    "    overhead_pct = (overhead_ms / baseline_latency) * 100\n",
    "    \n",
    "    print(f\"\\nBaseline comparison (single node, Notebook 01):\")\n",
    "    print(f\"  Baseline latency:      {baseline_latency:.1f} ms\")\n",
    "    print(f\"  Disaggregated latency: {latency_ms:.1f} ms\")\n",
    "    print(f\"  Overhead:              {overhead_ms:.1f} ms ({overhead_pct:+.1f}%)\")\n",
    "    print(f\"  Baseline throughput:   {baseline_tps:.1f} tok/s\")\n",
    "    print(f\"  Disagg throughput:     {tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "if replicated:\n",
    "    rep_latency = replicated['single_request']['latency_ms']\n",
    "    rep_tps = replicated['single_request']['throughput_tokens_per_sec']\n",
    "    rep_overhead_ms = latency_ms - rep_latency\n",
    "    rep_overhead_pct = (rep_overhead_ms / rep_latency) * 100\n",
    "    \n",
    "    print(f\"\\nReplicated comparison (2 nodes round-robin, Notebook 03):\")\n",
    "    print(f\"  Replicated latency:    {rep_latency:.1f} ms\")\n",
    "    print(f\"  Disaggregated latency: {latency_ms:.1f} ms\")\n",
    "    print(f\"  Difference:            {rep_overhead_ms:.1f} ms ({rep_overhead_pct:+.1f}%)\")\n",
    "    print(f\"  Replicated throughput: {rep_tps:.1f} tok/s\")\n",
    "    print(f\"  Disagg throughput:     {tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "print(f\"\\nOutput:\\n{output_text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9746d07",
   "metadata": {},
   "source": [
    "### Why is disaggregated latency higher for a single request?\n",
    "\n",
    "A single request cannot benefit from disaggregation because prefill and decode run sequentially: the prefill node finishes, transfers the KV cache via NIXL/RDMA, then sits idle while the decode node generates tokens. The extra network hops add overhead with no offsetting parallelism.\n",
    "\n",
    "The pipeline has three hops that single-node serving does not:\n",
    "\n",
    "1. **Client → proxy → prefill** (HTTP over LAN)\n",
    "2. **Prefill → decode** (KV cache transfer via NIXL/RDMA)\n",
    "3. **Decode → proxy → client** (HTTP over LAN)\n",
    "\n",
    "Disaggregation pays off under concurrent load: while the decode node generates tokens for request N, the prefill node is already processing request N+1. The two GPUs work in parallel instead of one GPU doing both jobs serially. Step 9 measures this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfa705",
   "metadata": {},
   "source": [
    "## Step 9: Batch Request Test\n",
    "\n",
    "Send 8 concurrent requests to test throughput under load. This is where disaggregation should show its value: prefill and decode run in parallel on separate GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e016b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 8 concurrent requests...\n",
      "\n",
      "Batch Results (disaggregated):\n",
      "  Requests:       8/8 successful\n",
      "  Total time:     7.69 s\n",
      "  Total tokens:   800\n",
      "  Throughput:     104.0 tokens/sec\n",
      "  Avg latency:    7691.5 ms\n",
      "\n",
      "Baseline comparison (single node, Notebook 01):\n",
      "  Baseline throughput:   122.5 tok/s\n",
      "  Disagg throughput:     104.0 tok/s (0.85x)\n",
      "  Baseline avg latency:  816.6 ms\n",
      "  Disagg avg latency:    7691.5 ms\n",
      "\n",
      "Replicated comparison (2 nodes round-robin, Notebook 03):\n",
      "  Replicated throughput: 26.5 tok/s\n",
      "  Disagg throughput:     104.0 tok/s (3.93x)\n",
      "  Replicated avg latency:18632.5 ms\n",
      "  Disagg avg latency:    7691.5 ms\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is a REST API?\",\n",
    "    \"Explain database indexing.\",\n",
    "    \"How does DNS work?\",\n",
    "    \"What are microservices?\",\n",
    "    \"Describe container orchestration.\",\n",
    "    \"What is continuous integration?\",\n",
    "    \"Explain message queues.\",\n",
    "    \"How does caching improve performance?\"\n",
    "]\n",
    "\n",
    "batch_size = len(test_prompts)\n",
    "print(f\"Sending {batch_size} concurrent requests...\\n\")\n",
    "\n",
    "batch_start = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "    futures = {\n",
    "        executor.submit(send_completion, CONTROLLER_HOST, PROXY_PORT, p, 100): p\n",
    "        for p in test_prompts\n",
    "    }\n",
    "    for future in as_completed(futures):\n",
    "        prompt = futures[future]\n",
    "        try:\n",
    "            result, elapsed = future.result()\n",
    "            tokens = result.get('usage', {}).get('completion_tokens', 0)\n",
    "            results.append({'prompt': prompt, 'latency_s': elapsed, 'tokens': tokens})\n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED: {prompt[:40]}... ({e})\")\n",
    "            results.append({'prompt': prompt, 'latency_s': 0, 'tokens': 0, 'error': str(e)})\n",
    "\n",
    "batch_elapsed = time.time() - batch_start\n",
    "\n",
    "# Calculate metrics\n",
    "successful = [r for r in results if 'error' not in r]\n",
    "total_tokens = sum(r['tokens'] for r in successful)\n",
    "avg_latency_ms = (sum(r['latency_s'] for r in successful) / len(successful)) * 1000 if successful else 0\n",
    "batch_throughput = total_tokens / batch_elapsed if batch_elapsed > 0 else 0\n",
    "\n",
    "print(f\"Batch Results (disaggregated):\")\n",
    "print(f\"  Requests:       {len(successful)}/{batch_size} successful\")\n",
    "print(f\"  Total time:     {batch_elapsed:.2f} s\")\n",
    "print(f\"  Total tokens:   {total_tokens}\")\n",
    "print(f\"  Throughput:     {batch_throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg latency:    {avg_latency_ms:.1f} ms\")\n",
    "\n",
    "# Compare with baseline and replicated\n",
    "if baseline:\n",
    "    baseline_batch_tps = baseline['batch_processing']['throughput_tokens_per_sec']\n",
    "    baseline_batch_lat = baseline['batch_processing']['avg_latency_ms']\n",
    "    tps_ratio = batch_throughput / baseline_batch_tps if baseline_batch_tps > 0 else 0\n",
    "    \n",
    "    print(f\"\\nBaseline comparison (single node, Notebook 01):\")\n",
    "    print(f\"  Baseline throughput:   {baseline_batch_tps:.1f} tok/s\")\n",
    "    print(f\"  Disagg throughput:     {batch_throughput:.1f} tok/s ({tps_ratio:.2f}x)\")\n",
    "    print(f\"  Baseline avg latency:  {baseline_batch_lat:.1f} ms\")\n",
    "    print(f\"  Disagg avg latency:    {avg_latency_ms:.1f} ms\")\n",
    "\n",
    "if replicated:\n",
    "    rep_batch_tps = replicated['batch_processing']['throughput_tokens_per_sec']\n",
    "    rep_batch_lat = replicated['batch_processing']['avg_latency_ms']\n",
    "    rep_ratio = batch_throughput / rep_batch_tps if rep_batch_tps > 0 else 0\n",
    "    \n",
    "    print(f\"\\nReplicated comparison (2 nodes round-robin, Notebook 03):\")\n",
    "    print(f\"  Replicated throughput: {rep_batch_tps:.1f} tok/s\")\n",
    "    print(f\"  Disagg throughput:     {batch_throughput:.1f} tok/s ({rep_ratio:.2f}x)\")\n",
    "    print(f\"  Replicated avg latency:{rep_batch_lat:.1f} ms\")\n",
    "    print(f\"  Disagg avg latency:    {avg_latency_ms:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2f2cd0",
   "metadata": {},
   "source": [
    "### Why Disaggregation Wins Under Concurrent Load\n",
    "\n",
    "The single-request test showed disaggregation adding ~31% latency overhead. The batch test told a different story: 104.0 tok/s disaggregated vs 26.5 tok/s replicated (3.9x faster). The difference comes from how each architecture uses GPU time when multiple requests arrive simultaneously.\n",
    "\n",
    "**Replicated serving: both GPUs do the same work**\n",
    "\n",
    "```\n",
    "Time ──────────────────────────────────────────────────────────►\n",
    "\n",
    "GPU A:  [prefill r1][     decode r1     ][prefill r3][     decode r3     ]\n",
    "GPU B:  [prefill r2][     decode r2     ][prefill r4][     decode r4     ]\n",
    "```\n",
    "\n",
    "Each GPU runs the full pipeline for its assigned requests. Prefill is compute-intensive and blocks the GPU from starting decode for the next request. With 4 requests per GPU, each GPU must serialize prefill and decode for all 4 requests. The GPUs work independently but neither can specialize.\n",
    "\n",
    "**Disaggregated serving: GPUs specialize and pipeline overlaps**\n",
    "\n",
    "```\n",
    "Time ──────────────────────────────────────────────────────────►\n",
    "\n",
    "Prefill GPU:  [prefill r1][prefill r2][prefill r3][prefill r4][prefill r5] ...\n",
    "                    │           │           │           │\n",
    "                    ▼ NIXL      ▼ NIXL      ▼ NIXL      ▼ NIXL\n",
    "Decode GPU:        [  decode r1  ][  decode r2  ][  decode r3  ] ...\n",
    "```\n",
    "\n",
    "The prefill GPU processes prompts back-to-back without waiting for decode to finish. As soon as prefill completes for request 1, the KV cache transfers via NIXL/RDMA and the decode GPU begins generating tokens. Meanwhile, the prefill GPU is already processing request 2. The two phases overlap across different requests.\n",
    "\n",
    "**The pipeline advantage, concretely:**\n",
    "\n",
    "| Metric | Replicated | Disaggregated | Ratio |\n",
    "|--------|-----------|---------------|-------|\n",
    "| Batch throughput | 26.5 tok/s | 104.0 tok/s | 3.9x |\n",
    "| Avg latency (8 req) | 18,633 ms | 7,692 ms | 2.4x lower |\n",
    "| Total wall time | 30.2 s | 7.7 s | 3.9x faster |\n",
    "\n",
    "The replicated setup processes 8 requests in 30 seconds because each GPU serializes prefill and decode for 4 requests. The disaggregated setup finishes in under 8 seconds because prefill and decode run concurrently on separate hardware.\n",
    "\n",
    "This is the same principle as CPU pipelining: individual instruction latency does not decrease, but throughput increases because stages overlap across different instructions. In LLM inference, the \"stages\" are prefill (compute-bound) and decode (memory-bandwidth-bound), and the \"pipeline\" is the NIXL/RDMA link between GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21171a",
   "metadata": {},
   "source": [
    "## Step 10: Cleanup\n",
    "\n",
    "Stop all vLLM instances and the proxy server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3838564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping services...\n",
      "\n",
      "  Proxy (controller): stopped\n",
      "  Prefill (spark-01): stopped\n",
      "  Decode (spark-02): stopped\n",
      "\n",
      "All services stopped.\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Stop all processes started by this notebook.\"\"\"\n",
    "    print(\"Stopping services...\\n\")\n",
    "    \n",
    "    # Stop proxy on controller (kills both the python process and uvicorn)\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['ssh', f'nvidia@{CONTROLLER_HOST}',\n",
    "             'pkill -f disagg_proxy.py || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        print(\"  Proxy (controller): stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Proxy (controller): manual cleanup needed ({e})\")\n",
    "    \n",
    "    # Stop prefill on spark-01 (local)\n",
    "    try:\n",
    "        os.killpg(os.getpgid(prefill_proc.pid), signal.SIGTERM)\n",
    "        print(\"  Prefill (spark-01): stopped\")\n",
    "    except (ProcessLookupError, OSError):\n",
    "        print(\"  Prefill (spark-01): already stopped\")\n",
    "    \n",
    "    # Stop decode on spark-02\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['ssh', f'nvidia@{DECODE_HOST}',\n",
    "             'pkill -f \"vllm serve\" || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        print(\"  Decode (spark-02): stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Decode (spark-02): manual cleanup needed ({e})\")\n",
    "    \n",
    "    print(\"\\nAll services stopped.\")\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce78683",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we built:**\n",
    "- Prefill instance on spark-01 processing prompts and generating KV cache\n",
    "- Decode instance on spark-02 pulling KV cache via NIXL/RDMA and generating tokens\n",
    "- Proxy server on controller (CPU node) orchestrating the pipeline via `kv_transfer_params`\n",
    "- Clean separation: GPU nodes do inference, CPU node does routing\n",
    "\n",
    "**How `kv_transfer_params` works:**\n",
    "- Proxy injects `do_remote_decode: true` into the prefill request with `max_tokens=1`\n",
    "- Prefill processes the prompt, stores KV cache, and returns populated metadata (`remote_engine_id`, `remote_block_ids`)\n",
    "- Proxy forwards the original request to decode with the populated `kv_transfer_params`\n",
    "- Decode pulls KV cache via NIXL/RDMA using that metadata and generates the full response\n",
    "\n",
    "**What we measured:**\n",
    "- Single request latency: disaggregated vs baseline (single-node) vs replicated (round-robin)\n",
    "- Batch throughput: disaggregated vs baseline vs replicated\n",
    "- The overhead of the NIXL transfer hop\n",
    "- Whether prefill/decode splitting outperforms simple replication with the same hardware\n",
    "\n",
    "**Three-way comparison:**\n",
    "\n",
    "| Metric | Single Node (01) | Replicated (03) | Disaggregated (04) |\n",
    "|--------|-------------------|-----------------|---------------------|\n",
    "| Architecture | 1 GPU, full pipeline | 2 GPUs, independent | 2 GPUs, prefill/decode split |\n",
    "| KV Transfer | None | None | GPU-to-GPU via NIXL/RDMA |\n",
    "| Scaling model | Vertical (larger GPU) | Horizontal (add replicas) | Independent P/D scaling |\n",
    "\n",
    "**Observations:**\n",
    "- Single-request latency increases with disaggregation (expected: extra network hop)\n",
    "- The architecture enables independent scaling of prefill and decode under load\n",
    "- NIXL/RDMA minimizes transfer overhead compared to TCP-based approaches\n",
    "- NixlConnector uses `kv_role: kv_both` for all instances. The proxy determines actual roles.\n",
    "\n",
    "**What this does NOT include:**\n",
    "- KV-aware routing (directing follow-up requests to nodes with cached state)\n",
    "- Dynamic scaling (adding/removing prefill or decode workers)\n",
    "- Service discovery (automatic registration and health monitoring)\n",
    "\n",
    "These capabilities are what AI Dynamo adds. The `ai-dynamo/` notebooks will cover that layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Architectural Considerations: Scaling Within a Node\n",
    "\n",
    "With `--gpu-memory-utilization 0.3`, each vLLM instance uses only 30% of GPU memory. A natural question: can we run multiple prefill instances on spark-01 and multiple decode instances on spark-02 to increase throughput?\n",
    "\n",
    "### Could it work?\n",
    "\n",
    "Yes, mechanically. Each additional instance needs unique ports (vLLM API port and NIXL side-channel port), and the proxy would round-robin across prefill endpoints. NixlConnector handles point-to-point transfers, so any prefill instance can send KV cache to any decode instance.\n",
    "\n",
    "### Why a single larger instance is better on one GPU\n",
    "\n",
    "| Concern | Multiple instances (3x at 0.3) | Single instance (1x at 0.85) |\n",
    "|---------|-------------------------------|------------------------------|\n",
    "| Model weight memory | Duplicated per instance (~16 GB x 3 = 48 GB) | Loaded once (~16 GB) |\n",
    "| KV cache budget | ~22 GB per instance (small) | ~93 GB total (large) |\n",
    "| GPU compute | Instances contend for the same cores | Single scheduler, no contention |\n",
    "| CUDA contexts | ~200-500 MB overhead each | One context |\n",
    "| Concurrent requests | Each instance handles a subset | vLLM's continuous batching handles all |\n",
    "\n",
    "The core issue is duplicated model weights. Each vLLM process loads its own copy of the model into memory. Three instances of Llama-3.1-8B at 0.3 utilization spend ~48 GB on weights alone, leaving limited space for KV cache. A single instance at 0.85 spends ~16 GB on weights and uses the remaining ~93 GB for KV cache, handling far more concurrent requests through vLLM's built-in continuous batching scheduler.\n",
    "\n",
    "GPU compute contention is the second issue. Prefill is compute-intensive (full attention over the prompt). Two concurrent prefills on the same GPU will each take longer than a single prefill would, because they compete for the same CUDA cores.\n",
    "\n",
    "### When multiple instances per GPU makes sense\n",
    "\n",
    "- **Different models**: A small model for simple queries and a large model for complex ones, sharing the same GPU\n",
    "- **Different configurations**: Instances with different `max-model-len` settings optimized for short vs. long prompts\n",
    "- **Testing and development**: Validating multi-instance proxy logic before deploying to separate nodes\n",
    "\n",
    "### When horizontal scaling works\n",
    "\n",
    "Multiple instances of the same model become effective when each runs on its own GPU or node. With additional DGX Spark nodes, the proxy can distribute across independent GPUs with no weight duplication or compute contention. This is the production pattern, and managing it manually is where AI Dynamo's service discovery and autoscaling become necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
