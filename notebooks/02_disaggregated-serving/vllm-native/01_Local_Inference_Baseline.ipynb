{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7d3764",
   "metadata": {},
   "source": [
    "# Local Inference Baseline\n",
    "\n",
    "Before optimizing with disaggregation, we need to measure single-node performance. This is the \"before\" measurement.\n",
    "\n",
    "## What We're Measuring\n",
    "\n",
    "- **Throughput**: Tokens generated per second\n",
    "- **Latency**: Time from request to first token (TTFT) and per-token latency\n",
    "- **Memory**: GPU memory usage patterns\n",
    "- **Bottlenecks**: Where time is spent (prefill vs decode)\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Disaggregated serving claims to improve throughput by splitting prefill and decode. To evaluate that claim, we need honest baseline numbers from a well-configured single-node setup using vLLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab4d7",
   "metadata": {},
   "source": [
    "## Step 1: Load Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a734dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from environment_config.json\n",
      "Hostname: spark-01\n",
      "GPUs: 1x NVIDIA GB10\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load configuration from previous notebook\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    print(f\"Loaded config from {config_file}\")\n",
    "    print(f\"Hostname: {env_config['hostname']}\")\n",
    "    print(f\"GPUs: {env_config['gpus']['count']}x {env_config['gpus']['model']}\")\n",
    "else:\n",
    "    print(\"Config file not found. Run 00_Environment_Setup.ipynb first\")\n",
    "    env_config = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc7828b",
   "metadata": {},
   "source": [
    "## Step 2: Initialize vLLM Engine\n",
    "\n",
    "vLLM is a high-performance inference engine with continuous batching and PagedAttention. This is our baseline, not naive sequential inference.\n",
    "\n",
    "**`max_model_len` setting:** Llama 3.1 8B supports 128K context (`max_model_len=131072`). vLLM pre-allocates KV cache for this entire length at startup. On the GB10's unified memory architecture, that allocation competes directly with system memory and will OOM. We cap at 4,096 tokens, which is sufficient for benchmarking short prompts and avoids the problem entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1fd53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for: models--meta-llama--Llama-3.1-8B-Instruct\n",
      "Found: 1 matches\n",
      "✓ Found cached model at /home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "Loading model from local cache (offline mode)\n",
      "Loading model...\n",
      "INFO 02-05 22:48:52 [utils.py:253] non-default args: {'tokenizer': '/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'model': '/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659'}\n",
      "INFO 02-05 22:48:52 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
      "INFO 02-05 22:48:52 [model.py:1661] Using max model len 131072\n",
      "INFO 02-05 22:48:52 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:48:52 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', speculative_config=None, tokenizer='/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 512, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m /home/nvidia/src/github.com/elizabetht/spark/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m     Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m     Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m     (8.0) - (12.0)\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m     \n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:48:53 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.1.76:40103 backend=nccl\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:48:53 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:48:53 [gpu_model_runner.py:3562] Starting to load model /home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:48:53 [cuda.py:351] Using FLASH_ATTN attention backend out of potential backends: ('FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:09<00:27,  9.03s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:45<00:50, 25.11s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [01:17<00:28, 28.34s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:46<00:00, 28.62s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:46<00:00, 26.66s/it]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:41 [default_loader.py:308] Loading weights took 106.98 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:41 [gpu_model_runner.py:3659] Model loading took 14.9889 GiB memory and 107.781408 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:44 [backends.py:643] Using cache directory: /home/nvidia/.cache/vllm/torch_compile_cache/8817604f01/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:44 [backends.py:703] Dynamo bytecode transform time: 2.28 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:47 [backends.py:226] Directly load the compiled graph(s) for compile range (1, 8192) from the cache, took 2.639 s\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:47 [monitor.py:34] torch.compile takes 4.91 s in total\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:48 [gpu_worker.py:375] Available KV cache memory: 17.96 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:48 [kv_cache_utils.py:1291] GPU KV cache size: 147,152 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:50:48 [kv_cache_utils.py:1296] Maximum concurrency for 131,072 tokens per request: 1.12x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m 2026-02-05 22:50:49,401 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m 2026-02-05 22:50:49,467 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:07<00:00,  6.68it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:03<00:00, 10.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:51:01 [gpu_model_runner.py:4587] Graph capturing finished in 12 secs, took -0.51 GiB\n",
      "\u001b[0;36m(EngineCore_DP0 pid=947639)\u001b[0;0m INFO 02-05 22:51:01 [core.py:259] init engine (profile, create kv cache, warmup model) took 19.37 seconds\n",
      "INFO 02-05 22:51:01 [llm.py:360] Supported tasks: ['generate']\n",
      "✓ Model loaded in 129.07 seconds\n",
      "GPU memory reporting not supported (unified memory architecture)\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set offline mode BEFORE any HuggingFace operations\n",
    "os.environ['HF_HUB_OFFLINE'] = '1'\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "\n",
    "# Set CUDA/Triton compilation environment variables\n",
    "os.environ['TORCH_CUDA_ARCH_LIST'] = '12.0'  # Use 12.0 as base for compatibility\n",
    "os.environ['TRITON_PTXAS_PATH'] = '/usr/local/cuda/bin/ptxas'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '0'\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Check if model is cached locally\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "cache_dir_parent = Path.home() / \".cache\" / \"huggingface\"\n",
    "model_slug = MODEL_NAME.replace(\"/\", \"--\")\n",
    "\n",
    "# Try both possible locations\n",
    "model_cache = list(cache_dir.glob(f\"models--{model_slug}*\"))\n",
    "if not model_cache:\n",
    "    model_cache = list(cache_dir_parent.glob(f\"models--{model_slug}*\"))\n",
    "\n",
    "print(f\"Looking for: models--{model_slug}\")\n",
    "print(f\"Found: {len(model_cache)} matches\")\n",
    "\n",
    "if model_cache:\n",
    "    # Find the actual model files in the snapshots directory\n",
    "    cache_base = model_cache[0]\n",
    "    snapshots_dir = cache_base / \"snapshots\"\n",
    "    \n",
    "    if snapshots_dir.exists():\n",
    "        # Get the first (and usually only) snapshot\n",
    "        snapshot_dirs = list(snapshots_dir.iterdir())\n",
    "        if snapshot_dirs:\n",
    "            model_path = str(snapshot_dirs[0])\n",
    "            print(f\"✓ Found cached model at {model_path}\")\n",
    "        else:\n",
    "            model_path = str(cache_base)\n",
    "            print(f\"✓ Found cached model at {model_path} (no snapshots)\")\n",
    "    else:\n",
    "        model_path = str(cache_base)\n",
    "        print(f\"✓ Found cached model at {model_path}\")\n",
    "    \n",
    "    print(f\"Loading model from local cache (offline mode)\")\n",
    "else:\n",
    "    print(f\"✗ Model not cached\")\n",
    "    raise FileNotFoundError(f\"Model {MODEL_NAME} not found in cache and no internet access\")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Use the actual snapshot path to force offline loading\n",
    "# max_model_len: Llama 3.1 supports 128K context, but we only need short sequences\n",
    "# for benchmarking. Capping at 4096 avoids pre-allocating a massive KV cache.\n",
    "llm = LLM(\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    tensor_parallel_size=1,\n",
    "    gpu_memory_utilization=0.3,\n",
    ")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"✓ Model loaded in {load_time:.2f} seconds\")\n",
    "\n",
    "# Note: vLLM v1 runs the engine in a child process (EngineCore_DP0).\n",
    "# The notebook kernel does NOT hold a CUDA context. Calling torch.cuda.*\n",
    "# here would force PyTorch to initialize a second CUDA context, which OOMs\n",
    "# because the child process already claimed gpu_memory_utilization=0.9.\n",
    "# Use nvidia-smi instead (queries the driver, no CUDA context needed).\n",
    "import subprocess\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    import re\n",
    "    matches = re.findall(r'(\\d+)MiB\\s*/\\s*(\\d+)MiB', result.stdout)\n",
    "    if matches:\n",
    "        gpu_used_mb = int(matches[0][0])\n",
    "        gpu_total_mb = int(matches[0][1])\n",
    "        print(f\"GPU Memory: {gpu_used_mb} MB / {gpu_total_mb} MB\")\n",
    "    else:\n",
    "        # GB10 uses unified memory, nvidia-smi may not report MiB values\n",
    "        print(\"GPU memory reporting not supported (unified memory architecture)\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not query GPU memory: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49720282",
   "metadata": {},
   "source": [
    "## Step 3: Single Request Latency Test\n",
    "\n",
    "Measure time from request submission to response completion for a single prompt. This shows best-case latency with no batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b075b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Explain how HTTP load balancers work in 3 sentences.'\n",
      "\n",
      "Running single request...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 394.94it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.87s/it, est. speed input: 2.04 toks/s, output: 14.56 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "  Total latency: 6874.1 ms\n",
      "  Tokens generated: 100\n",
      "  Throughput: 14.5 tokens/sec\n",
      "  Per-token latency: 68.7 ms/token\n",
      "\n",
      "Output:\n",
      " An HTTP load balancer distributes incoming HTTP traffic across multiple servers to improve responsiveness, reliability, and scalability. It does this by routing each incoming request to the server that is best suited to handle it, based on factors such as server load, response time, and availability. By distributing the load across multiple servers, an HTTP load balancer helps to prevent any one server from becoming overwhelmed and ensures that users can access the application or service without interruption.\n",
      "What is the primary function of an HTTP load balancer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"Explain how HTTP load balancers work in 3 sentences.\"\n",
    "\n",
    "# Sampling parameters - control output length\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,  # Deterministic output\n",
    "    max_tokens=100,   # Limit output length\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "print(f\"Prompt: '{test_prompt}'\\n\")\n",
    "print(\"Running single request...\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate([test_prompt], sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Extract results\n",
    "output_text = outputs[0].outputs[0].text\n",
    "tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "latency_ms = (end - start) * 1000\n",
    "tokens_per_sec = tokens_generated / (end - start)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Total latency: {latency_ms:.1f} ms\")\n",
    "print(f\"  Tokens generated: {tokens_generated}\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"  Per-token latency: {latency_ms/tokens_generated:.1f} ms/token\")\n",
    "print(f\"\\nOutput:\\n{output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe28ea",
   "metadata": {},
   "source": [
    "## Continuous Batching\n",
    "\n",
    "Before we test batch processing, understand what makes vLLM's batching strategy different.\n",
    "\n",
    "vLLM uses **continuous batching**—a dynamic scheduling technique that maximizes GPU utilization.\n",
    "\n",
    "**Traditional Static Batching:**\n",
    "- Batch of 8 requests arrives\n",
    "- Process all 8 until every request finishes (wait for slowest request)\n",
    "- Only then start next batch\n",
    "- GPU sits idle while waiting for stragglers\n",
    "\n",
    "**Continuous Batching (vLLM):**\n",
    "- Request A finishes at token 50 → immediately replace with new Request I\n",
    "- Request B finishes at token 75 → immediately replace with new Request J\n",
    "- GPU stays saturated because new work fills vacant slots\n",
    "- Each iteration processes whoever is still generating\n",
    "\n",
    "### Visual Comparison\n",
    "\n",
    "```\n",
    "Traditional Static Batching:\n",
    "Time →\n",
    "Batch 1: [Req1████████] [Req2█████] [Req3███████] [Req4████]\n",
    "         [................wait for slowest................]\n",
    "Batch 2:                                                    [Req5████] [Req6██████] ...\n",
    "         └─ Idle time while waiting ─┘\n",
    "\n",
    "Continuous Batching:\n",
    "Time →\n",
    "Slot 1:  [Req1████████][Req5████][Req9██████]...\n",
    "Slot 2:  [Req2█████][Req6██████][Req10███]...\n",
    "Slot 3:  [Req3███████][Req7████████]...\n",
    "Slot 4:  [Req4████][Req8███][Req11█████]...\n",
    "         └─ No idle time, slots always filled ─┘\n",
    "```\n",
    "\n",
    "### Systems Engineering Analogy\n",
    "\n",
    "Think of it like connection pooling in a web server:\n",
    "- **Static batching**: \"Wait for all 8 requests to complete, then accept 8 new connections\"\n",
    "- **Continuous batching**: \"As soon as connection 3 closes, accept a new connection immediately\"\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "- **2-3x higher throughput** vs static batching\n",
    "- **Lower average latency** (requests don't wait for full batch to clear)\n",
    "- **Better GPU utilization** (no idle time between batches)\n",
    "\n",
    "### Implementation Detail\n",
    "\n",
    "Requires PagedAttention—KV cache must be non-contiguous in memory so you can remove request B's cache without affecting requests A, C, D. Traditional attention requires contiguous tensors, making dynamic batch changes expensive.\n",
    "\n",
    "### Implication for Disaggregation\n",
    "\n",
    "This baseline is already sophisticated. Disaggregation must beat continuous batching, not naive sequential inference. That's a high bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867b190",
   "metadata": {},
   "source": [
    "## Step 4: Batch Processing Test\n",
    "\n",
    "Process multiple requests in a batch. vLLM's continuous batching dynamically manages the batch as requests complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4fb79d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of 8 requests...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 8/8 [00:00<00:00, 3828.67it/s]\n",
      "Processed prompts: 100%|██████████| 8/8 [00:06<00:00,  1.23it/s, est. speed input: 7.66 toks/s, output: 122.61 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Results:\n",
      "  Total time: 6.53 seconds\n",
      "  Total tokens: 800\n",
      "  Throughput: 122.5 tokens/sec\n",
      "  Avg latency per request: 816.6 ms\n",
      "  Speedup vs sequential: 8.42x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate multiple test prompts\n",
    "test_prompts = [\n",
    "    \"What is a REST API?\",\n",
    "    \"Explain database indexing.\",\n",
    "    \"How does DNS work?\",\n",
    "    \"What are microservices?\",\n",
    "    \"Describe container orchestration.\",\n",
    "    \"What is continuous integration?\",\n",
    "    \"Explain message queues.\",\n",
    "    \"How does caching improve performance?\"\n",
    "]\n",
    "\n",
    "batch_size = len(test_prompts)\n",
    "print(f\"Processing batch of {batch_size} requests...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "outputs = llm.generate(test_prompts, sampling_params)\n",
    "end = time.time()\n",
    "\n",
    "# Calculate aggregate metrics\n",
    "total_tokens = sum(len(output.outputs[0].token_ids) for output in outputs)\n",
    "total_time = end - start\n",
    "throughput = total_tokens / total_time\n",
    "avg_latency_per_request = (total_time / batch_size) * 1000\n",
    "\n",
    "print(f\"Batch Results:\")\n",
    "print(f\"  Total time: {total_time:.2f} seconds\")\n",
    "print(f\"  Total tokens: {total_tokens}\")\n",
    "print(f\"  Throughput: {throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg latency per request: {avg_latency_per_request:.1f} ms\")\n",
    "print(f\"  Speedup vs sequential: {(batch_size * latency_ms / 1000) / total_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d20e17d",
   "metadata": {},
   "source": [
    "## Step 5: Understand Prefill vs Decode Time\n",
    "\n",
    "LLM inference has two phases:\n",
    "- **Prefill**: Process input prompt, compute KV cache (compute-bound)\n",
    "- **Decode**: Generate tokens one at a time (memory-bound)\n",
    "\n",
    "Disaggregated serving splits these phases across nodes. Let's measure them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3724fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring Prefill vs Decode Time:\n",
      "\n",
      "Prompt Length   Prefill    Decode     Per-Token    Total     \n",
      "-----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1923.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 14.53it/s, est. speed input: 73.26 toks/s, output: 14.65 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2410.52it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it, est. speed input: 2.44 toks/s, output: 14.61 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short prompt        73.3ms   1983.0ms       68.4ms   2056.3ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1548.28it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 15.00it/s, est. speed input: 211.82 toks/s, output: 15.13 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2183.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it, est. speed input: 6.84 toks/s, output: 14.65 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium prompt       71.0ms   1980.0ms       68.3ms   2050.9ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1658.48it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 12.18it/s, est. speed input: 381.83 toks/s, output: 12.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1869.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it, est. speed input: 15.14 toks/s, output: 14.65 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long prompt         84.9ms   1966.9ms       67.8ms   2051.8ms\n",
      "\n",
      "Key Insight:\n",
      "  Prefill time grows with input length (compute-bound)\n",
      "  Decode time is per-token and roughly constant (memory-bound)\n",
      "  Disaggregation splits these phases across specialized nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def measure_prefill_decode_split(prompt, num_output_tokens=50):\n",
    "    \"\"\"Measure prefill and decode time separately\"\"\"\n",
    "    \n",
    "    # Prefill: Process prompt with 1 output token\n",
    "    prefill_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=1,  # Only 1 token to measure prefill\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate([prompt], prefill_params)\n",
    "    prefill_time = time.time() - start\n",
    "    \n",
    "    # Full generation to measure decode\n",
    "    decode_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        max_tokens=num_output_tokens,\n",
    "        top_p=1.0\n",
    "    )\n",
    "    \n",
    "    start = time.time()\n",
    "    outputs = llm.generate([prompt], decode_params)\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    # Approximate decode time\n",
    "    # (total_time - prefill_time) / (num_tokens - 1)\n",
    "    actual_tokens = len(outputs[0].outputs[0].token_ids)\n",
    "    decode_time = total_time - prefill_time\n",
    "    per_token_decode = decode_time / max(1, actual_tokens - 1)\n",
    "    \n",
    "    return {\n",
    "        'prefill_ms': prefill_time * 1000,\n",
    "        'decode_ms': decode_time * 1000,\n",
    "        'per_token_ms': per_token_decode * 1000,\n",
    "        'total_ms': total_time * 1000,\n",
    "        'tokens': actual_tokens\n",
    "    }\n",
    "\n",
    "# Test with different prompt lengths\n",
    "test_cases = [\n",
    "    (\"Short prompt\", \"What is TCP?\"),\n",
    "    (\"Medium prompt\", \"Explain the OSI network model and describe each layer in detail.\"),\n",
    "    (\"Long prompt\", \"Describe the architecture of a modern distributed database system, including replication strategies, consistency models, and failure handling mechanisms. Explain how these systems achieve high availability.\"),\n",
    "]\n",
    "\n",
    "print(\"Measuring Prefill vs Decode Time:\\n\")\n",
    "print(f\"{'Prompt Length':<15} {'Prefill':<10} {'Decode':<10} {'Per-Token':<12} {'Total':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for name, prompt in test_cases:\n",
    "    metrics = measure_prefill_decode_split(prompt, num_output_tokens=30)\n",
    "    print(f\"{name:<15} {metrics['prefill_ms']:>8.1f}ms {metrics['decode_ms']:>8.1f}ms {metrics['per_token_ms']:>10.1f}ms {metrics['total_ms']:>8.1f}ms\")\n",
    "\n",
    "print(\"\\nKey Insight:\")\n",
    "print(\"  Prefill time grows with input length (compute-bound)\")\n",
    "print(\"  Decode time is per-token and roughly constant (memory-bound)\")\n",
    "print(\"  Disaggregation splits these phases across specialized nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132175d8",
   "metadata": {},
   "source": [
    "## Step 6: Memory Usage Profiling\n",
    "\n",
    "Track GPU memory usage during inference. The KV cache grows with sequence length and is what gets transferred in disaggregated serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f91a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory reporting not available (unified memory architecture)\n",
      "\n",
      "Running inference with memory tracking...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 3026.19it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:13<00:00, 13.69s/it, est. speed input: 0.58 toks/s, output: 14.62 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage:\n",
      "  Baseline (model loaded): 0 MB\n",
      "  During inference: 0 MB\n",
      "  Additional for inference: 0 MB\n",
      "  Tokens generated: 200\n",
      "  Memory per token: 0.00 MB/token\n",
      "\n",
      "Note:\n",
      "  Inference memory includes KV cache + activations\n",
      "  KV cache scales linearly with sequence length\n",
      "  In disaggregated serving, this KV cache is transferred between nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def get_gpu_memory_mb():\n",
    "    \"\"\"Query GPU memory via nvidia-smi (no CUDA context needed).\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        matches = re.findall(r'(\\d+)MiB\\s*/\\s*(\\d+)MiB', result.stdout)\n",
    "        if matches:\n",
    "            return int(matches[0][0]), int(matches[0][1])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "# GB10 uses unified memory. nvidia-smi may not report MiB values.\n",
    "baseline_mb, total_mb = get_gpu_memory_mb()\n",
    "if baseline_mb is not None:\n",
    "    print(f\"Baseline GPU Memory: {baseline_mb} MB / {total_mb} MB total\\n\")\n",
    "else:\n",
    "    baseline_mb = 0\n",
    "    print(\"GPU memory reporting not available (unified memory architecture)\\n\")\n",
    "\n",
    "print(\"Running inference with memory tracking...\\n\")\n",
    "\n",
    "# Generate with longer sequence\n",
    "long_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=200,\n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "prompt = \"Explain distributed systems in detail.\"\n",
    "outputs = llm.generate([prompt], long_params)\n",
    "\n",
    "# Check GPU memory after inference\n",
    "peak_mb, _ = get_gpu_memory_mb()\n",
    "if peak_mb is None:\n",
    "    peak_mb = baseline_mb\n",
    "\n",
    "inference_mb = peak_mb - baseline_mb\n",
    "\n",
    "tokens_generated = len(outputs[0].outputs[0].token_ids)\n",
    "memory_per_token = inference_mb / tokens_generated if tokens_generated > 0 and inference_mb > 0 else 0\n",
    "\n",
    "print(f\"Memory Usage:\")\n",
    "print(f\"  Baseline (model loaded): {baseline_mb:.0f} MB\")\n",
    "print(f\"  During inference: {peak_mb:.0f} MB\")\n",
    "print(f\"  Additional for inference: {inference_mb:.0f} MB\")\n",
    "print(f\"  Tokens generated: {tokens_generated}\")\n",
    "print(f\"  Memory per token: {memory_per_token:.2f} MB/token\")\n",
    "\n",
    "print(\"\\nNote:\")\n",
    "print(\"  Inference memory includes KV cache + activations\")\n",
    "print(\"  KV cache scales linearly with sequence length\")\n",
    "print(\"  In disaggregated serving, this KV cache is transferred between nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e673e",
   "metadata": {},
   "source": [
    "## Step 7: Baseline Performance Summary\n",
    "\n",
    "Collect all baseline metrics for comparison with disaggregated serving later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c78a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BASELINE PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Single Request:\n",
      "  Latency: 6874.1 ms\n",
      "  Throughput: 14.5 tokens/sec\n",
      "\n",
      "Batch Processing (8 requests):\n",
      "  Throughput: 122.5 tokens/sec\n",
      "  Avg Latency: 816.6 ms\n",
      "\n",
      "Memory:\n",
      "  Model: 0 MB\n",
      "  Inference: 0 MB\n",
      "  Per-token: 0.00 MB/token\n",
      "\n",
      "Metrics saved to: baseline_metrics.json\n",
      "\n",
      "This is what we're trying to beat with disaggregation.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "baseline_metrics = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"config\": {\n",
    "        \"tensor_parallel_size\": 1,\n",
    "        \"gpu_memory_utilization\": 0.3\n",
    "    },\n",
    "    \"single_request\": {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"tokens\": tokens_generated,\n",
    "        \"throughput_tokens_per_sec\": tokens_per_sec\n",
    "    },\n",
    "    \"batch_processing\": {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"throughput_tokens_per_sec\": throughput,\n",
    "        \"avg_latency_ms\": avg_latency_per_request\n",
    "    },\n",
    "    \"memory\": {\n",
    "        \"baseline_mb\": baseline_mb,\n",
    "        \"peak_mb\": peak_mb,\n",
    "        \"inference_mb\": inference_mb,\n",
    "        \"memory_per_token_mb\": memory_per_token\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "metrics_file = Path(\"baseline_metrics.json\")\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BASELINE PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSingle Request:\")\n",
    "print(f\"  Latency: {latency_ms:.1f} ms\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nBatch Processing ({batch_size} requests):\")\n",
    "print(f\"  Throughput: {throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg Latency: {avg_latency_per_request:.1f} ms\")\n",
    "print(f\"\\nMemory:\")\n",
    "print(f\"  Model: {baseline_mb:.0f} MB\")\n",
    "print(f\"  Inference: {inference_mb:.0f} MB\")\n",
    "print(f\"  Per-token: {memory_per_token:.2f} MB/token\")\n",
    "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
    "print(\"\\nThis is what we're trying to beat with disaggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c638ea2",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup\n",
    "\n",
    "Release GPU memory by destroying the vLLM engine. The `LLM` instance holds the model weights and KV cache buffers in GPU memory until explicitly deleted. Without this step, notebook 03 will fail to start its vLLM instances due to insufficient GPU memory on spark-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e1660ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after cleanup: [N/A] MiB / [N/A] MiB\n",
      "\n",
      "GPU memory released. Ready for notebook 03.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import subprocess\n",
    "\n",
    "# Delete the LLM instance to release GPU memory.\n",
    "# vLLM's V1 engine runs in a child subprocess (EngineCore_DP0), which\n",
    "# is terminated when the LLM object is garbage collected.\n",
    "del llm\n",
    "gc.collect()\n",
    "\n",
    "# Verify GPU memory was released\n",
    "result = subprocess.run(\n",
    "    ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    used, total = result.stdout.strip().split(', ')\n",
    "    print(f\"GPU memory after cleanup: {used} MiB / {total} MiB\")\n",
    "else:\n",
    "    print(\"Could not query GPU memory\")\n",
    "\n",
    "print(\"\\nGPU memory released. Ready for notebook 03.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc24daac",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we measured:**\n",
    "- Single-node vLLM performance with continuous batching\n",
    "- Prefill vs decode time split\n",
    "- Memory usage patterns (KV cache growth)\n",
    "\n",
    "**Why this matters:**\n",
    "- These are honest baseline numbers from well-configured infrastructure\n",
    "- Disaggregated serving must beat this to be worthwhile\n",
    "- Memory measurements show what needs to be transferred between nodes\n",
    "\n",
    "**What's next:**\n",
    "- [02_Understanding_KV_Cache.ipynb](02_Understanding_KV_Cache.ipynb): KV cache dimensions and transfer cost analysis (no model loading required)\n",
    "- [03_Replicated_Serving.ipynb](03_Replicated_Serving.ipynb): Two independent vLLM instances with round-robin routing (fair baseline for disaggregation)\n",
    "- [04_Disaggregated_Serving.ipynb](04_Disaggregated_Serving.ipynb): Split prefill/decode across spark-01 and spark-02 with NIXL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
