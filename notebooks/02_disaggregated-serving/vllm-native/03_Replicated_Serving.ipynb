{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c08976b",
   "metadata": {},
   "source": [
    "# Replicated Serving Baseline\n",
    "\n",
    "Run two independent vLLM instances (spark-01 and spark-02, each at 0.3 `gpu-memory-utilization`) behind a round-robin proxy on the controller. This is the fair comparison for disaggregated serving: same hardware footprint, same total memory budget, different architecture.\n",
    "\n",
    "## What We're Building\n",
    "\n",
    "```\n",
    "Client Request\n",
    "      |\n",
    "      v\n",
    "  [Round-Robin Proxy] (controller:8192)\n",
    "      |\n",
    "      +---> [vLLM Instance A] (spark-01:8100)   ← independent, full pipeline\n",
    "      |\n",
    "      +---> [vLLM Instance B] (spark-02:8200)   ← independent, full pipeline\n",
    "```\n",
    "\n",
    "Each instance runs the complete inference pipeline (prefill + decode) independently. No KV cache transfer, no NIXL, no coordination between GPUs. The proxy distributes requests across both instances.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Notebook 01 measured single-node performance at 0.3 utilization. Notebook 04 measures disaggregated serving with two nodes at 0.3 utilization each. Without this notebook, the comparison is one GPU vs two GPUs, and two GPUs will always look better.\n",
    "\n",
    "This notebook uses the same two GPUs with the same memory budget. The question becomes: **does splitting prefill and decode across nodes outperform running two identical replicas?**\n",
    "\n",
    "## Prerequisites\n",
    "- Notebooks 00 and 01 completed (environment verified, baseline measured)\n",
    "- vLLM cu130 build installed on both GPU nodes\n",
    "- Model cached on both GPU nodes\n",
    "- Passwordless SSH to controller, spark-01, and spark-02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b5ed6",
   "metadata": {},
   "source": [
    "## Architecture: Replicated vs Disaggregated\n",
    "\n",
    "The core difference between this notebook and Notebook 04 is how each GPU participates in serving a request.\n",
    "\n",
    "### Replicated Serving (This Notebook)\n",
    "\n",
    "Each instance runs the full inference pipeline independently. The proxy's only job is distributing requests.\n",
    "\n",
    "```\n",
    "                          ┌─────────────────────────────────────────────────┐\n",
    "                          │              controller:8192                    │\n",
    "                          │           Round-Robin Proxy                     │\n",
    "                          │                                                 │\n",
    "                          │   Request 1 ──► Instance A                      │\n",
    "                          │   Request 2 ──► Instance B                      │\n",
    "                          │   Request 3 ──► Instance A                      │\n",
    "                          │   Request 4 ──► Instance B                      │\n",
    "                          └────────┬────────────────────┬───────────────────┘\n",
    "                                   │                    │\n",
    "                    ┌──────────────▼──────────┐  ┌──────▼──────────────────┐\n",
    "                    │   spark-01:8100         │  │   spark-02:8200         │\n",
    "                    │   vLLM Instance A       │  │   vLLM Instance B       │\n",
    "                    │                         │  │                         │\n",
    "                    │   ┌───────────────────┐ │  │   ┌───────────────────┐ │\n",
    "                    │   │ Prefill (compute)  │ │  │   │ Prefill (compute)  │ │\n",
    "                    │   └────────┬──────────┘ │  │   └────────┬──────────┘ │\n",
    "                    │            ▼            │  │            ▼            │\n",
    "                    │   ┌───────────────────┐ │  │   ┌───────────────────┐ │\n",
    "                    │   │ KV Cache (local)   │ │  │   │ KV Cache (local)   │ │\n",
    "                    │   └────────┬──────────┘ │  │   └────────┬──────────┘ │\n",
    "                    │            ▼            │  │            ▼            │\n",
    "                    │   ┌───────────────────┐ │  │   ┌───────────────────┐ │\n",
    "                    │   │ Decode (generate)  │ │  │   │ Decode (generate)  │ │\n",
    "                    │   └───────────────────┘ │  │   └───────────────────┘ │\n",
    "                    │                         │  │                         │\n",
    "                    │   No cross-node comms   │  │   No cross-node comms   │\n",
    "                    └─────────────────────────┘  └─────────────────────────┘\n",
    "```\n",
    "\n",
    "### Disaggregated Serving (Notebook 04)\n",
    "\n",
    "One node specializes in prefill (compute-heavy), the other in decode (memory-bandwidth-bound). KV cache transfers over NIXL/RDMA after prefill completes.\n",
    "\n",
    "```\n",
    "                          ┌─────────────────────────────────────────────────┐\n",
    "                          │              controller:8192                    │\n",
    "                          │           Disaggregated Proxy                   │\n",
    "                          │                                                 │\n",
    "                          │   All requests ──► Prefill node first           │\n",
    "                          │   After prefill ──► Decode node generates       │\n",
    "                          └────────┬────────────────────┬───────────────────┘\n",
    "                                   │                    │\n",
    "                    ┌──────────────▼──────────┐  ┌──────▼──────────────────┐\n",
    "                    │   spark-01:8100         │  │   spark-02:8200         │\n",
    "                    │   Prefill Worker        │  │   Decode Worker         │\n",
    "                    │                         │  │                         │\n",
    "                    │   ┌───────────────────┐ │  │                         │\n",
    "                    │   │ Prefill (compute)  │ │  │                         │\n",
    "                    │   └────────┬──────────┘ │  │                         │\n",
    "                    │            ▼            │  │                         │\n",
    "                    │   ┌───────────────────┐ │  │   ┌───────────────────┐ │\n",
    "                    │   │ KV Cache (local) ──┼─╋──╋──► KV Cache (remote)  │ │\n",
    "                    │   └───────────────────┘ │  │   └────────┬──────────┘ │\n",
    "                    │                         │  │            ▼            │\n",
    "                    │      NIXL/RDMA ─────────┼──┼─►  GPU-to-GPU transfer │\n",
    "                    │                         │  │            ▼            │\n",
    "                    │                         │  │   ┌───────────────────┐ │\n",
    "                    │                         │  │   │ Decode (generate)  │ │\n",
    "                    │                         │  │   └───────────────────┘ │\n",
    "                    └─────────────────────────┘  └─────────────────────────┘\n",
    "```\n",
    "\n",
    "The tradeoff: replicated serving avoids the KV transfer overhead but cannot specialize hardware for different phases. Disaggregated serving pays the transfer cost upfront but enables independent scaling of prefill and decode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18a918",
   "metadata": {},
   "source": [
    "## Step 1: Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e8daed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from environment_config.json\n",
      "Loaded baseline from baseline_metrics.json\n",
      "  Single request: 6874.1 ms, 14.5 tok/s\n",
      "  Batch (8 req):  122.5 tok/s\n",
      "\n",
      "Configuration:\n",
      "  Model:       meta-llama/Llama-3.1-8B-Instruct\n",
      "  Node A:      192.168.100.10:8100 (LAN: 192.168.1.76)\n",
      "  Node B:      192.168.100.11:8200 (LAN: 192.168.1.77)\n",
      "  Proxy:       192.168.1.75:8192 (round-robin)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment config from Notebook 00\n",
    "config_file = Path(\"environment_config.json\")\n",
    "if config_file.exists():\n",
    "    with open(config_file) as f:\n",
    "        env_config = json.load(f)\n",
    "    print(f\"Loaded config from {config_file}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Run 00_Environment_Setup.ipynb first\")\n",
    "\n",
    "# Load baseline metrics from Notebook 01\n",
    "baseline_file = Path(\"baseline_metrics.json\")\n",
    "if baseline_file.exists():\n",
    "    with open(baseline_file) as f:\n",
    "        baseline = json.load(f)\n",
    "    print(f\"Loaded baseline from {baseline_file}\")\n",
    "    print(f\"  Single request: {baseline['single_request']['latency_ms']:.1f} ms, \"\n",
    "          f\"{baseline['single_request']['throughput_tokens_per_sec']:.1f} tok/s\")\n",
    "    print(f\"  Batch (8 req):  {baseline['batch_processing']['throughput_tokens_per_sec']:.1f} tok/s\")\n",
    "else:\n",
    "    print(\"WARNING: baseline_metrics.json not found. Run 01_Local_Inference_Baseline.ipynb first.\")\n",
    "    baseline = None\n",
    "\n",
    "# Configuration\n",
    "# InfiniBand IPs (192.168.100.x): direct link between GPU nodes.\n",
    "NODE1_HOST = env_config['network']['node1_ip']   # spark-01: 192.168.100.10\n",
    "NODE2_HOST = env_config['network']['node2_ip']   # spark-02: 192.168.100.11\n",
    "\n",
    "# LAN IPs (192.168.1.x): shared subnet between all three nodes.\n",
    "# The proxy on the controller uses these to reach vLLM's HTTP API.\n",
    "NODE1_LAN_HOST = \"192.168.1.76\"                  # spark-01 LAN\n",
    "NODE2_LAN_HOST = \"192.168.1.77\"                  # spark-02 LAN\n",
    "CONTROLLER_HOST = \"192.168.1.75\"                 # controller: CPU-only node\n",
    "MODEL_NAME = env_config['model']['name']\n",
    "NODE1_PORT = 8100\n",
    "NODE2_PORT = 8200\n",
    "PROXY_PORT = 8192\n",
    "\n",
    "# Virtual environment path (same on all nodes)\n",
    "VENV_PATH = os.path.expanduser(\"~/src/github.com/elizabetht/spark/.venv\")\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model:       {MODEL_NAME}\")\n",
    "print(f\"  Node A:      {NODE1_HOST}:{NODE1_PORT} (LAN: {NODE1_LAN_HOST})\")\n",
    "print(f\"  Node B:      {NODE2_HOST}:{NODE2_PORT} (LAN: {NODE2_LAN_HOST})\")\n",
    "print(f\"  Proxy:       {CONTROLLER_HOST}:{PROXY_PORT} (round-robin)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34774714",
   "metadata": {},
   "source": [
    "## Step 2: Verify SSH and Dependencies\n",
    "\n",
    "Same checks as Notebook 04 (disaggregated): SSH connectivity, model availability, vLLM cu130 build, and proxy dependencies on the controller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8ef799e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking SSH connectivity...\n",
      "\n",
      "  SSH to spark-02 (192.168.100.11): OK (hostname: spark-02)\n",
      "  SSH to controller (192.168.1.75): OK (hostname: controller)\n",
      "\n",
      "Model on spark-02: FOUND\n",
      "\n",
      "vLLM CUDA 13 build check:\n",
      "  spark-01: Version: 0.13.0+cu130 (CUDA 13 build)\n",
      "  spark-02: Version: 0.13.0+cu130 (CUDA 13 build)\n",
      "\n",
      "Controller proxy dependencies:\n",
      "  fastapi: v0.128.2\n",
      "  httpx: v0.28.1\n",
      "  uvicorn: v0.40.0\n",
      "  All dependencies available\n"
     ]
    }
   ],
   "source": [
    "def check_ssh(host, label):\n",
    "    \"\"\"Verify passwordless SSH connectivity.\"\"\"\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', '-o', 'BatchMode=yes',\n",
    "         f'nvidia@{host}', 'hostname'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  SSH to {label} ({host}): OK (hostname: {result.stdout.strip()})\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"  SSH to {label} ({host}): FAILED\")\n",
    "        print(f\"    Error: {result.stderr.strip()}\")\n",
    "        print(f\"    Fix: ssh-copy-id nvidia@{host}\")\n",
    "        return False\n",
    "\n",
    "print(\"Checking SSH connectivity...\\n\")\n",
    "check_ssh(NODE2_HOST, \"spark-02\")\n",
    "check_ssh(CONTROLLER_HOST, \"controller\")\n",
    "\n",
    "# Verify model is cached on spark-02\n",
    "result = subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{NODE2_HOST}',\n",
    "     'ls -d ~/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct 2>/dev/null && echo FOUND || echo MISSING'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "model_status = result.stdout.strip().split('\\n')[-1]\n",
    "print(f\"\\nModel on spark-02: {model_status}\")\n",
    "\n",
    "# Verify vLLM cu130 on both nodes\n",
    "VLLM_INSTALL_CMD = (\n",
    "    f\"{VENV_PATH}/bin/pip install vllm==0.13.0 \"\n",
    "    f\"--extra-index-url https://wheels.vllm.ai/0.13.0/cu130 \"\n",
    "    f\"--extra-index-url https://download.pytorch.org/whl/cu130\"\n",
    ")\n",
    "TORCH_INSTALL_CMD = (\n",
    "    f\"{VENV_PATH}/bin/pip install torch==2.9.1 torchvision==0.24.1 torchaudio==2.9.1 \"\n",
    "    f\"--index-url https://download.pytorch.org/whl/cu130\"\n",
    ")\n",
    "\n",
    "print(\"\\nvLLM CUDA 13 build check:\")\n",
    "for host, label in [(NODE1_HOST, \"spark-01\"), (NODE2_HOST, \"spark-02\")]:\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "         f'{VENV_PATH}/bin/pip show vllm 2>/dev/null | grep Version'],\n",
    "        capture_output=True, text=True, timeout=15\n",
    "    )\n",
    "    version_line = result.stdout.strip()\n",
    "    if \"cu130\" in version_line:\n",
    "        print(f\"  {label}: {version_line} (CUDA 13 build)\")\n",
    "    elif version_line:\n",
    "        print(f\"  {label}: {version_line} (wrong build, needs cu130)\")\n",
    "        print(f\"    Installing vLLM cu130 on {label}... (this takes several minutes)\")\n",
    "        vllm_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}', VLLM_INSTALL_CMD],\n",
    "            capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "        if vllm_result.returncode == 0:\n",
    "            print(f\"    vLLM cu130 installed on {label}\")\n",
    "        else:\n",
    "            print(f\"    vLLM install failed on {label}: {vllm_result.stderr.strip()[-200:]}\")\n",
    "\n",
    "        print(f\"    Installing torch cu130 on {label}...\")\n",
    "        torch_result = subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}', TORCH_INSTALL_CMD],\n",
    "            capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "        if torch_result.returncode == 0:\n",
    "            print(f\"    torch cu130 installed on {label}\")\n",
    "        else:\n",
    "            print(f\"    torch install failed on {label}: {torch_result.stderr.strip()[-200:]}\")\n",
    "    else:\n",
    "        print(f\"  {label}: vLLM NOT FOUND\")\n",
    "        print(f\"    Install with: ssh nvidia@{host} '{VLLM_INSTALL_CMD}'\")\n",
    "\n",
    "# Verify controller proxy dependencies\n",
    "print(f\"\\nController proxy dependencies:\")\n",
    "missing_pkgs = []\n",
    "for pkg in [\"fastapi\", \"httpx\", \"uvicorn\"]:\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "         f'{VENV_PATH}/bin/python -c \"import {pkg}; print({pkg}.__version__)\"'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(f\"  {pkg}: v{result.stdout.strip()}\")\n",
    "    else:\n",
    "        print(f\"  {pkg}: MISSING\")\n",
    "        missing_pkgs.append(pkg)\n",
    "\n",
    "if missing_pkgs:\n",
    "    pkgs_str = \" \".join(missing_pkgs)\n",
    "    print(f\"\\nInstalling missing packages on controller: {pkgs_str}\")\n",
    "    install = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "         f'{VENV_PATH}/bin/pip install {pkgs_str}'],\n",
    "        capture_output=True, text=True, timeout=120\n",
    "    )\n",
    "    if install.returncode == 0:\n",
    "        print(f\"  Installed successfully\")\n",
    "    else:\n",
    "        print(f\"  Installation failed: {install.stderr.strip()}\")\n",
    "else:\n",
    "    print(\"  All dependencies available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a79c00",
   "metadata": {},
   "source": [
    "## Step 3: Stop Existing vLLM Processes\n",
    "\n",
    "Clear any leftover vLLM or proxy processes on all three nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf56fcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for stale processes...\n",
      "\n",
      "  spark-01 (192.168.100.10): found 3 vLLM process(es), stopping...\n",
      "  spark-01 (192.168.100.10): stopped\n",
      "  spark-02 (192.168.100.11): found 3 vLLM process(es), stopping...\n",
      "  spark-02 (192.168.100.11): stopped\n",
      "  controller (192.168.1.75): cleared any stale proxy\n",
      "\n",
      "GPU memory status:\n",
      "  spark-01: [N/A] MiB / [N/A] MiB used\n",
      "  spark-02: [N/A] MiB / [N/A] MiB used\n"
     ]
    }
   ],
   "source": [
    "def stop_stale_vllm(host, label):\n",
    "    \"\"\"Kill any running vLLM processes on a node.\"\"\"\n",
    "    check = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "         'pgrep -f \"vllm\" | head -5'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if check.stdout.strip():\n",
    "        pids = check.stdout.strip().split('\\n')\n",
    "        print(f\"  {label} ({host}): found {len(pids)} vLLM process(es), stopping...\")\n",
    "        subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "             'pkill -f \"vllm\" || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        time.sleep(2)\n",
    "        subprocess.run(\n",
    "            ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "             'pkill -9 -f \"vllm\" 2>/dev/null || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        print(f\"  {label} ({host}): stopped\")\n",
    "    else:\n",
    "        print(f\"  {label} ({host}): no vLLM processes running\")\n",
    "\n",
    "print(\"Checking for stale processes...\\n\")\n",
    "stop_stale_vllm(NODE1_HOST, \"spark-01\")\n",
    "stop_stale_vllm(NODE2_HOST, \"spark-02\")\n",
    "\n",
    "# Stop any leftover proxy on the controller.\n",
    "# Kill both proxy scripts separately: pkill uses POSIX basic regex,\n",
    "# where | is not an OR operator (that is ERE syntax).\n",
    "subprocess.run(\n",
    "    ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{CONTROLLER_HOST}',\n",
    "     'pkill -f replicated_proxy.py 2>/dev/null; pkill -f disagg_proxy.py 2>/dev/null; true'],\n",
    "    capture_output=True, timeout=10\n",
    ")\n",
    "print(f\"  controller ({CONTROLLER_HOST}): cleared any stale proxy\")\n",
    "\n",
    "# Verify GPU memory is free on both nodes\n",
    "print(\"\\nGPU memory status:\")\n",
    "for host, label in [(NODE1_HOST, \"spark-01\"), (NODE2_HOST, \"spark-02\")]:\n",
    "    result = subprocess.run(\n",
    "        ['ssh', '-o', 'ConnectTimeout=5', f'nvidia@{host}',\n",
    "         'nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits'],\n",
    "        capture_output=True, text=True, timeout=10\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        used, total = result.stdout.strip().split(', ')\n",
    "        print(f\"  {label}: {used} MiB / {total} MiB used\")\n",
    "    else:\n",
    "        print(f\"  {label}: unable to query GPU (nvidia-smi failed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6681e06",
   "metadata": {},
   "source": [
    "## Step 4: Start vLLM Instance A (spark-01)\n",
    "\n",
    "A standard `vllm serve` with no KV transfer configuration. Each instance runs the full inference pipeline independently: prefill, decode, and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c6e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: /home/nvidia/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n",
      "Starting vLLM Instance A on spark-01...\n",
      "Command: vllm serve ... --port 8100 --gpu-memory-utilization 0.3\n",
      "Instance A started (PID: 963810)\n",
      "Log: tail -f /tmp/vllm_node1.log\n"
     ]
    }
   ],
   "source": [
    "# Find model snapshot path (same logic as Notebook 01)\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "model_slug = MODEL_NAME.replace(\"/\", \"--\")\n",
    "model_cache = list(cache_dir.glob(f\"models--{model_slug}*\"))\n",
    "\n",
    "if model_cache:\n",
    "    snapshots_dir = model_cache[0] / \"snapshots\"\n",
    "    if snapshots_dir.exists():\n",
    "        snapshot_dirs = list(snapshots_dir.iterdir())\n",
    "        if snapshot_dirs:\n",
    "            MODEL_PATH = str(snapshot_dirs[0])\n",
    "        else:\n",
    "            MODEL_PATH = str(model_cache[0])\n",
    "    else:\n",
    "        MODEL_PATH = str(model_cache[0])\n",
    "    print(f\"Model path: {MODEL_PATH}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model {MODEL_NAME} not found in cache\")\n",
    "\n",
    "# Build the vLLM command for spark-01.\n",
    "# No --kv-transfer-config: this is a standalone instance.\n",
    "node1_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"CUDA_HOME=/usr/local/cuda-13.0 \"\n",
    "    f\"HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \"\n",
    "    f\"vllm serve {MODEL_PATH} \"\n",
    "    f\"--port {NODE1_PORT} \"\n",
    "    f\"--gpu-memory-utilization 0.3 \"\n",
    "    f\"--tensor-parallel-size 1 \"\n",
    "    f\"> /tmp/vllm_node1.log 2>&1\"\n",
    ")\n",
    "\n",
    "print(\"Starting vLLM Instance A on spark-01...\")\n",
    "print(f\"Command: vllm serve ... --port {NODE1_PORT} --gpu-memory-utilization 0.3\")\n",
    "\n",
    "node1_proc = subprocess.Popen(\n",
    "    node1_cmd, shell=True,\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    "    preexec_fn=os.setsid\n",
    ")\n",
    "\n",
    "print(f\"Instance A started (PID: {node1_proc.pid})\")\n",
    "print(f\"Log: tail -f /tmp/vllm_node1.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d34f61",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Instance B (spark-02)\n",
    "\n",
    "Same configuration as Instance A, running on spark-02 via SSH. No NixlConnector, no RDMA side-channel. Each instance is completely independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e91705f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vLLM Instance B on spark-02 via SSH...\n",
      "Command: ssh nvidia@192.168.100.11 'vllm serve ... --port 8200 --gpu-memory-utilization 0.3'\n",
      "Instance B started on spark-02 (local PID: 963918)\n",
      "Remote log: ssh nvidia@192.168.100.11 'tail -f /tmp/vllm_node2.log'\n"
     ]
    }
   ],
   "source": [
    "# Build the vLLM command for spark-02.\n",
    "# Identical to spark-01: standalone instance, no KV transfer.\n",
    "node2_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"CUDA_HOME=/usr/local/cuda-13.0 \"\n",
    "    f\"HF_HUB_OFFLINE=1 TRANSFORMERS_OFFLINE=1 \"\n",
    "    f\"vllm serve {MODEL_PATH} \"\n",
    "    f\"--port {NODE2_PORT} \"\n",
    "    f\"--gpu-memory-utilization 0.3 \"\n",
    "    f\"--tensor-parallel-size 1 \"\n",
    "    f\"> /tmp/vllm_node2.log 2>&1\"\n",
    ")\n",
    "\n",
    "print(\"Starting vLLM Instance B on spark-02 via SSH...\")\n",
    "print(f\"Command: ssh nvidia@{NODE2_HOST} 'vllm serve ... --port {NODE2_PORT} --gpu-memory-utilization 0.3'\")\n",
    "\n",
    "node2_proc = subprocess.Popen(\n",
    "    ['ssh', f'nvidia@{NODE2_HOST}', node2_cmd],\n",
    "    stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,\n",
    "    preexec_fn=os.setsid\n",
    ")\n",
    "\n",
    "print(f\"Instance B started on spark-02 (local PID: {node2_proc.pid})\")\n",
    "print(f\"Remote log: ssh nvidia@{NODE2_HOST} 'tail -f /tmp/vllm_node2.log'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32222a9",
   "metadata": {},
   "source": [
    "## Step 6: Wait for Both Instances\n",
    "\n",
    "Poll health endpoints until both vLLM instances respond. Startup is faster than the disaggregated case because there is no NIXL initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "844312b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for vLLM instances to load model...\n",
      "(Typically 2-4 minutes per instance)\n",
      "\n",
      "  Instance A (spark-01): ready (0s)\n",
      "  Instance B (spark-02): ready (0s)\n",
      "\n",
      "Both instances ready.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "def wait_for_server(host, port, label, timeout=300, interval=10):\n",
    "    \"\"\"Poll a vLLM server's health endpoint until it responds.\"\"\"\n",
    "    url = f\"http://{host}:{port}/health\"\n",
    "    start = time.time()\n",
    "\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            req = urllib.request.Request(url, method='GET')\n",
    "            with urllib.request.urlopen(req, timeout=5) as resp:\n",
    "                if resp.status == 200:\n",
    "                    elapsed = time.time() - start\n",
    "                    print(f\"  {label}: ready ({elapsed:.0f}s)\")\n",
    "                    return True\n",
    "        except (urllib.error.URLError, ConnectionRefusedError, OSError):\n",
    "            pass\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"  {label}: waiting... ({elapsed:.0f}s / {timeout}s)\")\n",
    "        time.sleep(interval)\n",
    "\n",
    "    print(f\"  {label}: TIMEOUT after {timeout}s\")\n",
    "    return False\n",
    "\n",
    "print(\"Waiting for vLLM instances to load model...\")\n",
    "print(\"(Typically 2-4 minutes per instance)\\n\")\n",
    "\n",
    "node1_ready = wait_for_server(NODE1_HOST, NODE1_PORT, \"Instance A (spark-01)\")\n",
    "node2_ready = wait_for_server(NODE2_HOST, NODE2_PORT, \"Instance B (spark-02)\")\n",
    "\n",
    "if node1_ready and node2_ready:\n",
    "    print(\"\\nBoth instances ready.\")\n",
    "else:\n",
    "    print(\"\\nOne or both instances failed to start.\")\n",
    "    print(\"Check logs:\")\n",
    "    print(f\"  Instance A: tail -50 /tmp/vllm_node1.log\")\n",
    "    print(f\"  Instance B: ssh nvidia@{NODE2_HOST} 'tail -50 /tmp/vllm_node2.log'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfc358",
   "metadata": {},
   "source": [
    "## Step 7: Start the Round-Robin Proxy (Controller)\n",
    "\n",
    "The proxy distributes requests across both instances using round-robin. Compared to the disaggregated proxy in Notebook 04, this one is simpler: no `kv_transfer_params`, no prefill-then-decode pipeline, no NIXL coordination. Each request goes to one instance and that instance handles everything.\n",
    "\n",
    "### Round-Robin vs Alternatives\n",
    "\n",
    "| Strategy | How It Works | Tradeoff |\n",
    "|----------|-------------|----------|\n",
    "| Round-robin | Alternate between instances | Simple, fair for equal-length requests |\n",
    "| Least-connections | Route to instance with fewest active requests | Better under variable load |\n",
    "| Random | Random selection | Statistically equivalent to round-robin at scale |\n",
    "\n",
    "Round-robin is appropriate here because all test requests have similar prompt lengths and max token counts. In production with variable request sizes, least-connections would be preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bed0b4",
   "metadata": {},
   "source": [
    "### Request Lifecycle: Single vs Batch\n",
    "\n",
    "The following diagrams show how requests flow through the round-robin proxy.\n",
    "\n",
    "**Single request:** The proxy forwards to whichever instance is next in the round-robin cycle. One GPU handles the full pipeline.\n",
    "\n",
    "```\n",
    "  Client                 Proxy                Instance A           Instance B\n",
    "    │                      │                      │                      │\n",
    "    │── POST /completions ─►│                      │                      │\n",
    "    │                      │── forward request ───►│                      │\n",
    "    │                      │                      │── prefill ──┐        │\n",
    "    │                      │                      │             │        │\n",
    "    │                      │                      │◄─ KV cache ─┘        │\n",
    "    │                      │                      │── decode ───┐        │\n",
    "    │                      │                      │  (token by  │        │\n",
    "    │                      │                      │   token)    │        │\n",
    "    │                      │                      │◄────────────┘        │\n",
    "    │                      │◄── response ─────────│                      │\n",
    "    │◄── completion ───────│                      │                      │\n",
    "    │                      │                      │                      │\n",
    "```\n",
    "\n",
    "**Batch of 8 concurrent requests:** The proxy alternates between instances. Each GPU gets 4 requests and applies continuous batching internally.\n",
    "\n",
    "```\n",
    "  Client                 Proxy                Instance A           Instance B\n",
    "    │                      │                      │                      │\n",
    "    │── req 1 ────────────►│── forward ──────────►│                      │\n",
    "    │── req 2 ────────────►│── forward ──────────────────────────────────►│\n",
    "    │── req 3 ────────────►│── forward ──────────►│                      │\n",
    "    │── req 4 ────────────►│── forward ──────────────────────────────────►│\n",
    "    │── req 5 ────────────►│── forward ──────────►│                      │\n",
    "    │── req 6 ────────────►│── forward ──────────────────────────────────►│\n",
    "    │── req 7 ────────────►│── forward ──────────►│                      │\n",
    "    │── req 8 ────────────►│── forward ──────────────────────────────────►│\n",
    "    │                      │                      │                      │\n",
    "    │                      │              ┌───────┴───────┐  ┌───────────┴──────┐\n",
    "    │                      │              │ 4 requests    │  │ 4 requests       │\n",
    "    │                      │              │ continuous     │  │ continuous       │\n",
    "    │                      │              │ batching       │  │ batching         │\n",
    "    │                      │              └───────┬───────┘  └───────────┬──────┘\n",
    "    │                      │                      │                      │\n",
    "    │                      │◄── responses ────────│                      │\n",
    "    │                      │◄── responses ───────────────────────────────│\n",
    "    │◄── 8 completions ────│                      │                      │\n",
    "    │                      │                      │                      │\n",
    "```\n",
    "\n",
    "The key observation: under batch load, total throughput should approach 2x single-node because each GPU processes half the batch independently. Latency per request stays roughly the same since each GPU sees only 4 requests instead of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "138ab010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy script written to /tmp/replicated_proxy.py\n",
      "Proxy script copied to controller (192.168.1.75)\n",
      "Proxy started on controller\n",
      "Log: ssh nvidia@192.168.1.75 'tail -f /tmp/replicated_proxy.log'\n",
      "  Proxy (controller): ready (0s)\n"
     ]
    }
   ],
   "source": [
    "# Round-robin proxy script.\n",
    "# Distributes requests across two independent vLLM instances.\n",
    "proxy_script = f'''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Round-robin proxy for replicated vLLM instances.\n",
    "\n",
    "Routes each completions request to the next backend in rotation.\n",
    "No KV transfer, no disaggregation. Each backend runs the full\n",
    "inference pipeline independently.\n",
    "\"\"\"\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "import httpx\n",
    "import uvicorn\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [proxy] %(message)s\")\n",
    "logger = logging.getLogger(\"replicated_proxy\")\n",
    "\n",
    "BACKENDS = [\n",
    "    \"http://{NODE1_LAN_HOST}:{NODE1_PORT}\",\n",
    "    \"http://{NODE2_LAN_HOST}:{NODE2_PORT}\",\n",
    "]\n",
    "TIMEOUT = httpx.Timeout(timeout=120.0)\n",
    "\n",
    "# itertools.cycle produces an infinite round-robin iterator\n",
    "backend_cycle = itertools.cycle(BACKENDS)\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {{\"status\": \"ok\"}}\n",
    "\n",
    "@app.post(\"/v1/completions\")\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def handle_request(request: Request):\n",
    "    \"\"\"Forward request to the next backend in round-robin order.\"\"\"\n",
    "    body = await request.json()\n",
    "    path = request.url.path\n",
    "    backend = next(backend_cycle)\n",
    "    url = f\"{{backend}}{{path}}\"\n",
    "\n",
    "    logger.info(f\"Routing to {{backend}}\")\n",
    "\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        resp = await client.post(url, json=body, timeout=TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    return JSONResponse(content=resp.json())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(f\"Proxy listening on 0.0.0.0:{PROXY_PORT}\")\n",
    "    for b in BACKENDS:\n",
    "        logger.info(f\"  Backend: {{b}}\")\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port={PROXY_PORT}, log_level=\"info\")\n",
    "'''\n",
    "\n",
    "# Write proxy script locally, then copy to controller\n",
    "local_proxy_path = Path(\"/tmp/replicated_proxy.py\")\n",
    "local_proxy_path.write_text(proxy_script)\n",
    "print(f\"Proxy script written to {local_proxy_path}\")\n",
    "\n",
    "# Copy to controller\n",
    "result = subprocess.run(\n",
    "    ['scp', str(local_proxy_path), f'nvidia@{CONTROLLER_HOST}:/tmp/replicated_proxy.py'],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(f\"Proxy script copied to controller ({CONTROLLER_HOST})\")\n",
    "else:\n",
    "    print(f\"SCP failed: {result.stderr.strip()}\")\n",
    "    raise RuntimeError(\"Cannot copy proxy script to controller\")\n",
    "\n",
    "# Start proxy on controller\n",
    "proxy_cmd = (\n",
    "    f\". {VENV_PATH}/bin/activate && \"\n",
    "    f\"nohup python /tmp/replicated_proxy.py > /tmp/replicated_proxy.log 2>&1 < /dev/null &\"\n",
    ")\n",
    "result = subprocess.run(\n",
    "    ['ssh', f'nvidia@{CONTROLLER_HOST}', proxy_cmd],\n",
    "    capture_output=True, text=True, timeout=10\n",
    ")\n",
    "print(f\"Proxy started on controller\")\n",
    "print(f\"Log: ssh nvidia@{CONTROLLER_HOST} 'tail -f /tmp/replicated_proxy.log'\")\n",
    "\n",
    "# Wait for proxy to be ready\n",
    "time.sleep(3)\n",
    "proxy_ready = wait_for_server(CONTROLLER_HOST, PROXY_PORT, \"Proxy (controller)\", timeout=15, interval=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96eda2",
   "metadata": {},
   "source": [
    "## Step 8: Single Request Test\n",
    "\n",
    "Send one request through the proxy. Expected latency should be close to the single-node baseline from Notebook 01, plus a small HTTP hop through the proxy. Unlike disaggregated serving, there is no KV cache transfer overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea05276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: 'Explain how HTTP load balancers work in 3 sentences.'\n",
      "Sending to proxy at 192.168.1.75:8192...\n",
      "\n",
      "Results (replicated, single request):\n",
      "  Latency:    7303.8 ms\n",
      "  Tokens:     100\n",
      "  Throughput: 13.7 tokens/sec\n",
      "\n",
      "Baseline comparison (single node, 0.3 util):\n",
      "  Baseline latency:    6874.1 ms\n",
      "  Replicated latency:  7303.8 ms\n",
      "  Overhead:            429.7 ms (+6.3%)\n",
      "  Baseline throughput: 14.5 tok/s\n",
      "  Replicated:          13.7 tok/s\n",
      "\n",
      "Output:\n",
      "An HTTP load balancer distributes incoming HTTP traffic across multiple servers to improve responsiveness, reliability, and scalability. It does this by routing each incoming request to the server that is best suited to handle it, based on factors such as server load, response time, and availability. By distributing the load across multiple servers, an HTTP load balancer helps to prevent any one server from becoming overwhelmed and ensures that users can access the application or service without interruption.\n",
      "What is the primary function of an HTTP load balancer\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "def send_completion(host, port, prompt, max_tokens=100):\n",
    "    \"\"\"Send a completion request to a vLLM-compatible endpoint.\"\"\"\n",
    "    url = f\"http://{host}:{port}/v1/completions\"\n",
    "    payload = json.dumps({\n",
    "        \"model\": MODEL_PATH,\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.0\n",
    "    }).encode('utf-8')\n",
    "\n",
    "    req = urllib.request.Request(\n",
    "        url, data=payload,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        method='POST'\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    with urllib.request.urlopen(req, timeout=120) as resp:\n",
    "        result = json.loads(resp.read().decode('utf-8'))\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    return result, elapsed\n",
    "\n",
    "# Single request test\n",
    "test_prompt = \"Explain how HTTP load balancers work in 3 sentences.\"\n",
    "print(f\"Prompt: '{test_prompt}'\")\n",
    "print(f\"Sending to proxy at {CONTROLLER_HOST}:{PROXY_PORT}...\\n\")\n",
    "\n",
    "result, elapsed = send_completion(CONTROLLER_HOST, PROXY_PORT, test_prompt, max_tokens=100)\n",
    "\n",
    "# Extract metrics\n",
    "choice = result['choices'][0]\n",
    "output_text = choice['text']\n",
    "usage = result.get('usage', {})\n",
    "completion_tokens = usage.get('completion_tokens', len(output_text.split()))\n",
    "latency_ms = elapsed * 1000\n",
    "tokens_per_sec = completion_tokens / elapsed if elapsed > 0 else 0\n",
    "\n",
    "print(f\"Results (replicated, single request):\")\n",
    "print(f\"  Latency:    {latency_ms:.1f} ms\")\n",
    "print(f\"  Tokens:     {completion_tokens}\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "\n",
    "# Compare with baseline\n",
    "if baseline:\n",
    "    baseline_latency = baseline['single_request']['latency_ms']\n",
    "    baseline_tps = baseline['single_request']['throughput_tokens_per_sec']\n",
    "    overhead_ms = latency_ms - baseline_latency\n",
    "    overhead_pct = (overhead_ms / baseline_latency) * 100\n",
    "\n",
    "    print(f\"\\nBaseline comparison (single node, 0.3 util):\")\n",
    "    print(f\"  Baseline latency:    {baseline_latency:.1f} ms\")\n",
    "    print(f\"  Replicated latency:  {latency_ms:.1f} ms\")\n",
    "    print(f\"  Overhead:            {overhead_ms:.1f} ms ({overhead_pct:+.1f}%)\")\n",
    "    print(f\"  Baseline throughput: {baseline_tps:.1f} tok/s\")\n",
    "    print(f\"  Replicated:          {tokens_per_sec:.1f} tok/s\")\n",
    "\n",
    "print(f\"\\nOutput:\\n{output_text.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a1f323",
   "metadata": {},
   "source": [
    "## Step 9: Batch Request Test\n",
    "\n",
    "Send 8 concurrent requests through the round-robin proxy. The proxy alternates between instances, so each GPU handles 4 requests. This is where replication shows its value: two GPUs processing requests in parallel, each using continuous batching independently.\n",
    "\n",
    "### Expected Behavior\n",
    "\n",
    "With round-robin across 2 identical instances:\n",
    "- Each instance gets 4 of the 8 requests\n",
    "- Each instance applies continuous batching to its 4 requests\n",
    "- Total throughput should approach 2x the single-node baseline\n",
    "- Per-request latency should be similar to single-node (half the batch load per GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf95d0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending 8 concurrent requests (round-robin across 2 instances)...\n",
      "\n",
      "Batch Results (replicated, round-robin):\n",
      "  Requests:       8/8 successful\n",
      "  Total time:     30.21 s\n",
      "  Total tokens:   800\n",
      "  Throughput:     26.5 tokens/sec\n",
      "  Avg latency:    18632.5 ms\n",
      "\n",
      "Baseline comparison (single node, batch of 8):\n",
      "  Baseline throughput:    122.5 tok/s\n",
      "  Replicated throughput:  26.5 tok/s (0.22x)\n",
      "  Baseline avg latency:   816.6 ms\n",
      "  Replicated avg latency: 18632.5 ms\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is a REST API?\",\n",
    "    \"Explain database indexing.\",\n",
    "    \"How does DNS work?\",\n",
    "    \"What are microservices?\",\n",
    "    \"Describe container orchestration.\",\n",
    "    \"What is continuous integration?\",\n",
    "    \"Explain message queues.\",\n",
    "    \"How does caching improve performance?\"\n",
    "]\n",
    "\n",
    "batch_size = len(test_prompts)\n",
    "print(f\"Sending {batch_size} concurrent requests (round-robin across 2 instances)...\\n\")\n",
    "\n",
    "batch_start = time.time()\n",
    "results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=batch_size) as executor:\n",
    "    futures = {\n",
    "        executor.submit(send_completion, CONTROLLER_HOST, PROXY_PORT, p, 100): p\n",
    "        for p in test_prompts\n",
    "    }\n",
    "    for future in as_completed(futures):\n",
    "        prompt = futures[future]\n",
    "        try:\n",
    "            result, elapsed = future.result()\n",
    "            tokens = result.get('usage', {}).get('completion_tokens', 0)\n",
    "            results.append({'prompt': prompt, 'latency_s': elapsed, 'tokens': tokens})\n",
    "        except Exception as e:\n",
    "            print(f\"  FAILED: {prompt[:40]}... ({e})\")\n",
    "            results.append({'prompt': prompt, 'latency_s': 0, 'tokens': 0, 'error': str(e)})\n",
    "\n",
    "batch_elapsed = time.time() - batch_start\n",
    "\n",
    "# Calculate metrics\n",
    "successful = [r for r in results if 'error' not in r]\n",
    "total_tokens = sum(r['tokens'] for r in successful)\n",
    "avg_latency_ms = (sum(r['latency_s'] for r in successful) / len(successful)) * 1000 if successful else 0\n",
    "batch_throughput = total_tokens / batch_elapsed if batch_elapsed > 0 else 0\n",
    "\n",
    "print(f\"Batch Results (replicated, round-robin):\")\n",
    "print(f\"  Requests:       {len(successful)}/{batch_size} successful\")\n",
    "print(f\"  Total time:     {batch_elapsed:.2f} s\")\n",
    "print(f\"  Total tokens:   {total_tokens}\")\n",
    "print(f\"  Throughput:     {batch_throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg latency:    {avg_latency_ms:.1f} ms\")\n",
    "\n",
    "# Compare with baseline\n",
    "if baseline:\n",
    "    baseline_batch_tps = baseline['batch_processing']['throughput_tokens_per_sec']\n",
    "    baseline_batch_lat = baseline['batch_processing']['avg_latency_ms']\n",
    "    tps_ratio = batch_throughput / baseline_batch_tps if baseline_batch_tps > 0 else 0\n",
    "\n",
    "    print(f\"\\nBaseline comparison (single node, batch of {batch_size}):\")\n",
    "    print(f\"  Baseline throughput:    {baseline_batch_tps:.1f} tok/s\")\n",
    "    print(f\"  Replicated throughput:  {batch_throughput:.1f} tok/s ({tps_ratio:.2f}x)\")\n",
    "    print(f\"  Baseline avg latency:   {baseline_batch_lat:.1f} ms\")\n",
    "    print(f\"  Replicated avg latency: {avg_latency_ms:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92166fc8",
   "metadata": {},
   "source": [
    "## Step 10: Save Metrics\n",
    "\n",
    "Save replicated serving metrics for comparison in Notebook 04 (disaggregated serving)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6e5d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REPLICATED SERVING PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "\n",
      "Single Request:\n",
      "  Latency:    7303.8 ms\n",
      "  Throughput: 13.7 tokens/sec\n",
      "\n",
      "Batch Processing (8 requests, round-robin):\n",
      "  Throughput: 26.5 tokens/sec\n",
      "  Avg Latency: 18632.5 ms\n",
      "\n",
      "Comparison with single-node baseline (0.3 util):\n",
      "  Batch throughput: 122.5 tok/s (baseline)\n",
      "                    26.5 tok/s (replicated, 0.22x)\n",
      "\n",
      "Metrics saved to: replicated_metrics.json\n",
      "\n",
      "Notebook 04 will compare disaggregated serving against these numbers.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "replicated_metrics = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"config\": {\n",
    "        \"num_instances\": 2,\n",
    "        \"gpu_memory_utilization\": 0.3,\n",
    "        \"routing\": \"round-robin\",\n",
    "        \"kv_transfer\": \"none\"\n",
    "    },\n",
    "    \"single_request\": {\n",
    "        \"latency_ms\": latency_ms,\n",
    "        \"tokens\": completion_tokens,\n",
    "        \"throughput_tokens_per_sec\": tokens_per_sec\n",
    "    },\n",
    "    \"batch_processing\": {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"throughput_tokens_per_sec\": batch_throughput,\n",
    "        \"avg_latency_ms\": avg_latency_ms\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metrics\n",
    "metrics_file = Path(\"replicated_metrics.json\")\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(replicated_metrics, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"REPLICATED SERVING PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSingle Request:\")\n",
    "print(f\"  Latency:    {latency_ms:.1f} ms\")\n",
    "print(f\"  Throughput: {tokens_per_sec:.1f} tokens/sec\")\n",
    "print(f\"\\nBatch Processing ({batch_size} requests, round-robin):\")\n",
    "print(f\"  Throughput: {batch_throughput:.1f} tokens/sec\")\n",
    "print(f\"  Avg Latency: {avg_latency_ms:.1f} ms\")\n",
    "\n",
    "if baseline:\n",
    "    print(f\"\\nComparison with single-node baseline (0.3 util):\")\n",
    "    print(f\"  Batch throughput: {baseline['batch_processing']['throughput_tokens_per_sec']:.1f} tok/s (baseline)\")\n",
    "    print(f\"                    {batch_throughput:.1f} tok/s (replicated, {tps_ratio:.2f}x)\")\n",
    "\n",
    "print(f\"\\nMetrics saved to: {metrics_file}\")\n",
    "print(f\"\\nNotebook 04 will compare disaggregated serving against these numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5051456",
   "metadata": {},
   "source": [
    "## Step 11: Cleanup\n",
    "\n",
    "Stop both vLLM instances and the proxy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2babb2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping services...\n",
      "\n",
      "  Proxy (controller): stopped\n",
      "  Instance A (spark-01): stopped\n",
      "  Instance B (spark-02): stopped\n",
      "\n",
      "All services stopped.\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"Stop all processes started by this notebook.\"\"\"\n",
    "    print(\"Stopping services...\\n\")\n",
    "\n",
    "    # Stop proxy on controller\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['ssh', f'nvidia@{CONTROLLER_HOST}',\n",
    "             'pkill -f replicated_proxy.py || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        print(\"  Proxy (controller): stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Proxy (controller): manual cleanup needed ({e})\")\n",
    "\n",
    "    # Stop Instance A on spark-01 (local)\n",
    "    try:\n",
    "        os.killpg(os.getpgid(node1_proc.pid), signal.SIGTERM)\n",
    "        print(\"  Instance A (spark-01): stopped\")\n",
    "    except (ProcessLookupError, OSError):\n",
    "        print(\"  Instance A (spark-01): already stopped\")\n",
    "\n",
    "    # Stop Instance B on spark-02\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            ['ssh', f'nvidia@{NODE2_HOST}',\n",
    "             'pkill -f \"vllm serve\" || true'],\n",
    "            capture_output=True, timeout=10\n",
    "        )\n",
    "        print(\"  Instance B (spark-02): stopped\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Instance B (spark-02): manual cleanup needed ({e})\")\n",
    "\n",
    "    print(\"\\nAll services stopped.\")\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ce9d06",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**What we built:**\n",
    "- Two independent vLLM instances on spark-01 and spark-02 (0.3 `gpu-memory-utilization` each)\n",
    "- Round-robin proxy on the controller distributing requests across both instances\n",
    "- No KV cache transfer, no NIXL, no coordination between GPUs\n",
    "\n",
    "**What we measured:**\n",
    "- Single-request latency: should be close to single-node baseline (minimal proxy overhead)\n",
    "- Batch throughput: should approach ~2x single-node baseline (two GPUs processing in parallel)\n",
    "\n",
    "**Why this is the fair comparison for disaggregation:**\n",
    "- Same hardware: 2 GPUs across 2 nodes\n",
    "- Same memory budget: 0.3 utilization per GPU\n",
    "- Different architecture: replicated (each GPU does everything) vs disaggregated (one prefills, one decodes)\n",
    "\n",
    "**Replication vs Disaggregation tradeoffs:**\n",
    "\n",
    "| Dimension | Replicated (this notebook) | Disaggregated (Notebook 04) |\n",
    "|-----------|---------------------------|----------------------------|\n",
    "| Architecture | Each instance is independent | Pipeline: prefill on one, decode on another |\n",
    "| KV transfer | None | GPU-to-GPU via NIXL/RDMA |\n",
    "| Single-request latency | Lower (no transfer hop) | Higher (NIXL transfer overhead) |\n",
    "| Batch throughput | 2x single-node (parallel replicas) | Depends on prefill/decode overlap |\n",
    "| Complexity | Simple round-robin | Proxy + kv_transfer_params + NIXL side-channel |\n",
    "| Scaling model | Add more replicas | Independent prefill/decode scaling |\n",
    "\n",
    "**When replication wins:**\n",
    "- Uniform request patterns (similar prompt and output lengths)\n",
    "- Low concurrency where pipeline parallelism has no advantage\n",
    "- Simplicity is a priority\n",
    "\n",
    "**When disaggregation wins:**\n",
    "- High concurrency where prefill and decode overlap matters\n",
    "- Asymmetric workloads (long prefills, short decodes, or vice versa)\n",
    "- Independent scaling of compute-bound (prefill) and memory-bound (decode) phases\n",
    "- Production systems with KV-aware routing (cache reuse across requests)\n",
    "\n",
    "**What's next:**\n",
    "- [04_Disaggregated_Serving.ipynb](04_Disaggregated_Serving.ipynb): Split prefill/decode across nodes with NixlConnector and compare against these replicated numbers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
